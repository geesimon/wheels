{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Q: (batch, seq_len, dim)\n",
    "    K: (batch, seq_len, dim)\n",
    "    V: (batch, seq_len, dim)\n",
    "    tau: softmax scaling\n",
    "    is_causal: whether to use causal mask\n",
    "    dropout_p: dropout probability\n",
    "    \n",
    "    return: (batch, seq_len, dim)\n",
    "'''\n",
    "\n",
    "def flash_attention(Q, K, V, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(b, N)\n",
    "    m = torch.ones(b, N) * -float('inf')\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = torch.matmul(Q_i, K_j.T) * tau\n",
    "                \n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "                m_ij = torch.max(S_ij, dim=1).values\n",
    "                S_ij = S_ij - m_ij.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.exp(S_ij)\n",
    "                # P_ij = torch.exp(torch.where(S_ij - m_ij.unsqueeze(1) != float('nan'), ))\n",
    "                l_ij = torch.sum(P_ij, dim=1)                \n",
    "                m_i_new = torch.max(m_i, m_ij)\n",
    "                l_i_new = torch.exp(m_i - m_i_new) * l_i + torch.exp(m_ij - m_i_new) * l_ij\n",
    "                if dropout_p > 0:\n",
    "                    torch.manual_seed(seed)\n",
    "                    P_ij = F.dropout(P_ij, p = dropout_p, training = True)\n",
    "                \n",
    "                O_i = torch.diag(1 / l_i_new) @ (torch.diag(l_i * torch.exp(m_i - m_i_new)) @ O_i\n",
    "                                                 + torch.diag(torch.exp(m_ij - m_i_new)) @ torch.matmul(P_ij, V_j))\n",
    "\n",
    "                O[b_i, N_r:N_r + Br] = O_i\n",
    "                l[b_i, N_r:N_r + Br] = l_i_new\n",
    "                m[b_i, N_r:N_r + Br] = m_i_new\n",
    "               \n",
    "    return O\n",
    "\n",
    "def attention(Q, K, V, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    S = (Q @ K.transpose(1,2)) * tau\n",
    "    if is_causal:\n",
    "        S = torch.where(torch.tril(torch.ones_like(S)) != 0, S, torch.tensor(-float('inf')))  # apply causal mask\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    if dropout_p > 0:\n",
    "        P = F.dropout(P, p = dropout_p, training = True)\n",
    "\n",
    "    O = P @ V\n",
    "\n",
    "    return O, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8029, -1.2119, -0.6681,  0.9253,  1.8810,  2.6240,  1.5704, -0.3023,\n",
      "        -0.4101,  0.7639])\n",
      "tensor([ 0.8029, -1.2119, -0.6681,  0.9253,  1.8810,  2.6240,  1.5704, -0.3023,\n",
      "        -0.4101,  0.7639])\n",
      "Total difference: 0.3089710474014282\n",
      "True\n",
      "tensor([ 0.8921, -1.3466, -0.7424,  1.0281,  2.0900,  2.9155,  1.7449, -0.3358,\n",
      "        -0.4557,  0.8488])\n",
      "CPU times: user 1min 9s, sys: 810 ms, total: 1min 10s\n",
      "Wall time: 7.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "b, n, d = 16, 1024, 512\n",
    "# b, n, d = 8, 128, 32\n",
    "\n",
    "Q = torch.randn(b, n, d)\n",
    "K = torch.randn(b, n, d)\n",
    "V = torch.randn(b, n, d)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True)\n",
    "O_flash = flash_attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True)\n",
    "\n",
    "\n",
    "print(O[0][0][:10])\n",
    "print(O_flash[0][0][:10])\n",
    "print(f'Total difference: {torch.sum(torch.abs(O_flash - O))}')\n",
    "print(O_flash.allclose(O, atol=1e-4))\n",
    "\n",
    "O_flash = flash_attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(O_flash[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_grad(Q, K, V, O, dO, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    pass\n",
    "\n",
    "def attention_grad(Q, K, V, P, dO):\n",
    "    dV = P.transpose(1,2) @ dO\n",
    "    dP = dO @ V.transpose(1,2)\n",
    "    dS = P * (dP - (P * dP).sum(dim=-1).unsqueeze(-1))\n",
    "    # dS = torch.zeros_like(dP)\n",
    "    # for i in range(dP.shape[1]):\n",
    "    #     for j in range(dP.shape[2]):\n",
    "    #         dS[:, i, j] = P[:, i, j] * (dP[:, i, j] - (P[:,i] * dP[:,i]).sum(dim=1))       \n",
    "    dQ = dS @ K\n",
    "    dK = dS.transpose(1,2) @ Q\n",
    "\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1081, -0.0515,  0.2460,  0.5301,  0.0485,  0.9343,  0.6369, -0.2612,\n",
      "        -0.9486,  0.8193]) tensor([-1.9719,  3.5232,  7.1646,  0.0403,  3.8813, -0.0787,  5.0678,  9.2380,\n",
      "         3.3660, -1.1149])\n",
      "tensor([ 1.2582, -0.9250, -0.5326, -1.2587,  0.9578, -0.0726, -0.6635, -1.1729,\n",
      "        -0.8874, -0.4861]) tensor([ 4.2310, -3.4429,  3.2846, -1.6327, -0.9032, -6.1530, -7.0747,  3.0991,\n",
      "        -3.9514,  1.9892])\n",
      "tensor([ 0.6357,  0.3955, -2.4005,  0.0158,  0.4290, -0.0535, -0.2182,  0.7395,\n",
      "         1.1625, -0.0243]) tensor([ 0.2419, -0.1313, -0.3112,  0.0349, -0.0551, -0.1749, -0.0601,  0.0838,\n",
      "        -0.0333,  0.0851])\n"
     ]
    }
   ],
   "source": [
    "b, n, d = 16, 1024, 512\n",
    "# b, n, d = 8, 16, 4\n",
    "\n",
    "Q = torch.randn(b, n, d)\n",
    "K = torch.randn(b, n, d)\n",
    "V = torch.randn(b, n, d)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True)\n",
    "dO = O * 0.1\n",
    "\n",
    "dQ, dK, dV = attention_grad(Q, K, V, P, dO)\n",
    "print(Q[0][0][:10], dQ[0][0][:10])\n",
    "print(K[0][0][:10], dK[0][0][:10])\n",
    "print(V[0][0][:10], dV[0][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6042,  1.7630,  3.3717],\n",
      "        [-1.5306,  0.8845, -2.1709],\n",
      "        [-0.0000, -0.6152, -0.0000]])\n",
      "tensor([[ 1.6042,  1.7630,  3.3717],\n",
      "        [-1.5306,  0.8845, -2.1709],\n",
      "        [-0.0000, -0.6152, -0.0000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define a tensor\n",
    "tensor = torch.randn(3, 3)\n",
    "\n",
    "# Set the seed\n",
    "torch.manual_seed(123)\n",
    "# Apply dropout\n",
    "print(F.dropout(tensor, p=0.5))\n",
    "\n",
    "torch.manual_seed(123)\n",
    "# Apply dropout\n",
    "print(F.dropout(tensor, p=0.5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
