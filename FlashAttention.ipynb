{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Q: (batch, seq_len, dim)\n",
    "    K: (batch, seq_len, dim)\n",
    "    V: (batch, seq_len, dim)\n",
    "    tau: softmax scaling\n",
    "    is_causal: whether to use causal mask\n",
    "    dropout_p: dropout probability\n",
    "    \n",
    "    return: (batch, seq_len, dim)\n",
    "'''\n",
    "\n",
    "def flash_attention(Q, K, V, M = 100_000, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(b, N)\n",
    "    m = torch.ones(b, N) * -float('inf')\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = torch.matmul(Q_i, K_j.T) * tau\n",
    "                \n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "                m_ij = torch.max(S_ij, dim=1).values\n",
    "                S_ij = S_ij - m_ij.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.exp(S_ij)\n",
    "                # P_ij = torch.exp(torch.where(S_ij - m_ij.unsqueeze(1) != float('nan'), ))\n",
    "                l_ij = torch.sum(P_ij, dim=1)                \n",
    "                m_i_new = torch.max(m_i, m_ij)\n",
    "                l_i_new = torch.exp(m_i - m_i_new) * l_i + torch.exp(m_ij - m_i_new) * l_ij\n",
    "                if dropout_p > 0:\n",
    "                    P_ij = F.dropout(P_ij, p = dropout_p, training = True)\n",
    "                \n",
    "                O_i = torch.diag(1 / l_i_new) @ (torch.diag(l_i * torch.exp(m_i - m_i_new)) @ O_i\n",
    "                                                 + torch.diag(torch.exp(m_ij - m_i_new)) @ torch.matmul(P_ij, V_j))\n",
    "\n",
    "                O[b_i, N_r:N_r + Br] = O_i\n",
    "                l[b_i, N_r:N_r + Br] = l_i_new\n",
    "                m[b_i, N_r:N_r + Br] = m_i_new\n",
    "               \n",
    "    return O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6445, -1.4075,  1.9288,  0.7326,  0.8834,  0.8957, -0.1964,  0.8984,\n",
      "         0.5274, -0.9962,  1.7195, -1.2960, -0.6398,  1.2130, -0.0071,  1.2505,\n",
      "        -0.0959,  0.2520,  0.6756, -1.5047,  1.3196, -0.3428, -0.6408, -0.1185,\n",
      "         0.5393,  0.1485, -0.8046,  1.1012,  0.0594, -0.7007])\n",
      "tensor([-1.6445, -1.4075,  1.9288,  0.7326,  0.8834,  0.8957, -0.1964,  0.8984,\n",
      "         0.5274, -0.9962,  1.7195, -1.2960, -0.6398,  1.2130, -0.0071,  1.2505,\n",
      "        -0.0959,  0.2520,  0.6756, -1.5047,  1.3196, -0.3428, -0.6408, -0.1185,\n",
      "         0.5393,  0.1485, -0.8046,  1.1012,  0.0594, -0.7007])\n",
      "Total difference: 0.31099173426628113\n",
      "True\n",
      "tensor([-1.8273, -1.5639,  2.1431,  0.8140,  0.9816,  0.9952, -0.2182,  0.9982,\n",
      "         0.5859, -1.1069,  1.9106, -1.4400, -0.7109,  1.3477, -0.0079,  1.3894,\n",
      "        -0.1065,  0.2800,  0.7506, -1.6719,  1.4662, -0.3809, -0.7120, -0.1317,\n",
      "         0.5992,  0.1650, -0.8940,  1.2236,  0.0660, -0.7786])\n",
      "CPU times: user 58.6 s, sys: 109 ms, total: 58.7 s\n",
      "Wall time: 5.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "## Test\n",
    "\n",
    "b, n, d = 16, 1024, 512\n",
    "# b, n, d = 8, 128, 32\n",
    "\n",
    "q=torch.randn(b, n, d)\n",
    "k=torch.randn(b, n, d)\n",
    "v=torch.randn(b, n, d)\n",
    "\n",
    "# o_a = flash_attention(q, k, v, tau=1/np.sqrt(d))\n",
    "o_a = flash_attention(q, k, v, tau=1/np.sqrt(d), is_causal = True)\n",
    "\n",
    "# o = torch.softmax(q @ k.transpose(1,2) / np.sqrt(d), dim=-1) @ v\n",
    "s = (q @ k.transpose(1,2)) / np.sqrt(d)\n",
    "s = torch.where(torch.tril(torch.ones_like(s)) != 0, s, torch.tensor(-float('inf')))  # apply causal mask\n",
    "p = torch.softmax(s, dim=-1)\n",
    "o = p @ v\n",
    "\n",
    "print(o_a[0][0][:30])\n",
    "print(o[0][0][:30])\n",
    "print(f'Total difference: {torch.sum(torch.abs(o_a - o))}')\n",
    "print(o_a.allclose(o, atol=1e-4))\n",
    "\n",
    "o_a = flash_attention(q, k, v, tau=1/np.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(o_a[0][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 4.8389e-01,  8.9228e-01, -8.7868e-01,  5.4902e-01,  2.2318e-01,\n",
      "        -6.5148e-01,  1.0080e+00,  8.6370e-01, -4.8014e-01, -2.0496e-01,\n",
      "         2.2228e+00,  1.4096e+00, -1.2294e-01, -2.4882e-01, -1.9225e-01,\n",
      "         1.1560e+00, -5.0234e-02,  3.0937e-01, -3.8720e-01,  1.2193e-03,\n",
      "        -8.5482e-03,        -inf,        -inf,        -inf,        -inf,\n",
      "               -inf,        -inf,        -inf,        -inf,        -inf]) tensor([ 4.8389e-01,  8.9228e-01, -8.7868e-01,  5.4902e-01,  2.2318e-01,\n",
      "        -6.5148e-01,  1.0080e+00,  8.6370e-01, -4.8014e-01, -2.0496e-01,\n",
      "         2.2228e+00,  1.4096e+00, -1.2294e-01, -2.4882e-01, -1.9225e-01,\n",
      "         1.1560e+00, -5.0234e-02,  3.0937e-01, -3.8720e-01,  1.2193e-03,\n",
      "        -8.5482e-03,  7.2774e-01,  1.3160e-01,  1.2285e+00, -2.4422e-01,\n",
      "         4.7846e-01, -5.2039e-01, -7.3353e-01, -1.6904e+00,  1.0902e+00])\n"
     ]
    }
   ],
   "source": [
    "N = 1024\n",
    "A = torch.randn(N, N)\n",
    "B = A.clone()\n",
    "\n",
    "Br=2\n",
    "Bc=2\n",
    "Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "for j in range(Tc):\n",
    "    N_c = Bc * j\n",
    "    for i in range(Tr):                \n",
    "        N_r = Br * i\n",
    "        S_ij = A[N_r:N_r + Br, N_c:N_c + Bc]\n",
    "        \n",
    "        r_max = N_c + Bc - N_r - 1\n",
    "        i_start = Bc - r_max\n",
    "        for r in range(min(r_max, Br)):\n",
    "            S_ij[r, max(i_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "print(A[20][:30], B[20][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
