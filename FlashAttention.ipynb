{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "\n",
    "- To do:\n",
    "  - Swith Q and KV loops (seems clearer)\n",
    "  - Try [FlashDecoding++](https://arxiv.org/abs/2311.01282) aynchronized softmax ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Q: (batch, seq_len, dim)\n",
    "    K: (batch, seq_len, dim)\n",
    "    V: (batch, seq_len, dim)\n",
    "    tau: softmax scaling\n",
    "    is_causal: whether to use causal mask\n",
    "    dropout_p: dropout probability\n",
    "    \n",
    "    return: (batch, seq_len, dim)\n",
    "'''\n",
    "\n",
    "def flash_attention(Q, K, V, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(b, N)\n",
    "    m = torch.ones(b, N) * -float('inf')\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = tau * (Q_i @ K_j.T)\n",
    "                \n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "                m_ij = torch.max(S_ij, dim=1).values\n",
    "                S_ij = S_ij - m_ij.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.exp(S_ij)\n",
    "                # P_ij = torch.exp(torch.where(S_ij - m_ij.unsqueeze(1) != float('nan'), ))\n",
    "                l_ij = torch.sum(P_ij, dim=1)\n",
    "                \n",
    "                m_i_new = torch.max(m_i, m_ij)\n",
    "                l_i_new = torch.exp(m_i - m_i_new) * l_i + torch.exp(m_ij - m_i_new) * l_ij\n",
    "                if dropout_p > 0:\n",
    "                    torch.manual_seed(seed)\n",
    "                    P_ij = F.dropout(P_ij, p = dropout_p, training = True)\n",
    "                \n",
    "                O_i = torch.diag(1 / l_i_new) @ (torch.diag(l_i * torch.exp(m_i - m_i_new)) @ O_i\n",
    "                                                 + torch.diag(torch.exp(m_ij - m_i_new)) @ (P_ij @ V_j))\n",
    "\n",
    "                O[b_i, N_r:N_r + Br] = O_i\n",
    "                l[b_i, N_r:N_r + Br] = l_i_new\n",
    "                m[b_i, N_r:N_r + Br] = m_i_new\n",
    "               \n",
    "    return O, l, m\n",
    "\n",
    "def attention(Q, K, V, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    S = tau * (Q @ K.transpose(1,2))\n",
    "    if is_causal:\n",
    "        S = torch.where(torch.tril(torch.ones_like(S)) != 0, S, torch.tensor(-float('inf')))  # apply causal mask\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    if dropout_p > 0:\n",
    "        P = F.dropout(P, p = dropout_p, training = True)\n",
    "\n",
    "    O = P @ V\n",
    "\n",
    "    return O, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0093, -0.1454,  1.0259, -0.1156, -0.2714])\n",
      "tensor([-0.0093, -0.1454,  1.0259, -0.1156, -0.2714])\n",
      "Total difference: 0.0009422217262908816\n",
      "True\n",
      "tensor([-0.0104, -0.1616,  1.1398, -0.1285, -0.3015])\n",
      "CPU times: user 393 ms, sys: 2.33 ms, total: 396 ms\n",
      "Wall time: 57.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import math\n",
    "\n",
    "# b, n, d = 16, 1024, 512\n",
    "b, n, d = 8, 128, 32\n",
    "\n",
    "Q = torch.randn(b, n, d)\n",
    "K = torch.randn(b, n, d)\n",
    "V = torch.randn(b, n, d)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/math.sqrt(d), is_causal = True)\n",
    "O_flash, _, _ = flash_attention(Q, K, V, tau=1/math.sqrt(d), is_causal = True)\n",
    "\n",
    "\n",
    "print(O[0][0][:5])\n",
    "print(O_flash[0][0][:5])\n",
    "print(f'Total difference: {torch.sum(torch.abs(O_flash - O))}')\n",
    "print(O_flash.allclose(O, atol=1e-4))\n",
    "\n",
    "O_flash, _, _ = flash_attention(Q, K, V, tau=1/math.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(O_flash[0][0][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_grad(Q, K, V, O, dO, l, m, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "    \n",
    "    dQ = torch.zeros_like(Q)\n",
    "    dK = torch.zeros_like(K)\n",
    "    dV = torch.zeros_like(V)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            dK_j = dK[b_i,N_c:N_c + Bc]\n",
    "            dV_j = dV[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                dQ_i = dQ[b_i, N_r:N_r + Br]\n",
    "                dO_i = dO[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = tau * (Q_i @ K_j.T)\n",
    "\n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "                \n",
    "                S_ij = S_ij - m_i.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.diag(1 / l_i) @ torch.exp(S_ij)\n",
    "                \n",
    "                Z_ij = torch.ones_like(P_ij)\n",
    "                if dropout_p > 0:\n",
    "                    torch.manual_seed(seed)\n",
    "                    Z_ij = F.dropout(Z_ij, p = dropout_p, training = True)\n",
    "                    P_ij = Z_ij * P_ij\n",
    "\n",
    "                dV_j += P_ij.T @ dO_i\n",
    "\n",
    "                dP_ij = (dO_i @ V_j.T) * Z_ij\n",
    "                D_i = torch.sum(dO_i * O_i, dim=1)\n",
    "                dS_ij = P_ij * (dP_ij - D_i.unsqueeze(1))\n",
    "                \n",
    "                dQ[b_i, N_r:N_r + Br] = dQ_i + tau * (dS_ij @ K_j )\n",
    "                \n",
    "                dK_j += tau * (dS_ij.T @ Q_i)\n",
    "            \n",
    "        dK[b_i,N_c:N_c + Bc] = dK_j\n",
    "        dV[b_i,N_c:N_c + Bc] = dV_j\n",
    "\n",
    "    return dQ, dK, dV\n",
    "\n",
    "\n",
    "def attention_grad(Q, K, V, P, dO, tau = 1):\n",
    "    dV = P.transpose(1,2) @ dO\n",
    "    dP = dO @ V.transpose(1, 2)\n",
    "    dS = P * (dP - (P * dP).sum(dim=-1).unsqueeze(-1))\n",
    "    # dS = torch.zeros_like(dP)\n",
    "    # for i in range(dP.shape[1]):\n",
    "    #     for j in range(dP.shape[2]):\n",
    "    #         dS[:, i, j] = P[:, i, j] * (dP[:, i, j] - (P[:,i] * dP[:,i]).sum(dim=1))\n",
    "    dQ = tau * dS @ K\n",
    "    dK = tau * dS.transpose(1,2) @ Q\n",
    "\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--dQ--\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0.])\n",
      "tensor([ 4.1220e-08, -2.5450e-08,  7.2401e-08,  1.1534e-09,  1.0810e-07])\n",
      "--dK--\n",
      "tensor([-0.0022,  0.0445,  0.0121,  0.0750, -0.0995])\n",
      "tensor([-0.0022,  0.0445,  0.0121,  0.0750, -0.0995])\n",
      "tensor([-0.0022,  0.0445,  0.0121,  0.0750, -0.0995])\n",
      "--dV--\n",
      "tensor([ 0.0896,  0.1473, -0.0259, -0.0246,  0.0578])\n",
      "tensor([ 0.0896,  0.1473, -0.0259, -0.0246,  0.0578])\n",
      "tensor([ 0.0896,  0.1473, -0.0259, -0.0246,  0.0578])\n",
      "True True\n",
      "True True\n",
      "True True\n",
      "\n",
      "tensor([-0.0443,  0.0274, -0.0779, -0.0012, -0.1163])\n",
      "tensor([-0.1187,  0.0812,  0.0141,  0.0924, -0.0488])\n",
      "tensor([ 0.0874,  0.1377, -0.0279,  0.0033,  0.0627])\n",
      "CPU times: user 610 ms, sys: 13.5 ms, total: 623 ms\n",
      "Wall time: 72.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import math\n",
    "\n",
    "torch.manual_seed(0)\n",
    "# b, n, d = 16, 1024, 512\n",
    "b, n, d = 8, 128, 32\n",
    "\n",
    "Q = torch.randn(b, n, d, requires_grad=True)\n",
    "K = torch.randn(b, n, d, requires_grad=True)\n",
    "V = torch.randn(b, n, d, requires_grad=True)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/math.sqrt(d), is_causal = True)\n",
    "O_flash, l_flash, m_flash = flash_attention(Q, K, V, tau = 1/math.sqrt(d), is_causal = True)\n",
    "dO = O * 0.1\n",
    "\n",
    "O.backward(dO)\n",
    "with torch.no_grad():\n",
    "    dQ, dK, dV = attention_grad(Q, K, V, P, dO, tau=1/math.sqrt(d))\n",
    "    dQ_flash, dK_flash, dV_flash = flash_attention_grad(Q, K, V, O, dO, l_flash, m_flash, tau = 1/math.sqrt(d), is_causal = True)\n",
    "\n",
    "print(f'--dQ--\\n{dQ[0][0][:5]}\\n{Q.grad[0][0][:5]}\\n{dQ_flash[0][0][:5]}')\n",
    "print(f'--dK--\\n{dK[0][0][:5]}\\n{K.grad[0][0][:5]}\\n{dK_flash[0][0][:5]}')\n",
    "print(f'--dV--\\n{dV[0][0][:5]}\\n{V.grad[0][0][:5]}\\n{dV_flash[0][0][:5]}')\n",
    "print(dQ.allclose(dQ_flash, atol = 1e-4), Q.grad.allclose(dQ_flash, atol = 1e-4))\n",
    "print(dK.allclose(dK_flash, atol = 1e-4), K.grad.allclose(dK_flash, atol = 1e-4))\n",
    "print(dV.allclose(dV_flash, atol = 1e-4), V.grad.allclose(dV_flash, atol = 1e-4))\n",
    "\n",
    "with torch.no_grad():\n",
    "    dQ_flash, dK_flash, dV_flash = flash_attention_grad(Q, K, V, O, dO, l_flash, m_flash, tau = 1/math.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(f'\\n{dQ_flash[0][0][:5]}\\n{dK_flash[0][0][:5]}\\n{dV_flash[0][0][:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
