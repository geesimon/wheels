{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Q: (batch, seq_len, dim)\n",
    "    K: (batch, seq_len, dim)\n",
    "    V: (batch, seq_len, dim)\n",
    "    tau: softmax scaling\n",
    "    is_causal: whether to use causal mask\n",
    "    dropout_p: dropout probability\n",
    "    \n",
    "    return: (batch, seq_len, dim)\n",
    "'''\n",
    "\n",
    "def flash_attention(Q, K, V, M = 100_000, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(b, N)\n",
    "    m = torch.ones(b, N) * -float('inf')\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = torch.matmul(Q_i, K_j.T) * tau\n",
    "                \n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "                m_ij = torch.max(S_ij, dim=1).values\n",
    "                S_ij = S_ij - m_ij.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.exp(S_ij)\n",
    "                # P_ij = torch.exp(torch.where(S_ij - m_ij.unsqueeze(1) != float('nan'), ))\n",
    "                l_ij = torch.sum(P_ij, dim=1)                \n",
    "                m_i_new = torch.max(m_i, m_ij)\n",
    "                l_i_new = torch.exp(m_i - m_i_new) * l_i + torch.exp(m_ij - m_i_new) * l_ij\n",
    "                if dropout_p > 0:\n",
    "                    P_ij = F.dropout(P_ij, p = dropout_p, training = True)\n",
    "                \n",
    "                O_i = torch.diag(1 / l_i_new) @ (torch.diag(l_i * torch.exp(m_i - m_i_new)) @ O_i\n",
    "                                                 + torch.diag(torch.exp(m_ij - m_i_new)) @ torch.matmul(P_ij, V_j))\n",
    "\n",
    "                O[b_i, N_r:N_r + Br] = O_i\n",
    "                l[b_i, N_r:N_r + Br] = l_i_new\n",
    "                m[b_i, N_r:N_r + Br] = m_i_new\n",
    "               \n",
    "    return O\n",
    "\n",
    "def attention(Q, K, V, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    S = (Q @ K.transpose(1,2)) * tau\n",
    "    if is_causal:\n",
    "        S = torch.where(torch.tril(torch.ones_like(S)) != 0, S, torch.tensor(-float('inf')))  # apply causal mask\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    if dropout_p > 0:\n",
    "        P = F.dropout(P, p = dropout_p, training = True)\n",
    "\n",
    "    O = P @ V\n",
    "\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4263, -0.8752,  1.2028,  0.3551,  0.4692, -0.6110, -0.0530,  0.4354,\n",
      "        -0.3878, -1.3846,  0.2785,  0.4763, -0.0973,  0.2934,  0.0785, -1.0716,\n",
      "        -0.0894, -0.1857, -0.7917,  0.3861,  0.2993,  1.3189,  1.6312, -0.7639,\n",
      "        -0.4453,  0.5761,  0.0612,  1.8182,  0.4532, -0.5557])\n",
      "tensor([ 1.4263, -0.8752,  1.2028,  0.3551,  0.4692, -0.6110, -0.0530,  0.4354,\n",
      "        -0.3878, -1.3846,  0.2785,  0.4763, -0.0973,  0.2934,  0.0785, -1.0716,\n",
      "        -0.0894, -0.1857, -0.7917,  0.3861,  0.2993,  1.3189,  1.6312, -0.7639,\n",
      "        -0.4453,  0.5761,  0.0612,  1.8182,  0.4532, -0.5557])\n",
      "Total difference: 0.3083489239215851\n",
      "True\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0.])\n",
      "CPU times: user 1min 3s, sys: 380 ms, total: 1min 3s\n",
      "Wall time: 6.48 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "## Test\n",
    "\n",
    "b, n, d = 16, 1024, 512\n",
    "# b, n, d = 8, 128, 32\n",
    "\n",
    "q=torch.randn(b, n, d)\n",
    "k=torch.randn(b, n, d)\n",
    "v=torch.randn(b, n, d)\n",
    "\n",
    "o_a = flash_attention(q, k, v, tau=1/np.sqrt(d), is_causal = True)\n",
    "o = attention(q, k, v, tau=1/np.sqrt(d), is_causal = True)\n",
    "\n",
    "print(o_a[0][0][:30])\n",
    "print(o[0][0][:30])\n",
    "print(f'Total difference: {torch.sum(torch.abs(o_a - o))}')\n",
    "print(o_a.allclose(o, atol=1e-4))\n",
    "\n",
    "o_a = flash_attention(q, k, v, tau=1/np.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(o_a[0][0][:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_grad():\n",
    "    pass\n",
    "\n",
    "def attention_grad():\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
