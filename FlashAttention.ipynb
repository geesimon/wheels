{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Q: (batch, seq_len, dim)\n",
    "    K: (batch, seq_len, dim)\n",
    "    V: (batch, seq_len, dim)\n",
    "    tau: softmax scaling\n",
    "    is_causal: whether to use causal mask\n",
    "    dropout_p: dropout probability\n",
    "    \n",
    "    return: (batch, seq_len, dim)\n",
    "'''\n",
    "\n",
    "def flash_attention(Q, K, V, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    O = torch.zeros_like(Q)\n",
    "    l = torch.zeros(b, N)\n",
    "    m = torch.ones(b, N) * -float('inf')\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = tau * (Q_i @ K_j.T)\n",
    "                \n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "\n",
    "                m_ij = torch.max(S_ij, dim=1).values\n",
    "                S_ij = S_ij - m_ij.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.exp(S_ij)\n",
    "                # P_ij = torch.exp(torch.where(S_ij - m_ij.unsqueeze(1) != float('nan'), ))\n",
    "                l_ij = torch.sum(P_ij, dim=1)\n",
    "                \n",
    "                m_i_new = torch.max(m_i, m_ij)\n",
    "                l_i_new = torch.exp(m_i - m_i_new) * l_i + torch.exp(m_ij - m_i_new) * l_ij\n",
    "                if dropout_p > 0:\n",
    "                    torch.manual_seed(seed)\n",
    "                    P_ij = F.dropout(P_ij, p = dropout_p, training = True)\n",
    "                \n",
    "                O_i = torch.diag(1 / l_i_new) @ (torch.diag(l_i * torch.exp(m_i - m_i_new)) @ O_i\n",
    "                                                 + torch.diag(torch.exp(m_ij - m_i_new)) @ (P_ij @ V_j))\n",
    "\n",
    "                O[b_i, N_r:N_r + Br] = O_i\n",
    "                l[b_i, N_r:N_r + Br] = l_i_new\n",
    "                m[b_i, N_r:N_r + Br] = m_i_new\n",
    "               \n",
    "    return O, l, m\n",
    "\n",
    "def attention(Q, K, V, tau = 1, dropout_p = 0, is_causal = False):\n",
    "    S = (Q @ K.transpose(1,2)) * tau\n",
    "    if is_causal:\n",
    "        S = torch.where(torch.tril(torch.ones_like(S)) != 0, S, torch.tensor(-float('inf')))  # apply causal mask\n",
    "    P = torch.softmax(S, dim=-1)\n",
    "    if dropout_p > 0:\n",
    "        P = F.dropout(P, p = dropout_p, training = True)\n",
    "\n",
    "    O = P @ V\n",
    "\n",
    "    return O, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0955,  0.6573, -1.4128, -0.6750,  0.7921, -0.4240,  0.4224, -1.4545,\n",
      "        -0.2952,  0.8288])\n",
      "tensor([ 0.0955,  0.6573, -1.4128, -0.6750,  0.7921, -0.4240,  0.4224, -1.4545,\n",
      "        -0.2952,  0.8288])\n",
      "Total difference: 0.30684229731559753\n",
      "True\n",
      "tensor([[ 0.1061,  0.7303, -1.5698,  ...,  0.4672,  1.0930,  1.3335],\n",
      "        [-0.1154,  0.1522, -0.3652,  ...,  0.0652,  0.7005,  0.2060],\n",
      "        [ 0.0710,  0.2778, -0.9313,  ...,  0.3003,  0.4251,  0.4335],\n",
      "        ...,\n",
      "        [ 1.1259,  0.9318, -1.2136,  ...,  0.5742, -0.5662,  0.3515],\n",
      "        [ 0.7906, -0.4637, -0.4523,  ...,  0.0695, -0.3271, -0.2006],\n",
      "        [ 0.8441,  0.4677, -0.6430,  ...,  0.1067, -0.3857,  0.2001]])\n",
      "CPU times: user 1min 10s, sys: 419 ms, total: 1min 11s\n",
      "Wall time: 7.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "b, n, d = 16, 1024, 512\n",
    "# b, n, d = 8, 128, 32\n",
    "\n",
    "Q = torch.randn(b, n, d)\n",
    "K = torch.randn(b, n, d)\n",
    "V = torch.randn(b, n, d)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True)\n",
    "O_flash, l, m = flash_attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True)\n",
    "\n",
    "\n",
    "print(O[0][0][:10])\n",
    "print(O_flash[0][0][:10])\n",
    "print(f'Total difference: {torch.sum(torch.abs(O_flash - O))}')\n",
    "print(O_flash.allclose(O, atol=1e-4))\n",
    "\n",
    "O_flash = flash_attention(Q, K, V, tau=1/np.sqrt(d), is_causal = True, dropout_p = 0.1)\n",
    "print(O_flash[0][0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flash_attention_grad(Q, K, V, O, dO, l, m, M = 100_000, tau = 1, is_causal = False, dropout_p = 0, seed = 1234):\n",
    "    b, N, d = Q.shape\n",
    "    Bc = M // (4 * d)\n",
    "    Br = min(Bc,d)\n",
    "    Tr = N // Br + (1 if N % Br != 0 else 0)\n",
    "    Tc = N // Bc + (1 if N % Bc != 0 else 0)\n",
    "    \n",
    "    dQ = torch.zeros_like(Q)\n",
    "    dK = torch.zeros_like(K)\n",
    "    dV = torch.zeros_like(V)\n",
    "\n",
    "    for b_i in range(b):\n",
    "        for j in range(Tc):\n",
    "            N_c = Bc * j\n",
    "            K_j = K[b_i,N_c:N_c + Bc]\n",
    "            V_j = V[b_i,N_c:N_c + Bc]\n",
    "            dK_j = dK[b_i,N_c:N_c + Bc]\n",
    "            dV_j = dV[b_i,N_c:N_c + Bc]\n",
    "            \n",
    "            for i in range(Tr):\n",
    "                N_r = Br * i\n",
    "                Q_i = Q[b_i, N_r:N_r + Br]\n",
    "                O_i = O[b_i, N_r:N_r + Br]\n",
    "                dQ_i = dQ[b_i, N_r:N_r + Br]\n",
    "                dO_i = dO[b_i, N_r:N_r + Br]\n",
    "                l_i = l[b_i, N_r:N_r + Br]\n",
    "                m_i = m[b_i, N_r:N_r + Br]\n",
    "\n",
    "                S_ij = tau * (Q_i @ K_j.T)\n",
    "\n",
    "                if is_causal:\n",
    "                    # Apply causal mask\n",
    "                    r_max = N_c + Bc - N_r - 1\n",
    "                    c_start = Bc - r_max\n",
    "                    for r in range(min(r_max, S_ij.shape[0])):\n",
    "                        S_ij[r, max(c_start + r, 0):] = torch.tensor(-float('inf'))\n",
    "                \n",
    "                S_ij = S_ij - m_i.unsqueeze(1)\n",
    "                # Need to make sure that the exp is not nan, value of nan could be caused by applying causal mask\n",
    "                S_ij[S_ij.isnan()] = 0\n",
    "                P_ij = torch.diag(1 / l_i) @ torch.exp(S_ij)\n",
    "                \n",
    "                Z_ij = torch.ones_like(P_ij)\n",
    "                if dropout_p > 0:\n",
    "                    torch.manual_seed(seed)\n",
    "                    Z_ij = F.dropout(Z_ij, p = dropout_p, training = True)\n",
    "                    P_ij = Z_ij * P_ij\n",
    "\n",
    "                dV_j += P_ij.T @ dO_i\n",
    "\n",
    "                dP_ij = (dO_i @ V_j.T) * Z_ij\n",
    "                D_i = torch.sum(dO_i * O_i, dim=1)\n",
    "                dS_ij = P_ij * (dP_ij - D_i.unsqueeze(1))\n",
    "                \n",
    "                dQ[b_i, N_r:N_r + Br] = dQ_i + tau * (dS_ij @ K_j )\n",
    "                \n",
    "                dK_j += tau * (dS_ij.T @ Q_i)\n",
    "            \n",
    "        dK[b_i,N_c:N_c + Bc] = dK_j\n",
    "        dV[b_i,N_c:N_c + Bc] = dV_j\n",
    "\n",
    "\n",
    "    return dQ, dK, dV\n",
    "\n",
    "\n",
    "def attention_grad(Q, K, V, P, dO):\n",
    "    dV = P.transpose(1,2) @ dO\n",
    "    dP = dO @ V.transpose(1, 2)\n",
    "    dS = P * (dP - (P * dP).sum(dim=-1).unsqueeze(-1))\n",
    "    # dS = torch.zeros_like(dP)\n",
    "    # for i in range(dP.shape[1]):\n",
    "    #     for j in range(dP.shape[2]):\n",
    "    #         dS[:, i, j] = P[:, i, j] * (dP[:, i, j] - (P[:,i] * dP[:,i]).sum(dim=1))       \n",
    "    dQ = dS @ K\n",
    "    dK = dS.transpose(1,2) @ Q\n",
    "\n",
    "    return dQ, dK, dV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([-0.0055,  0.0057, -0.0039, -0.0091,  0.0157, -0.0019, -0.0066, -0.0072,\n",
      "         0.0027, -0.0008]) tensor([-0.0010,  0.0010, -0.0007, -0.0016,  0.0028, -0.0003, -0.0012, -0.0013,\n",
      "         0.0005, -0.0001])\n",
      "tensor([-0.0296, -0.0018, -0.0439, -0.0665, -0.0246,  0.0578, -0.0130, -0.1125,\n",
      "         0.0191, -0.0237]) tensor([-0.0052, -0.0003, -0.0078, -0.0118, -0.0043,  0.0102, -0.0023, -0.0199,\n",
      "         0.0034, -0.0042])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "# b, n, d = 16, 1024, 512\n",
    "b, n, d = 8, 128, 32\n",
    "\n",
    "Q = torch.randn(b, n, d)\n",
    "K = torch.randn(b, n, d)\n",
    "V = torch.randn(b, n, d)\n",
    "\n",
    "O, P = attention(Q, K, V, tau=1/np.sqrt(d), is_causal = False)\n",
    "O_flash, l_flash, m_flash = flash_attention(Q, K, V, tau=1/np.sqrt(d), is_causal = False)\n",
    "print(O_flash.allclose(O, atol=1e-4))\n",
    "\n",
    "dO = O * 0.1\n",
    "# dO = torch.randn(b, n, d)\n",
    "\n",
    "dQ, dK, dV = attention_grad(Q, K, V, P, dO)\n",
    "dQ_flash, dK_flash, dV_flash = flash_attention_grad(Q, K, V, O, dO, l_flash, m_flash, tau=1/np.sqrt(d), is_causal = False)\n",
    "print(dQ[0][0][:10], dQ_flash[0][0][:10])\n",
    "print(dK[0][0][:10], dK_flash[0][0][:10])\n",
    "print(dV.allclose(dV_flash, atol=1e-4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
