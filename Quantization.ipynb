{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -q\n",
    "%pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize_i8(X: torch.Tensor):\n",
    "    absmax = torch.max(torch.abs(X))\n",
    "    X_i8 = ((X * 127) / absmax).to(torch.int8)\n",
    "    return X_i8, X_i8.to(torch.float32) * absmax / 127\n",
    "\n",
    "def zeropoint_quantize_i8(X: torch.Tensor):\n",
    "    r = torch.max(X) - torch.min(X)\n",
    "    r = 1 if r == 0 else r\n",
    "    scale = 255 / r\n",
    "\n",
    "    zeropoint = (-scale * torch.min(X) - 128)\n",
    "    X_i8 =  (X * scale + zeropoint).round().to(torch.int8)\n",
    "    \n",
    "    return X_i8, (X_i8 - zeropoint) / scale\n",
    "\n",
    "# def absmax_quantize(X):\n",
    "#     # Calculate scale\n",
    "#     scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "#     # Quantize\n",
    "#     X_quant = (scale * X).round()\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = X_quant / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# def zeropoint_quantize(X):\n",
    "#     # Calculate value range (denominator)\n",
    "#     x_range = torch.max(X) - torch.min(X)\n",
    "#     x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "#     # Calculate scale\n",
    "#     scale = 255 / x_range\n",
    "\n",
    "#     # Shift by zero-point\n",
    "#     zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "#     # Scale and round the inputs\n",
    "#     X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def zp_mul(A, B):\n",
    "    # Calculate value range (denominator)\n",
    "    a_range = torch.max(A) - torch.min(A)\n",
    "    b_range = torch.max(B) - torch.min(B)\n",
    "    a_range = 1 if a_range == 0 else a_range\n",
    "    b_range = 1 if b_range == 0 else b_range\n",
    "    \n",
    "    # Calculate scale\n",
    "    a_scale = 255 / a_range\n",
    "    b_scale = 255 / b_range\n",
    "    c_scale = a_scale * b_scale\n",
    "\n",
    "    # Shift by zero-point\n",
    "    a_zp = (-a_scale * torch.min(A) - 128).round()\n",
    "    b_zp = (-b_scale * torch.min(B) - 128).round()\n",
    "    c_zp = a_zp * b_zp\n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    A_quant = torch.clip((A * a_scale + a_zp).round(), -128, 127).to(torch.int8)\n",
    "    B_quant = torch.clip((B * b_scale + b_zp).round(), -128, 127).to(torch.int8)    \n",
    "    \n",
    "    # print(f'c_scale:{c_scale}, c_zp:{c_zp}')\n",
    "    # print(f'A_quant:{A_quant}, B_quant:{B_quant}')\n",
    "    # Multiply\n",
    "    C_quant = (A_quant.to(torch.int16) * B_quant.to(torch.int16)) + c_zp #- A_quant * b_zp.to(torch.float32) - B_quant * a_zp.to(torch.float32) \n",
    "    # print(f'c_quant:{C_quant}')\n",
    "    C = C_quant / c_scale\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vector_abs_i8(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    A_scale = 127 / torch.max(torch.abs(A), dim=1).values\n",
    "    B_scale = 127 / torch.max(torch.abs(B), dim=0).values\n",
    "    C_scale = torch.matmul(A_scale.unsqueeze(1), B_scale.unsqueeze(0))\n",
    "\n",
    "    A_i8 = torch.clip((A  * A_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    B_i8 = torch.clip((B  * B_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    return torch.matmul(A_i8.to(torch.int32), B_i8.to(torch.int32)) / C_scale\n",
    "\n",
    "def LLM_matmul_abs_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "    W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)\n",
    "\n",
    "def LLM_matmul_zp_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    # Calculate value range (denominator)\n",
    "    X2_range = torch.max(X2, dim=1).values - torch.min(X2, dim=1).values\n",
    "    W2_range = torch.max(W2, dim=0).values - torch.min(W2, dim=0).values\n",
    "    \n",
    "    # Calculate scale\n",
    "    X2_scale = 255 / X2_range\n",
    "    W2_scale = 255 / W2_range\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    # Shift by zero-point\n",
    "    X2_zp = (-X2_scale * torch.min(X2, dim = 1).values - 128).round()\n",
    "    W2_zp = (-W2_scale * torch.min(W2, dim = 0).values - 128).round()\n",
    "    O_zp = torch.matmul(X2_zp.unsqueeze(1), W2_zp.unsqueeze(0))    \n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    X2_quant = torch.clip((X2 * X2_scale.unsqueeze(1) + X2_zp.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_quant = torch.clip((W2 * W2_scale.unsqueeze(0) + W2_zp.unsqueeze(0)).round(), -128, 127).to(torch.int8)   \n",
    "    O2_quant = (X2_quant.to(torch.int32) @ W2_quant.to(torch.int32)) \\\n",
    "                - X2_quant.to(X2) @ W2_zp.unsqueeze(0).expand(X2.shape[1], -1) \\\n",
    "                - X2_zp.unsqueeze(1).expand(-1, W2.shape[0]) @ W2_quant.to(W2) \\\n",
    "                + O_zp * X2.shape[1]\n",
    "    O2 = O2_quant / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 10.0%\n",
      "LLM.int8() absmax -> Acc: 66560.0, Avg: 0.265625\n",
      "Reserved 10.0%\n",
      "LLM.int8() zero-point -> Acc: 68096.0, Avg: 0.271484375\n",
      "tensor(68096., dtype=torch.bfloat16) tensor(0.2715, dtype=torch.bfloat16)\n",
      "int8 abs -> Acc: 69632.0, Avg: 0.279296875\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(500, 1000, dtype=torch.bfloat16)\n",
    "W = torch.randn(1000, 500, dtype=torch.bfloat16)\n",
    "X[0, 0: X.shape[1] // 10] = 6\n",
    "\n",
    "error = torch.abs(LLM_matmul_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() absmax -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "\n",
    "error = torch.abs(LLM_matmul_zp_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() zero-point -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(matmul_vector_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'int8 abs -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modle Quantization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/miniconda3/envs/wheels/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 510,342,192 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[-21, -11,  -4,  ...,   2,  -2,   1],\n",
      "        [  3,   6,  10,  ...,  -2,   0,   0],\n",
      "        [  0,   3,  16,  ...,   5,   1,  -1],\n",
      "        ...,\n",
      "        [-11,   0,   8,  ...,   0,  -2,   1],\n",
      "        [  6,   9,   4,  ...,   1,  -1,  -2],\n",
      "        [-18,  -8, -10,  ...,   0,   0,   0]], dtype=torch.int8) \n",
      " tensor([[-0.4702, -0.2463, -0.0896,  ...,  0.0448, -0.0448,  0.0224],\n",
      "        [ 0.0672,  0.1343,  0.2239,  ..., -0.0448,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0672,  0.3583,  ...,  0.1120,  0.0224, -0.0224],\n",
      "        ...,\n",
      "        [-0.2463,  0.0000,  0.1791,  ...,  0.0000, -0.0448,  0.0224],\n",
      "        [ 0.1343,  0.2015,  0.0896,  ...,  0.0224, -0.0224, -0.0448],\n",
      "        [-0.4030, -0.1791, -0.2239,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "Zero-point quantized weights:\n",
      "tensor([[-21, -11,  -4,  ...,   3,  -2,   2],\n",
      "        [  5,   7,  11,  ...,  -2,   0,   0],\n",
      "        [  1,   4,  17,  ...,   6,   2,  -1],\n",
      "        ...,\n",
      "        [-11,   0,  10,  ...,   1,  -2,   2],\n",
      "        [  7,  10,   5,  ...,   2,  -1,  -2],\n",
      "        [-18,  -8, -10,  ...,   0,   1,   1]], dtype=torch.int8) \n",
      " tensor([[-0.4774, -0.2562, -0.1014,  ...,  0.0534, -0.0572,  0.0313],\n",
      "        [ 0.0976,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0129],\n",
      "        [ 0.0092,  0.0755,  0.3630,  ...,  0.1197,  0.0313, -0.0351],\n",
      "        ...,\n",
      "        [-0.2562, -0.0129,  0.2082,  ...,  0.0092, -0.0572,  0.0313],\n",
      "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0313, -0.0351, -0.0572],\n",
      "        [-0.4110, -0.1899, -0.2341,  ..., -0.0129,  0.0092,  0.0092]])\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, weights_abs_dequant = absmax_quantize_i8(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant, '\\n', weights_abs_dequant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, weights_zp_dequant = zeropoint_quantize_i8(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant, '\\n', weights_zp_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream, but I'm scared I might fail.\"\n",
      "\n",
      "She did not say what she was scared of, but she certainly wasn't scared of what she will do if she tries any of these things. Even if it is just to\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream' to give a the. is the o in it h s to be\n",
      " is to s i g to s s o m th t o p th o sh ti t t t th s s o nt t th th\n",
      "--------------------------------------------------\n",
      "Zeropoint model:\n",
      "I have a dream of my life now and I wish we get an exam tomorrow night, I can tell you I am gonna make a career, that the dreams of my husband would be on the day that morning, I am gonna have a hard time\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  11.11\n",
      "Absmax perplexity:    55.95\n",
      "Zeropoint perplexity: 28.01\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation    \n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, zp_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Int8 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearInt8(nn.Module):\n",
    "    def __init__(self, weight: torch.tensor, bias = None):\n",
    "        super().__init__()\n",
    "        self.zp_quantize_weight_i8(weight)\n",
    "\n",
    "        self.bias = None if bias is None else bias.to(torch.bfloat16)\n",
    "\n",
    "    def zp_quantize_weight_i8(self, weight: torch.Tensor):\n",
    "        r = torch.max(weight, dim=0).values - torch.min(weight, dim=0).values\n",
    "        r = torch.where(r == 0, torch.tensor(1), r)        \n",
    "        \n",
    "        scale = 255 / r\n",
    "        zeropoint = (-scale * torch.min(weight, dim=0).values - 128)        \n",
    "\n",
    "        self.weight_i8 =  (weight * scale.unsqueeze(0) + zeropoint.unsqueeze(0)).round().to(torch.int8)\n",
    "\n",
    "        self.scale = scale.to(torch.bfloat16)\n",
    "        self.zeropoint = zeropoint.to(torch.bfloat16)\n",
    "    \n",
    "    def zp_dequantize_weight_i8(self) -> torch.Tensor:\n",
    "        return (self.weight_i8 - self.zeropoint.unsqueeze(0)) / self.scale.unsqueeze(0)\n",
    "\n",
    "    def LLM_matmul_abs_i8(self, X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "        X_bf16 = X.to(torch.bfloat16)\n",
    "\n",
    "        X_col_filter = torch.max(torch.abs(X_bf16), dim = -2).values > alpha\n",
    "        X1 = X_bf16[:, X_col_filter]\n",
    "        W1 = W[X_col_filter, :]\n",
    "        X2 = X_bf16[:, ~X_col_filter]\n",
    "        W2 = W[~X_col_filter, :]\n",
    "        \n",
    "        O1 = torch.matmul(X1, W1)\n",
    "        print(f'Reserved {(X1.shape[1] / X_bf16.shape[1] * 100):.1f}%')\n",
    "        X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "        W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "        O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "        X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "        W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "        O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "        \n",
    "        return (O1 + O2.to(O1)).to(X)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_flattened = X.view(-1, X.shape[-1])\n",
    "        ret = self.LLM_matmul_abs_i8(x_flattened, self.zp_dequantize_weight_i8())\n",
    "        if self.bias is not None:\n",
    "            ret += self.bias\n",
    "\n",
    "        return ret.view(X.shape[:-2] + (-1, ret.shape[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight avg err: 0.0013\n",
      "Reserved 0.0%\n",
      "Activation avg err: 0.0603\n"
     ]
    }
   ],
   "source": [
    "W = model.transformer.h[0].attn.c_attn.weight\n",
    "bias = model.transformer.h[0].attn.c_attn.bias\n",
    "X = torch.randn(2, model.transformer.h[0].attn.c_attn.weight.shape[0])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "print(f'Weight avg err: {torch.sum(torch.abs(l_i8.zp_dequantize_weight_i8() - W))/W.numel():.4f}')\n",
    "print(f'Activation avg err: { torch.sum(torch.abs(X @ W + bias - l_i8(X))) / (X.shape[0] * W.shape[1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[[-4.5625,  5.0469, -0.7942],\n",
       "           [-2.5664,  4.3281, -0.8362],\n",
       "           [-5.5781,  4.3438, -3.3203]],\n",
       " \n",
       "          [[-0.1094,  1.8750, -0.4785],\n",
       "           [-2.4062,  1.9531, -1.0488],\n",
       "           [ 4.0625, -1.6406,  1.6484]]],\n",
       " \n",
       " \n",
       "         [[[-0.9844,  1.0234,  0.8281],\n",
       "           [ 0.0547, -0.6250, -0.9351],\n",
       "           [-4.0625,  3.1992, -2.1328]],\n",
       " \n",
       "          [[-3.5859,  2.6387, -1.6992],\n",
       "           [-0.1484,  2.6738,  1.6172],\n",
       "           [-3.6406,  3.2695, -1.9219]]]]),\n",
       " tensor([[[[-4.5584,  5.0371, -0.7897],\n",
       "           [-2.5751,  4.3486, -0.8422],\n",
       "           [-5.5767,  4.3458, -3.3390]],\n",
       " \n",
       "          [[-0.0874,  1.8682, -0.4697],\n",
       "           [-2.4035,  1.9390, -1.0667],\n",
       "           [ 4.0971, -1.6320,  1.6712]]],\n",
       " \n",
       " \n",
       "         [[[-0.9914,  1.0345,  0.8459],\n",
       "           [ 0.0594, -0.6097, -0.9273],\n",
       "           [-4.0672,  3.1826, -2.1384]],\n",
       " \n",
       "          [[-3.5855,  2.6118, -1.7067],\n",
       "           [-0.1543,  2.6610,  1.6695],\n",
       "           [-3.6345,  3.2470, -1.9398]]]]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(5, 3)\n",
    "X = torch.randn(2, 2, 3, 5)\n",
    "bias = torch.randn(W.shape[-1])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "l_i8(X), X @ W + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "#                                              device_map='auto',\n",
    "#                                              load_in_8bit=True,\n",
    "#                                              )\n",
    "# print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
