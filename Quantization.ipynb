{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize_i8(X: torch.Tensor):\n",
    "    absmax = torch.max(torch.abs(X))\n",
    "    X_i8 = ((X * 127) / absmax).to(torch.int8)\n",
    "    return X_i8, X_i8.to(torch.float32) * absmax / 127\n",
    "\n",
    "def zeropoint_quantize_i8(X: torch.Tensor):\n",
    "    r = torch.max(X) - torch.min(X)\n",
    "    r = 1 if r == 0 else r\n",
    "    scale = 255 / r\n",
    "\n",
    "    zeropoint = (-scale * torch.min(X) - 128)\n",
    "    X_i8 =  (X * scale + zeropoint).round().to(torch.int8)\n",
    "    \n",
    "    return X_i8, (X_i8 - zeropoint) / scale\n",
    "\n",
    "def zeropoint_quantize(X):\n",
    "    # Calculate value range (denominator)\n",
    "    x_range = torch.max(X) - torch.min(X)\n",
    "    x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "    # Calculate scale\n",
    "    scale = 255 / x_range\n",
    "\n",
    "    # Shift by zero-point\n",
    "    zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "    # Scale and round the inputs\n",
    "    X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "    # Dequantize\n",
    "    X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "    return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def zp_mul(A, B):\n",
    "    # Calculate value range (denominator)\n",
    "    a_range = torch.max(A) - torch.min(A)\n",
    "    b_range = torch.max(B) - torch.min(B)\n",
    "    a_range = 1 if a_range == 0 else a_range\n",
    "    b_range = 1 if b_range == 0 else b_range\n",
    "    \n",
    "    # Calculate scale\n",
    "    a_scale = 255 / a_range\n",
    "    b_scale = 255 / b_range\n",
    "    c_scale = a_scale * b_scale\n",
    "\n",
    "    # Shift by zero-point\n",
    "    a_zp = (-a_scale * torch.min(A) - 128).round()\n",
    "    b_zp = (-b_scale * torch.min(B) - 128).round()\n",
    "    c_zp = a_zp * b_zp\n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    A_quant = torch.clip((A * a_scale + a_zp).round(), -128, 127).to(torch.int8)\n",
    "    B_quant = torch.clip((B * b_scale + b_zp).round(), -128, 127).to(torch.int8)    \n",
    "    \n",
    "    # print(f'c_scale:{c_scale}, c_zp:{c_zp}')\n",
    "    # print(f'A_quant:{A_quant}, B_quant:{B_quant}')\n",
    "    # Multiply\n",
    "    C_quant = (A_quant.to(torch.int16) * B_quant.to(torch.int16)) + c_zp #- A_quant * b_zp.to(torch.float32) - B_quant * a_zp.to(torch.float32) \n",
    "    # print(f'c_quant:{C_quant}')\n",
    "    C = C_quant / c_scale\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vector_abs_i8(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    A_scale = 127 / torch.max(torch.abs(A), dim=1).values\n",
    "    B_scale = 127 / torch.max(torch.abs(B), dim=0).values\n",
    "    C_scale = torch.matmul(A_scale.unsqueeze(1), B_scale.unsqueeze(0))\n",
    "\n",
    "    A_i8 = torch.clip((A  * A_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    B_i8 = torch.clip((B  * B_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    return torch.matmul(A_i8.to(torch.int32), B_i8.to(torch.int32)) / C_scale\n",
    "\n",
    "def LLM_matmul_abs_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "    W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)\n",
    "\n",
    "def LLM_matmul_zp_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    # Calculate value range (denominator)\n",
    "    X2_range = torch.max(X2, dim=1).values - torch.min(X2, dim=1).values\n",
    "    W2_range = torch.max(W2, dim=0).values - torch.min(W2, dim=0).values\n",
    "    \n",
    "    # Calculate scale\n",
    "    X2_scale = 255 / X2_range\n",
    "    W2_scale = 255 / W2_range\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    # Shift by zero-point\n",
    "    X2_zp = (-X2_scale * torch.min(X2, dim = 1).values - 128).round()\n",
    "    W2_zp = (-W2_scale * torch.min(W2, dim = 0).values - 128).round()\n",
    "    O_zp = torch.matmul(X2_zp.unsqueeze(1), W2_zp.unsqueeze(0))    \n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    X2_quant = torch.clip((X2 * X2_scale.unsqueeze(1) + X2_zp.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_quant = torch.clip((W2 * W2_scale.unsqueeze(0) + W2_zp.unsqueeze(0)).round(), -128, 127).to(torch.int8)   \n",
    "    O2_quant = (X2_quant.to(torch.int32) @ W2_quant.to(torch.int32)) \\\n",
    "                - X2_quant.to(X2) @ W2_zp.unsqueeze(0).expand(X2.shape[1], -1) \\\n",
    "                - X2_zp.unsqueeze(1).expand(-1, W2.shape[0]) @ W2_quant.to(W2) \\\n",
    "                + O_zp * X2.shape[1]\n",
    "    O2 = O2_quant / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 10.0%\n",
      "tensor(66560., dtype=torch.bfloat16) tensor(0.2656, dtype=torch.bfloat16)\n",
      "Reserved 10.0%\n",
      "tensor(68608., dtype=torch.bfloat16) tensor(0.2754, dtype=torch.bfloat16)\n",
      "tensor(69632., dtype=torch.bfloat16) tensor(0.2793, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(500, 1000, dtype=torch.bfloat16)\n",
    "W = torch.randn(1000, 500, dtype=torch.bfloat16)\n",
    "X[0, 0: X.shape[1] // 10] = 6\n",
    "\n",
    "error = torch.abs(LLM_matmul_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(LLM_matmul_zp_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(matmul_vector_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
