{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -q\n",
    "%pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize_i8(X: torch.Tensor):\n",
    "    absmax = torch.max(torch.abs(X))\n",
    "    X_i8 = ((X * 127) / absmax).to(torch.int8)\n",
    "    return X_i8, X_i8.to(torch.float32) * absmax / 127\n",
    "\n",
    "def zeropoint_quantize_i8(X: torch.Tensor):\n",
    "    r = torch.max(X) - torch.min(X)\n",
    "    r = 1 if r == 0 else r\n",
    "    scale = 255 / r\n",
    "\n",
    "    zeropoint = (-scale * torch.min(X) - 128)\n",
    "    X_i8 =  (X * scale + zeropoint).round().to(torch.int8)\n",
    "    \n",
    "    return X_i8, (X_i8 - zeropoint) / scale\n",
    "\n",
    "# def absmax_quantize(X):\n",
    "#     # Calculate scale\n",
    "#     scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "#     # Quantize\n",
    "#     X_quant = (scale * X).round()\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = X_quant / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# def zeropoint_quantize(X):\n",
    "#     # Calculate value range (denominator)\n",
    "#     x_range = torch.max(X) - torch.min(X)\n",
    "#     x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "#     # Calculate scale\n",
    "#     scale = 255 / x_range\n",
    "\n",
    "#     # Shift by zero-point\n",
    "#     zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "#     # Scale and round the inputs\n",
    "#     X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def zp_mul(A, B):\n",
    "    # Calculate value range (denominator)\n",
    "    a_range = torch.max(A) - torch.min(A)\n",
    "    b_range = torch.max(B) - torch.min(B)\n",
    "    a_range = 1 if a_range == 0 else a_range\n",
    "    b_range = 1 if b_range == 0 else b_range\n",
    "    \n",
    "    # Calculate scale\n",
    "    a_scale = 255 / a_range\n",
    "    b_scale = 255 / b_range\n",
    "    c_scale = a_scale * b_scale\n",
    "\n",
    "    # Shift by zero-point\n",
    "    a_zp = (-a_scale * torch.min(A) - 128).round()\n",
    "    b_zp = (-b_scale * torch.min(B) - 128).round()\n",
    "    c_zp = a_zp * b_zp\n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    A_quant = torch.clip((A * a_scale + a_zp).round(), -128, 127).to(torch.int8)\n",
    "    B_quant = torch.clip((B * b_scale + b_zp).round(), -128, 127).to(torch.int8)    \n",
    "    \n",
    "    # print(f'c_scale:{c_scale}, c_zp:{c_zp}')\n",
    "    # print(f'A_quant:{A_quant}, B_quant:{B_quant}')\n",
    "    # Multiply\n",
    "    C_quant = (A_quant.to(torch.int16) * B_quant.to(torch.int16)) + c_zp #- A_quant * b_zp.to(torch.float32) - B_quant * a_zp.to(torch.float32) \n",
    "    # print(f'c_quant:{C_quant}')\n",
    "    C = C_quant / c_scale\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vector_abs_i8(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    A_scale = 127 / torch.max(torch.abs(A), dim=1).values\n",
    "    B_scale = 127 / torch.max(torch.abs(B), dim=0).values\n",
    "    C_scale = torch.matmul(A_scale.unsqueeze(1), B_scale.unsqueeze(0))\n",
    "\n",
    "    A_i8 = torch.clip((A  * A_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    B_i8 = torch.clip((B  * B_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    return torch.matmul(A_i8.to(torch.int32), B_i8.to(torch.int32)) / C_scale\n",
    "\n",
    "def LLM_matmul_abs_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "    W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)\n",
    "\n",
    "def LLM_matmul_zp_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    # Calculate value range (denominator)\n",
    "    X2_range = torch.max(X2, dim=1).values - torch.min(X2, dim=1).values\n",
    "    W2_range = torch.max(W2, dim=0).values - torch.min(W2, dim=0).values\n",
    "    \n",
    "    # Calculate scale\n",
    "    X2_scale = 255 / X2_range\n",
    "    W2_scale = 255 / W2_range\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    # Shift by zero-point\n",
    "    X2_zp = (-X2_scale * torch.min(X2, dim = 1).values - 128).round()\n",
    "    W2_zp = (-W2_scale * torch.min(W2, dim = 0).values - 128).round()\n",
    "    O_zp = torch.matmul(X2_zp.unsqueeze(1), W2_zp.unsqueeze(0))    \n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    X2_quant = torch.clip((X2 * X2_scale.unsqueeze(1) + X2_zp.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_quant = torch.clip((W2 * W2_scale.unsqueeze(0) + W2_zp.unsqueeze(0)).round(), -128, 127).to(torch.int8)   \n",
    "    O2_quant = (X2_quant.to(torch.int32) @ W2_quant.to(torch.int32)) \\\n",
    "                - X2_quant.to(X2) @ W2_zp.unsqueeze(0).expand(X2.shape[1], -1) \\\n",
    "                - X2_zp.unsqueeze(1).expand(-1, W2.shape[0]) @ W2_quant.to(W2) \\\n",
    "                + O_zp * X2.shape[1]\n",
    "    O2 = O2_quant / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 10.0%\n",
      "LLM.int8() absmax -> Acc: 66560.0, Avg: 0.265625\n",
      "Reserved 10.0%\n",
      "LLM.int8() zero-point -> Acc: 68608.0, Avg: 0.275390625\n",
      "tensor(68608., dtype=torch.bfloat16) tensor(0.2754, dtype=torch.bfloat16)\n",
      "int8 abs -> Acc: 69632.0, Avg: 0.279296875\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(500, 1000, dtype=torch.bfloat16)\n",
    "W = torch.randn(1000, 500, dtype=torch.bfloat16)\n",
    "X[0, 0: X.shape[1] // 10] = 6\n",
    "\n",
    "error = torch.abs(LLM_matmul_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() absmax -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "\n",
    "error = torch.abs(LLM_matmul_zp_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() zero-point -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(matmul_vector_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'int8 abs -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modle Quantization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/anaconda3/envs/wheels/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 510,342,192 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[-21, -11,  -4,  ...,   2,  -2,   1],\n",
      "        [  3,   6,  10,  ...,  -2,   0,   0],\n",
      "        [  0,   3,  16,  ...,   5,   1,  -1],\n",
      "        ...,\n",
      "        [-11,   0,   8,  ...,   0,  -2,   1],\n",
      "        [  6,   9,   4,  ...,   1,  -1,  -2],\n",
      "        [-18,  -8, -10,  ...,   0,   0,   0]], dtype=torch.int8) \n",
      " tensor([[-0.4702, -0.2463, -0.0896,  ...,  0.0448, -0.0448,  0.0224],\n",
      "        [ 0.0672,  0.1343,  0.2239,  ..., -0.0448,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0672,  0.3583,  ...,  0.1120,  0.0224, -0.0224],\n",
      "        ...,\n",
      "        [-0.2463,  0.0000,  0.1791,  ...,  0.0000, -0.0448,  0.0224],\n",
      "        [ 0.1343,  0.2015,  0.0896,  ...,  0.0224, -0.0224, -0.0448],\n",
      "        [-0.4030, -0.1791, -0.2239,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "Zero-point quantized weights:\n",
      "tensor([[-21, -11,  -4,  ...,   3,  -2,   2],\n",
      "        [  5,   7,  11,  ...,  -2,   0,   0],\n",
      "        [  1,   4,  17,  ...,   6,   2,  -1],\n",
      "        ...,\n",
      "        [-11,   0,  10,  ...,   1,  -2,   2],\n",
      "        [  7,  10,   5,  ...,   2,  -1,  -2],\n",
      "        [-18,  -8, -10,  ...,   0,   1,   1]], dtype=torch.int8) \n",
      " tensor([[-0.4774, -0.2562, -0.1014,  ...,  0.0534, -0.0572,  0.0313],\n",
      "        [ 0.0976,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0129],\n",
      "        [ 0.0092,  0.0755,  0.3630,  ...,  0.1197,  0.0313, -0.0351],\n",
      "        ...,\n",
      "        [-0.2562, -0.0129,  0.2082,  ...,  0.0092, -0.0572,  0.0313],\n",
      "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0313, -0.0351, -0.0572],\n",
      "        [-0.4110, -0.1899, -0.2341,  ..., -0.0129,  0.0092,  0.0092]])\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, weights_abs_dequant = absmax_quantize_i8(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant, '\\n', weights_abs_dequant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, weights_zp_dequant = zeropoint_quantize_i8(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant, '\\n', weights_zp_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream, and it is a dream I believe I would get to live in my future. I love my mother, and there was that one time I had been told that my family wasn't even that strong. And then I got the\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream\n",
      "For your\n",
      "ForAndSheForHerWithYourLThenSo\n",
      "TheSheSheOfHerThatSheLToYouThatSoThatSheOfSoToThatSoAndOfThatThatThatThatNowOfThatThatOfThat\n",
      "--------------------------------------------------\n",
      "Zeropoint model:\n",
      "I have a dreamy little girl with my head and I'll watch every playfield on the other side of our town for a couple of years. I love how fun it is to have that, and I think it's great, but I'm\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  15.53\n",
      "Absmax perplexity:    34.43\n",
      "Zeropoint perplexity: 18.98\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation    \n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, zp_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearInt8(nn.Module):\n",
    "    def __init__(self, weight: torch.tensor, bias = None):\n",
    "        super().__init__()\n",
    "        self.zp_quantize_weight_i8(weight)\n",
    "\n",
    "        self.bias = None if bias is None else bias.to(torch.bfloat16)\n",
    "\n",
    "    def zp_quantize_weight_i8(self, weight: torch.Tensor):\n",
    "        r = torch.max(weight, dim=0).values - torch.min(weight, dim=0).values\n",
    "        r = torch.where(r == 0, torch.tensor(1), r)        \n",
    "        \n",
    "        scale = 255 / r\n",
    "        zeropoint = (-scale * torch.min(weight, dim=0).values - 128)        \n",
    "\n",
    "        self.weight_i8 =  (weight * scale.unsqueeze(0) + zeropoint.unsqueeze(0)).round().to(torch.int8)\n",
    "\n",
    "        self.scale = scale.to(torch.bfloat16)\n",
    "        self.zeropoint = zeropoint.to(torch.bfloat16)\n",
    "    \n",
    "    def zp_dequantize_weight_i8(self) -> torch.Tensor:\n",
    "        return (self.weight_i8 - self.zeropoint.unsqueeze(0)) / self.scale.unsqueeze(0)\n",
    "\n",
    "    def LLM_matmul_abs_i8(self, X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "        X_bf16 = X.to(torch.bfloat16)\n",
    "\n",
    "        X_col_filter = torch.max(torch.abs(X_bf16), dim = -2).values > alpha\n",
    "        X1 = X_bf16[:, X_col_filter]\n",
    "        W1 = W[X_col_filter, :]\n",
    "        X2 = X_bf16[:, ~X_col_filter]\n",
    "        W2 = W[~X_col_filter, :]\n",
    "        \n",
    "        O1 = torch.matmul(X1, W1)\n",
    "        print(f'Reserved {(X1.shape[1] / X_bf16.shape[1] * 100):.1f}%')\n",
    "        X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "        W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "        O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "        X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "        W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "        O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "        \n",
    "        return (O1 + O2.to(O1)).to(X)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(end_dim=-2)\n",
    "        ret = self.LLM_matmul_abs_i8(x, self.zp_dequantize_weight_i8())\n",
    "        if self.bias is not None:\n",
    "            ret += self.bias\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight avg err: 0.0013\n",
      "Reserved 0.0%\n",
      "Activation avg err: 0.0589\n"
     ]
    }
   ],
   "source": [
    "# W = torch.randn(5, 3)\n",
    "# X = torch.randn(2, 5)\n",
    "W = model.transformer.h[0].attn.c_attn.weight\n",
    "bias = model.transformer.h[0].attn.c_attn.bias\n",
    "X = torch.randn(2, model.transformer.h[0].attn.c_attn.weight.shape[0])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "print(f'Weight avg err: {torch.sum(torch.abs(l_i8.zp_dequantize_weight_i8() - W))/W.numel():.4f}')\n",
    "print(f'Activation avg err: { torch.sum(torch.abs(X @ W + bias - l_i8(X))) / (X.shape[0] * W.shape[1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 0.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.6777,  0.0938, -0.8069],\n",
       "         [ 0.2734, -1.9512, -2.4961],\n",
       "         [ 0.4844, -2.8730, -3.4805],\n",
       "         [-0.6367,  0.1973,  1.1055],\n",
       "         [ 1.0938, -3.7324, -7.3555],\n",
       "         [-0.7852, -2.1543, -6.0742]]),\n",
       " tensor([[[-1.6827,  0.0952, -0.8056],\n",
       "          [ 0.2907, -1.9622, -2.5328],\n",
       "          [ 0.4907, -2.8780, -3.5294]],\n",
       " \n",
       "         [[-0.6502,  0.2042,  1.0990],\n",
       "          [ 1.1015, -3.7289, -7.3894],\n",
       "          [-0.7944, -2.1390, -6.0891]]]))"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(5, 3)\n",
    "X = torch.randn(2, 3, 5)\n",
    "bias = torch.randn(W.shape[-1])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "l_i8(X), X @ W + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model_int8 \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_int8\u001b[38;5;241m.\u001b[39mget_memory_footprint()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/wheels/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:566\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 566\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    572\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/wheels/lib/python3.10/site-packages/transformers/modeling_utils.py:2714\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_in_8bit \u001b[38;5;129;01mor\u001b[39;00m load_in_4bit:\n\u001b[1;32m   2713\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_accelerate_available() \u001b[38;5;129;01mand\u001b[39;00m is_bitsandbytes_available()):\n\u001b[0;32m-> 2714\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m   2715\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2716\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2717\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pip install bitsandbytes` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2718\u001b[0m         )\n\u001b[1;32m   2720\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2721\u001b[0m         \u001b[38;5;66;03m# We force the `dtype` to be float16, this is a requirement from `bitsandbytes`\u001b[39;00m\n\u001b[1;32m   2722\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   2723\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverriding torch_dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with `torch_dtype=torch.float16` due to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2724\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2725\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m torch_dtype=torch.float16 to remove this warning.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2727\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `load_in_8bit=True` requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes `pip install -i https://test.pypi.org/simple/ bitsandbytes` or pip install bitsandbytes` "
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "#                                              device_map='auto',\n",
    "#                                              load_in_8bit=True,\n",
    "#                                              )\n",
    "# print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
