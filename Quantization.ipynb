{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize_i8(X: torch.Tensor):\n",
    "    absmax = torch.max(torch.abs(X))\n",
    "    X_i8 = ((X * 127) / absmax).to(torch.int8)\n",
    "    return X_i8, X_i8.to(torch.float32) * absmax / 127\n",
    "\n",
    "def zeropoint_quantize_i8(X: torch.Tensor):\n",
    "    r = torch.max(X) - torch.min(X)\n",
    "    r = 1 if r == 0 else r\n",
    "    scale = 255 / r\n",
    "\n",
    "    zeropoint = (-scale * torch.min(X) - 128)\n",
    "    X_i8 =  (X * scale + zeropoint).round().to(torch.int8)\n",
    "    \n",
    "    return X_i8, (X_i8 - zeropoint) / scale\n",
    "\n",
    "# def absmax_quantize(X):\n",
    "#     # Calculate scale\n",
    "#     scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "#     # Quantize\n",
    "#     X_quant = (scale * X).round()\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = X_quant / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# def zeropoint_quantize(X):\n",
    "#     # Calculate value range (denominator)\n",
    "#     x_range = torch.max(X) - torch.min(X)\n",
    "#     x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "#     # Calculate scale\n",
    "#     scale = 255 / x_range\n",
    "\n",
    "#     # Shift by zero-point\n",
    "#     zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "#     # Scale and round the inputs\n",
    "#     X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def zp_mul(A, B):\n",
    "    # Calculate value range (denominator)\n",
    "    a_range = torch.max(A) - torch.min(A)\n",
    "    b_range = torch.max(B) - torch.min(B)\n",
    "    a_range = 1 if a_range == 0 else a_range\n",
    "    b_range = 1 if b_range == 0 else b_range\n",
    "    \n",
    "    # Calculate scale\n",
    "    a_scale = 255 / a_range\n",
    "    b_scale = 255 / b_range\n",
    "    c_scale = a_scale * b_scale\n",
    "\n",
    "    # Shift by zero-point\n",
    "    a_zp = (-a_scale * torch.min(A) - 128).round()\n",
    "    b_zp = (-b_scale * torch.min(B) - 128).round()\n",
    "    c_zp = a_zp * b_zp\n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    A_quant = torch.clip((A * a_scale + a_zp).round(), -128, 127).to(torch.int8)\n",
    "    B_quant = torch.clip((B * b_scale + b_zp).round(), -128, 127).to(torch.int8)    \n",
    "    \n",
    "    # print(f'c_scale:{c_scale}, c_zp:{c_zp}')\n",
    "    # print(f'A_quant:{A_quant}, B_quant:{B_quant}')\n",
    "    # Multiply\n",
    "    C_quant = (A_quant.to(torch.int16) * B_quant.to(torch.int16)) + c_zp #- A_quant * b_zp.to(torch.float32) - B_quant * a_zp.to(torch.float32) \n",
    "    # print(f'c_quant:{C_quant}')\n",
    "    C = C_quant / c_scale\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vector_abs_i8(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    A_scale = 127 / torch.max(torch.abs(A), dim=1).values\n",
    "    B_scale = 127 / torch.max(torch.abs(B), dim=0).values\n",
    "    C_scale = torch.matmul(A_scale.unsqueeze(1), B_scale.unsqueeze(0))\n",
    "\n",
    "    A_i8 = torch.clip((A  * A_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    B_i8 = torch.clip((B  * B_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    return torch.matmul(A_i8.to(torch.int32), B_i8.to(torch.int32)) / C_scale\n",
    "\n",
    "def LLM_matmul_abs_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "    W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)\n",
    "\n",
    "def LLM_matmul_zp_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    # Calculate value range (denominator)\n",
    "    X2_range = torch.max(X2, dim=1).values - torch.min(X2, dim=1).values\n",
    "    W2_range = torch.max(W2, dim=0).values - torch.min(W2, dim=0).values\n",
    "    \n",
    "    # Calculate scale\n",
    "    X2_scale = 255 / X2_range\n",
    "    W2_scale = 255 / W2_range\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    # Shift by zero-point\n",
    "    X2_zp = (-X2_scale * torch.min(X2, dim = 1).values - 128).round()\n",
    "    W2_zp = (-W2_scale * torch.min(W2, dim = 0).values - 128).round()\n",
    "    O_zp = torch.matmul(X2_zp.unsqueeze(1), W2_zp.unsqueeze(0))    \n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    X2_quant = torch.clip((X2 * X2_scale.unsqueeze(1) + X2_zp.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_quant = torch.clip((W2 * W2_scale.unsqueeze(0) + W2_zp.unsqueeze(0)).round(), -128, 127).to(torch.int8)   \n",
    "    O2_quant = (X2_quant.to(torch.int32) @ W2_quant.to(torch.int32)) \\\n",
    "                - X2_quant.to(X2) @ W2_zp.unsqueeze(0).expand(X2.shape[1], -1) \\\n",
    "                - X2_zp.unsqueeze(1).expand(-1, W2.shape[0]) @ W2_quant.to(W2) \\\n",
    "                + O_zp * X2.shape[1]\n",
    "    O2 = O2_quant / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 10.0%\n",
      "LLM.int8() absmax -> Acc: 66560.0, Avg: 0.265625\n",
      "Reserved 10.0%\n",
      "LLM.int8() zero-point -> Acc: 68096.0, Avg: 0.271484375\n",
      "tensor(68096., dtype=torch.bfloat16) tensor(0.2715, dtype=torch.bfloat16)\n",
      "int8 abs -> Acc: 69632.0, Avg: 0.279296875\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(500, 1000, dtype=torch.bfloat16)\n",
    "W = torch.randn(1000, 500, dtype=torch.bfloat16)\n",
    "X[0, 0: X.shape[1] // 10] = 6\n",
    "\n",
    "error = torch.abs(LLM_matmul_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() absmax -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "\n",
    "error = torch.abs(LLM_matmul_zp_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() zero-point -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(matmul_vector_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'int8 abs -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modle Quantization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 510,342,192 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n",
      "\n",
      "Absmax quantized weights:\n",
      "tensor([[-21, -12,  -4,  ...,   2,  -3,   1],\n",
      "        [  4,   7,  11,  ...,  -2,  -1,  -1],\n",
      "        [  0,   3,  16,  ...,   5,   2,  -1],\n",
      "        ...,\n",
      "        [-12,  -1,   9,  ...,   0,  -2,   1],\n",
      "        [  7,  10,   5,  ...,   1,  -2,  -2],\n",
      "        [-18,  -9, -11,  ...,   0,   0,   1]], dtype=torch.int8) \n",
      " tensor([[-0.4702, -0.2687, -0.0896,  ...,  0.0448, -0.0672,  0.0224],\n",
      "        [ 0.0896,  0.1567,  0.2463,  ..., -0.0448, -0.0224, -0.0224],\n",
      "        [ 0.0000,  0.0672,  0.3583,  ...,  0.1120,  0.0448, -0.0224],\n",
      "        ...,\n",
      "        [-0.2687, -0.0224,  0.2015,  ...,  0.0000, -0.0448,  0.0224],\n",
      "        [ 0.1567,  0.2239,  0.1120,  ...,  0.0224, -0.0448, -0.0448],\n",
      "        [-0.4030, -0.2015, -0.2463,  ..., -0.0000,  0.0000,  0.0224]])\n",
      "\n",
      "Zero-point quantized weights:\n",
      "tensor([[-20, -11,  -3,  ...,   3,  -2,   2],\n",
      "        [  5,   8,  12,  ...,  -1,   0,   0],\n",
      "        [  1,   4,  18,  ...,   6,   3,   0],\n",
      "        ...,\n",
      "        [-11,   0,  10,  ...,   1,  -1,   2],\n",
      "        [  8,  11,   6,  ...,   2,  -1,  -1],\n",
      "        [-18,  -8, -10,  ...,   1,   1,   2]], dtype=torch.int8) \n",
      " tensor([[-0.4644, -0.2654, -0.0885,  ...,  0.0442, -0.0663,  0.0221],\n",
      "        [ 0.0885,  0.1548,  0.2433,  ..., -0.0442, -0.0221, -0.0221],\n",
      "        [ 0.0000,  0.0663,  0.3760,  ...,  0.1106,  0.0442, -0.0221],\n",
      "        ...,\n",
      "        [-0.2654, -0.0221,  0.1990,  ...,  0.0000, -0.0442,  0.0221],\n",
      "        [ 0.1548,  0.2211,  0.1106,  ...,  0.0221, -0.0442, -0.0442],\n",
      "        [-0.4202, -0.1990, -0.2433,  ...,  0.0000,  0.0000,  0.0221]])\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, weights_abs_dequant = absmax_quantize(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print(weights_abs_quant, '\\n', weights_abs_dequant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, weights_zp_dequant = zeropoint_quantize(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print(weights_zp_quant, '\\n', weights_zp_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream, it's all about getting a big break but that doesn't sound like a great start. I was really happy with the way things turned out for me and it got me through some of the tough times that people were dealing with\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream.\"\n",
      "And it come I is like he that was he be come\n",
      "\n",
      "It to that not all was he he to that is he\n",
      "And in as said he was is she that him all be she that he it said\n",
      "--------------------------------------------------\n",
      "Zeropoint model:\n",
      "I have a dream of becoming something.\"\n",
      "\n",
      "\"Well, so do I will not be that someday that I will find out.\"\n",
      "\n",
      "It is all for the sake of her; but I shall prove her.\n",
      "\n",
      "\"So, my\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  13.95\n",
      "Absmax perplexity:    74.65\n",
      "Zeropoint perplexity: 19.24\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation    \n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, zp_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
