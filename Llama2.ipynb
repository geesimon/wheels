{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=4, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=5000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = 4\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]]\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            hidden_dim: int,\n",
    "            multiple_of: int,\n",
    "            ffn_dim_multiplier: Optional[float],\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "*Note:* 2 differences with [original Llama implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L176)\n",
    "- The weight matrix has 3 dimensions as: `number_head * model_dimension * head_dimension` instead of `model_dimension * model_dimension`. This is strictly follow the \"Attention is all you need\" paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 128]) torch.Size([4, 128, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 128, 32])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = nn.Parameter(nn.init.kaiming_normal_(torch.empty(model_config.n_heads,model_config.dim, model_config.dim // model_config.n_heads),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "x = torch.randn(model_config.max_batch_size,model_config.max_seq_len, model_config.dim)\n",
    "print(x.unsqueeze(1).shape, w.shape)\n",
    "(x.unsqueeze(1) @ w).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "\n",
    "    shared_rope : RoPE = None    \n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(self.head_dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_k = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_v = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_o = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads* self.head_dim, config.dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.n_kv_heads, config.max_seq_len, self.head_dim, requires_grad=False)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k, requires_grad=False)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, dim)\n",
    "        q: (batch_size, n_heads, seq_len, head_dim)\n",
    "        k: (batch_size, n_heads, seq_len, head_dim)\n",
    "        v: (batch_size, n_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        q = x.unsqueeze(1) @ self.w_q\n",
    "        k = x.unsqueeze(1) @ self.w_k\n",
    "        v = x.unsqueeze(1) @ self.w_v\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)\n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, :, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, :, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        o = F.scaled_dot_product_attention(q, k, v, dropout_p = dropout_p, is_causal = is_causal)\n",
    "        o = o.permute(0, 2, 1, 3).contiguous().view(o.shape[0], o.shape[2], -1)       # concatenate heads        \n",
    "        o = o @ self.w_o\n",
    "        o = self.dropout(o)\n",
    "\n",
    "        return o\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        h_heads = [head(x, start_pos) for head in self.heads]\n",
    "        h = torch.cat(h_heads, dim=-1)\n",
    "        h = self.linear(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = Attention(config)\n",
    "        self.attention_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_swiglu = FFN_SwiGLU(config.dim, config.dim * 4, config.multiple_of, config.ffn_dim_multiplier)\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = x + self.attention(self.attention_norm(x), start_pos)\n",
    "        out = x + self.ffn_swiglu(self.ffn_norm(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", Llama2Block(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        h = self.embeddings(idx)\n",
    "\n",
    "        for block in self.llama_blocks:\n",
    "            h = block(h, start_pos)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama2):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama2, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 16])\n",
      "model params: 541824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 4.071 | Time 0.134 | ETA in seconds 6.682\n",
      "Epoch 100 | val loss 2.348 | Time 7.977 | ETA in seconds 390.864\n",
      "Epoch 200 | val loss 2.120 | Time 8.213 | ETA in seconds 394.213\n",
      "Epoch 300 | val loss 1.968 | Time 8.836 | ETA in seconds 415.297\n",
      "Epoch 400 | val loss 1.865 | Time 9.284 | ETA in seconds 427.056\n",
      "Epoch 500 | val loss 1.814 | Time 10.204 | ETA in seconds 459.165\n",
      "Epoch 600 | val loss 1.764 | Time 9.455 | ETA in seconds 416.030\n",
      "Epoch 700 | val loss 1.759 | Time 9.332 | ETA in seconds 401.261\n",
      "Epoch 800 | val loss 1.734 | Time 9.392 | ETA in seconds 394.474\n",
      "Epoch 900 | val loss 1.714 | Time 10.088 | ETA in seconds 413.619\n",
      "Epoch 1000 | val loss 1.698 | Time 11.258 | ETA in seconds 450.318\n",
      "Epoch 1100 | val loss 1.666 | Time 9.553 | ETA in seconds 372.562\n",
      "Epoch 1200 | val loss 1.678 | Time 9.165 | ETA in seconds 348.257\n",
      "Epoch 1300 | val loss 1.684 | Time 9.174 | ETA in seconds 339.420\n",
      "Epoch 1400 | val loss 1.675 | Time 9.235 | ETA in seconds 332.460\n",
      "Epoch 1500 | val loss 1.629 | Time 9.091 | ETA in seconds 318.179\n",
      "Epoch 1600 | val loss 1.645 | Time 9.245 | ETA in seconds 314.335\n",
      "Epoch 1700 | val loss 1.633 | Time 9.229 | ETA in seconds 304.563\n",
      "Epoch 1800 | val loss 1.608 | Time 9.145 | ETA in seconds 292.630\n",
      "Epoch 1900 | val loss 1.610 | Time 9.321 | ETA in seconds 288.964\n",
      "Epoch 2000 | val loss 1.623 | Time 9.179 | ETA in seconds 275.376\n",
      "Epoch 2100 | val loss 1.594 | Time 9.074 | ETA in seconds 263.153\n",
      "Epoch 2200 | val loss 1.610 | Time 9.124 | ETA in seconds 255.462\n",
      "Epoch 2300 | val loss 1.618 | Time 9.050 | ETA in seconds 244.345\n",
      "Epoch 2400 | val loss 1.578 | Time 9.190 | ETA in seconds 238.947\n",
      "Epoch 2500 | val loss 1.576 | Time 9.070 | ETA in seconds 226.753\n",
      "Epoch 2600 | val loss 1.573 | Time 9.171 | ETA in seconds 220.101\n",
      "Epoch 2700 | val loss 1.589 | Time 9.293 | ETA in seconds 213.729\n",
      "Epoch 2800 | val loss 1.562 | Time 9.585 | ETA in seconds 210.871\n",
      "Epoch 2900 | val loss 1.572 | Time 9.419 | ETA in seconds 197.799\n",
      "Epoch 3000 | val loss 1.549 | Time 9.070 | ETA in seconds 181.393\n",
      "Epoch 3100 | val loss 1.595 | Time 9.052 | ETA in seconds 171.987\n",
      "Epoch 3200 | val loss 1.551 | Time 9.238 | ETA in seconds 166.291\n",
      "Epoch 3300 | val loss 1.560 | Time 9.864 | ETA in seconds 167.696\n",
      "Epoch 3400 | val loss 1.570 | Time 9.687 | ETA in seconds 154.991\n",
      "Epoch 3500 | val loss 1.588 | Time 9.149 | ETA in seconds 137.236\n",
      "Epoch 3600 | val loss 1.586 | Time 9.096 | ETA in seconds 127.344\n",
      "Epoch 3700 | val loss 1.556 | Time 8.997 | ETA in seconds 116.966\n",
      "Epoch 3800 | val loss 1.567 | Time 8.930 | ETA in seconds 107.161\n",
      "Epoch 3900 | val loss 1.549 | Time 9.050 | ETA in seconds 99.545\n",
      "Epoch 4000 | val loss 1.548 | Time 8.761 | ETA in seconds 87.608\n",
      "Epoch 4100 | val loss 1.550 | Time 8.785 | ETA in seconds 79.062\n",
      "Epoch 4200 | val loss 1.564 | Time 8.834 | ETA in seconds 70.675\n",
      "Epoch 4300 | val loss 1.568 | Time 8.694 | ETA in seconds 60.858\n",
      "Epoch 4400 | val loss 1.571 | Time 8.721 | ETA in seconds 52.326\n",
      "Epoch 4500 | val loss 1.553 | Time 9.007 | ETA in seconds 45.033\n",
      "Epoch 4600 | val loss 1.558 | Time 8.904 | ETA in seconds 35.615\n",
      "Epoch 4700 | val loss 1.524 | Time 8.917 | ETA in seconds 26.751\n",
      "Epoch 4800 | val loss 1.546 | Time 9.163 | ETA in seconds 18.327\n",
      "Epoch 4900 | val loss 1.552 | Time 9.104 | ETA in seconds 9.104\n",
      "validation loss:  1.551768672466278\n",
      "CPU times: user 1h 20min 43s, sys: 29.1 s, total: 1h 21min 12s\n",
      "Wall time: 8min 7s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama2(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama2_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama2, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 16])\n",
      "model params: 541824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama2(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have such my pactise all the bundam ag's kin about,\n",
      "and brist as young and air gates. For soon of \n",
      "\n",
      "Of the secret mark afe of my rose were do see,\n",
      "He's site of lyes!' thou art hate my boys.\n",
      "O mercy?\n",
      "S\n",
      "\n",
      "\n",
      "HASTINGS:\n",
      "And save I hard so, pay as here 'Hence.\n",
      "\n",
      "KING HENRY VI:\n",
      "Had my face-erge: if your breath \n",
      "\n",
      "Or ear the repaitors respite.\n",
      "\n",
      "AEdile:\n",
      "A mose my all seek'd sorrow me. I am gensing, my hope.\n",
      "\n",
      "HASTI\n",
      "\n",
      "Meantime is ours,\n",
      "Which thee I must night-buildness entertains,\n",
      "And, he's after to my shrig and this\n",
      "\n",
      "I will come to see dealm mark, will I warrant,\n",
      "To pespite soul me: rue is makes, you say me!\n",
      "So, par\n",
      "\n",
      "So he house, make will she bad pleasing us\n",
      "The harms I hate!\n",
      "\n",
      "EARL OF LIZELOW:\n",
      "I am rept it would he\n",
      "\n",
      "May son which to him to sometime ill.\n",
      "Here in my bried shepherd to shall limposude!\n",
      "And you go: sch \n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "What thy dead what! you, either-keer Very head,\n",
      "Nor again to this daughter would I \n",
      "\n",
      "Rathers, who show thee my mother sully fearf;\n",
      "As love, since my blood out but my bosom,\n",
      "Thy lips, ma\n",
      "\n",
      "Nurse; and his noblems, our draw the neights his broad\n",
      "Say descation. You, Woe once now of thy stay \n",
      "\n",
      "Theme rawness and enim, one I do reacher's before:\n",
      "Be despided world by\n",
      "Of that their spoted amain! \n",
      "\n",
      "For you gave night?\n",
      "\n",
      "LEONTES:\n",
      "Come, you know lies, all at news,\n",
      "Did nurn well: she in heaven, live f\n",
      "\n",
      "Well have the hunder'd by him, recond as.\n",
      "\n",
      "CLIFFORD:\n",
      "It icked there and it down say as advice\n",
      "As is \n",
      "\n",
      "\n",
      "MERCUTIO:\n",
      "Thou fair: if you.\n",
      "\n",
      "HENRY BOMPHEN:\n",
      "All the kingly back, potent befall damned\n",
      "The traw her\n",
      "\n",
      "\n",
      "ARCHICENBERS:\n",
      "He woful rwas not my death ne'er in them and that youring,\n",
      "Which be side to medless s\n",
      "\n",
      "When they Paris:\n",
      "Keeper of his?\n",
      "\n",
      "BALTHASAR:\n",
      "Why had a house,\n",
      "Your ask with sharp we pothecared heard\n",
      "\n",
      "WARWICK:\n",
      "While not plain and go one pecer is that drunk\n",
      "Turn beat the battle amazemined to thy broke\n",
      "\n",
      "\n",
      "That afford the queen'st those be needs to think,\n",
      "A trembling and his is as good Rivers with unnard\n",
      "\n",
      "HASTINGS:\n",
      "Out not my hour.\n",
      "\n",
      "First Lord Richard, no, were Edwardly to meet.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Had Call\n",
      "\n",
      "It king my sight's life\n",
      "A can warly as there tell it, sir; thou seercy:\n",
      "That woe to do did good fell\n",
      "\n",
      "Brectongest way: how not itself?\n",
      "\n",
      "First Servant:\n",
      "Like.\n",
      "\n",
      "AUTOLYCUS:\n",
      "O, by the arms power before pound\n",
      "\n",
      "Never yet me to it: It say believing shonour not,\n",
      "Expus!\n",
      "\n",
      "Rame on heaven.\n",
      "\n",
      "LADY ANNE:\n",
      "Where is my su\n",
      "\n",
      "My poor Chairs dearely wars prince for fapw our pity,\n",
      "To breathe's prepared with tears, for a bounts\n",
      "\n",
      "And unworty expect how.\n",
      "If beg about of throw?\n",
      "\n",
      "FLORIZEL:\n",
      "What all cally proved; this if it cuts of \n",
      "\n",
      "First London,\n",
      "And it faults teach thou act him of me live you:\n",
      "The quest, that I body, I spard the s\n",
      "\n",
      "Thy connomitity. Bushy, nobly wory.\n",
      "\n",
      "POLIXENES:\n",
      "\n",
      "Messenger:\n",
      "Had yet did keep brown the truth of tyra\n",
      "\n",
      "CORIOLANUS:\n",
      "Stay, foot nupied appey, he will I received,\n",
      "That hast hath been before thy good to batt\n",
      "\n",
      "Do you art.\n",
      "\n",
      "MENENIUS:\n",
      "E?\n",
      "\n",
      "Lord Aay, some as a maid in thy sit before I be\n",
      "present such all glive ho\n",
      "\n",
      "I do feath seek it is headitied lay their die,\n",
      "There my parly which spides.\n",
      "\n",
      "Shepherd:\n",
      "Who bright ne\n",
      "\n",
      "Alacksd in paler you, my lord, they nexper,\n",
      "Wich wear aviers shame this extremit flowers,\n",
      "If my lord\n",
      "\n",
      "ORNCEOMINIA:\n",
      "Come, come itself. This is half weep of they\n",
      "Did great ounder Warwick, hath me too,\n",
      "To \n",
      "CPU times: user 1.11 s, sys: 0 ns, total: 1.11 s\n",
      "Wall time: 117 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
