{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=4, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=5000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = 4\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]]\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config: ModelArgs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = (config.dim * 4) * 2 // 3\n",
    "        # custom dim factor multiplier\n",
    "        if config.ffn_dim_multiplier is not None:\n",
    "            hidden_dim = config.ffn_dim_multiplier * hidden_dim\n",
    "        hidden_dim = config.multiple_of * ((hidden_dim + config.multiple_of - 1) // config.multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(config.dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(config.dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, config.dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "*Note:* 2 differences with the [original Llama implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L176)\n",
    "- The weight matrix has 3 dimensions as: `(number_head * model_dimension * head_dimension)` instead of `(model_dimension * model_dimension)`. This is strictly follow the \"Attention is all you need\" paper\n",
    "- For Group Query Attention implmentation, instead of repeating KV values which is against the purpose of GQA (to reduce KV load overhead). Here reshapes the Q/K/V to 5 dimensions: `(batch_size * number_kv_heads * number_shared_kv (number_heads/number_kv_heads) * sequence_length * head_dimension)` and do matmut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16]) torch.Size([1, 2, 4, 3, 2]) torch.Size([1, 2, 1, 3, 2]) torch.Size([1, 2, 1, 3, 2]) torch.Size([1, 2, 4, 3, 3]) torch.Size([1, 2, 4, 3, 2]) torch.Size([1, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "n_heads = 8\n",
    "n_kv_heads = 2\n",
    "batch_size = 1\n",
    "seq_len = 5\n",
    "model_dim = 16\n",
    "head_dim = model_dim // n_heads\n",
    "\n",
    "w_q = nn.Parameter(torch.ones(n_heads,model_dim, head_dim) * 0.1)\n",
    "w_k = nn.Parameter(torch.ones(n_kv_heads,model_dim, head_dim) * 0.2)\n",
    "w_v = nn.Parameter(torch.ones(n_kv_heads,model_dim, head_dim) * 0.3)\n",
    "\n",
    "x = torch.zeros(batch_size, seq_len, model_dim)\n",
    "x[:, 1, :] = 1\n",
    "x[:, 2, :] = 2\n",
    "x[:, 3, :] = 3\n",
    "x[:, 4, :] = 4\n",
    "x = x[:, :-2]\n",
    "\n",
    "q = (x.unsqueeze(1) @ w_q)\n",
    "q = q.view(batch_size, n_kv_heads, -1, q.shape[-2], head_dim)\n",
    "k = (x.unsqueeze(1) @ w_k)\n",
    "k = k.view(batch_size, n_kv_heads, -1, k.shape[-2], head_dim)\n",
    "v = (x.unsqueeze(1) @ w_v)\n",
    "v = v.view(batch_size, n_kv_heads, -1, v.shape[-2], head_dim)\n",
    "s = (q @ k.transpose(-2, -1))\n",
    "o = F.scaled_dot_product_attention(q, k, v, dropout_p = 0.1, is_causal = True)\n",
    "out = o.permute(0, 3, 1, 2, 4).contiguous().view(batch_size, x.shape[-2], -1)\n",
    "\n",
    "print(x.shape, q.shape, k.shape, v.shape, s.shape, o.shape, out.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"Multi-head attention module.\"\"\"\n",
    "\n",
    "    shared_rope : RoPE = None    \n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.head_dim = config.dim // config.n_heads\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(self.head_dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_k = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_v = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads,config.dim, self.head_dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "        self.w_o = nn.Parameter(nn.init.kaiming_normal_(torch.empty(config.n_heads* self.head_dim, config.dim),\n",
    "                                                        mode='fan_out', nonlinearity='relu'))\n",
    "\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.n_kv_heads, config.max_seq_len, self.head_dim, requires_grad=False)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k, requires_grad=False)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, dim)\n",
    "        q: (batch_size, n_heads, seq_len, head_dim)\n",
    "        k: (batch_size, n_heads, seq_len, head_dim)\n",
    "        v: (batch_size, n_heads, seq_len, head_dim)\n",
    "        \"\"\"\n",
    "        q = x.unsqueeze(1) @ self.w_q\n",
    "        k = x.unsqueeze(1) @ self.w_k\n",
    "        v = x.unsqueeze(1) @ self.w_v\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)\n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, :, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, :, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        # split heads and reshape Q, K, V as (batch_size, kv_heads, n_heads//kv_heads, seq_len, head_dim)\n",
    "        q = q.view(self.config.max_batch_size, self.config.n_kv_heads, -1, q.shape[-2], self.head_dim)\n",
    "        k = k.view(self.config.max_batch_size, self.config.n_kv_heads, -1, k.shape[-2], self.head_dim)\n",
    "        v = v.view(self.config.max_batch_size, self.config.n_kv_heads, -1, v.shape[-2], self.head_dim)\n",
    "        \n",
    "        o = F.scaled_dot_product_attention(q, k, v, dropout_p = dropout_p, is_causal = is_causal)\n",
    "        # o = o.permute(0, 2, 1, 3).contiguous().view(o.shape[0], o.shape[2], -1)       # concatenate heads        \n",
    "        o = o.permute(0, 3, 1, 2, 4).contiguous().view(o.shape[0], o.shape[3], -1)       # concatenate heads\n",
    "        o = o @ self.w_o\n",
    "        o = self.dropout(o)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = Attention(config)\n",
    "        self.attention_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_swiglu = FFN_SwiGLU(config)\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = x + self.attention(self.attention_norm(x), start_pos)\n",
    "        out = x + self.ffn_swiglu(self.ffn_norm(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", Llama2Block(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        h = self.embeddings(idx)\n",
    "\n",
    "        for block in self.llama_blocks:\n",
    "            h = block(h, start_pos)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama2):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama2, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 16])\n",
      "model params: 541824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 4.133 | Time 0.216 | ETA in seconds 10.798\n",
      "Epoch 100 | val loss 2.326 | Time 7.915 | ETA in seconds 387.851\n",
      "Epoch 200 | val loss 2.090 | Time 9.204 | ETA in seconds 441.786\n",
      "Epoch 300 | val loss 1.961 | Time 8.790 | ETA in seconds 413.117\n",
      "Epoch 400 | val loss 1.867 | Time 8.580 | ETA in seconds 394.694\n",
      "Epoch 500 | val loss 1.815 | Time 10.294 | ETA in seconds 463.224\n",
      "Epoch 600 | val loss 1.784 | Time 9.449 | ETA in seconds 415.743\n",
      "Epoch 700 | val loss 1.747 | Time 10.010 | ETA in seconds 430.423\n",
      "Epoch 800 | val loss 1.738 | Time 9.387 | ETA in seconds 394.252\n",
      "Epoch 900 | val loss 1.694 | Time 9.442 | ETA in seconds 387.106\n",
      "Epoch 1000 | val loss 1.665 | Time 9.543 | ETA in seconds 381.717\n",
      "Epoch 1100 | val loss 1.679 | Time 9.361 | ETA in seconds 365.065\n",
      "Epoch 1200 | val loss 1.674 | Time 9.554 | ETA in seconds 363.042\n",
      "Epoch 1300 | val loss 1.663 | Time 9.514 | ETA in seconds 352.011\n",
      "Epoch 1400 | val loss 1.660 | Time 9.548 | ETA in seconds 343.717\n",
      "Epoch 1500 | val loss 1.617 | Time 9.376 | ETA in seconds 328.171\n",
      "Epoch 1600 | val loss 1.626 | Time 9.366 | ETA in seconds 318.430\n",
      "Epoch 1700 | val loss 1.629 | Time 9.348 | ETA in seconds 308.496\n",
      "Epoch 1800 | val loss 1.613 | Time 9.421 | ETA in seconds 301.475\n",
      "Epoch 1900 | val loss 1.602 | Time 9.371 | ETA in seconds 290.506\n",
      "Epoch 2000 | val loss 1.611 | Time 9.497 | ETA in seconds 284.919\n",
      "Epoch 2100 | val loss 1.598 | Time 10.044 | ETA in seconds 291.280\n",
      "Epoch 2200 | val loss 1.608 | Time 10.355 | ETA in seconds 289.949\n",
      "Epoch 2300 | val loss 1.584 | Time 9.344 | ETA in seconds 252.296\n",
      "Epoch 2400 | val loss 1.600 | Time 9.199 | ETA in seconds 239.176\n",
      "Epoch 2500 | val loss 1.569 | Time 9.295 | ETA in seconds 232.382\n",
      "Epoch 2600 | val loss 1.568 | Time 9.541 | ETA in seconds 228.983\n",
      "Epoch 2700 | val loss 1.576 | Time 9.387 | ETA in seconds 215.909\n",
      "Epoch 2800 | val loss 1.593 | Time 9.862 | ETA in seconds 216.955\n",
      "Epoch 2900 | val loss 1.565 | Time 9.346 | ETA in seconds 196.267\n",
      "Epoch 3000 | val loss 1.582 | Time 9.593 | ETA in seconds 191.856\n",
      "Epoch 3100 | val loss 1.559 | Time 9.512 | ETA in seconds 180.731\n",
      "Epoch 3200 | val loss 1.571 | Time 9.456 | ETA in seconds 170.200\n",
      "Epoch 3300 | val loss 1.542 | Time 9.358 | ETA in seconds 159.085\n",
      "Epoch 3400 | val loss 1.567 | Time 9.444 | ETA in seconds 151.098\n",
      "Epoch 3500 | val loss 1.548 | Time 9.404 | ETA in seconds 141.065\n",
      "Epoch 3600 | val loss 1.536 | Time 9.505 | ETA in seconds 133.065\n",
      "Epoch 3700 | val loss 1.566 | Time 9.693 | ETA in seconds 126.003\n",
      "Epoch 3800 | val loss 1.554 | Time 9.752 | ETA in seconds 117.029\n",
      "Epoch 3900 | val loss 1.569 | Time 9.697 | ETA in seconds 106.664\n",
      "Epoch 4000 | val loss 1.564 | Time 9.829 | ETA in seconds 98.289\n",
      "Epoch 4100 | val loss 1.543 | Time 9.398 | ETA in seconds 84.580\n",
      "Epoch 4200 | val loss 1.550 | Time 9.708 | ETA in seconds 77.664\n",
      "Epoch 4300 | val loss 1.567 | Time 9.719 | ETA in seconds 68.034\n",
      "Epoch 4400 | val loss 1.562 | Time 9.651 | ETA in seconds 57.907\n",
      "Epoch 4500 | val loss 1.528 | Time 9.263 | ETA in seconds 46.315\n",
      "Epoch 4600 | val loss 1.531 | Time 9.187 | ETA in seconds 36.750\n",
      "Epoch 4700 | val loss 1.536 | Time 9.590 | ETA in seconds 28.769\n",
      "Epoch 4800 | val loss 1.535 | Time 9.640 | ETA in seconds 19.281\n",
      "Epoch 4900 | val loss 1.540 | Time 9.455 | ETA in seconds 9.455\n",
      "validation loss:  1.5399956345558166\n",
      "CPU times: user 1h 23min 4s, sys: 38.5 s, total: 1h 23min 42s\n",
      "Wall time: 8min 22s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLT0lEQVR4nO3deXxU5b0/8M+ZfSaZTPaNBBII+74pQRAEAcVS115rqUp7e3+lBZd67YLtvdbWXrzVX6vW1qV1pxZ/bUBpXVFIQAVlCYsIYc1CFkLWSTKZ/fz+eCaTDGTPzJwk83m/Xuc1c86cmfPNCTAfnuc5z5FkWZZBREREpBCV0gUQERFRZGMYISIiIkUxjBAREZGiGEaIiIhIUQwjREREpCiGESIiIlIUwwgREREpimGEiIiIFKVRuoDe8Hq9qKiogNlshiRJSpdDREREvSDLMpqampCeng6Vquv2jyERRioqKpCZmal0GURERNQPZWVlyMjI6PL1IRFGzGYzAPHDxMTEKFwNERER9YbVakVmZqb/e7wrQyKMtHXNxMTEMIwQERENMT0NseAAViIiIlIUwwgREREpimGEiIiIFDUkxowQERGFgizLcLvd8Hg8SpcyJKnVamg0mgFPu8EwQkREEcnpdKKyshI2m03pUoY0k8mEtLQ06HS6fn8GwwgREUUcr9eLc+fOQa1WIz09HTqdjpNq9pEsy3A6nbh48SLOnTuHsWPHdjuxWXcYRoiIKOI4nU54vV5kZmbCZDIpXc6QZTQaodVqUVJSAqfTCYPB0K/P4QBWIiKKWP39nzy1C8Y55G+BiIiIFMUwQkRERIpiGCEiIopQWVlZePLJJ5UugwNYiYiIhpLFixdjxowZQQkR+/btQ1RU1MCLGqCIDiO2fZvgOX8Q0uQbET1ukdLlEBERDZgsy/B4PNBoev6KT0pKCkNFPYvobppjBf+A+fCLOH5gt9KlEBGRwmRZhs3pVmSRZblXNa5ZswYFBQV46qmnIEkSJEnCK6+8AkmS8MEHH2DOnDnQ6/XYvXs3zpw5gxtvvBEpKSmIjo7G3Llz8dFHHwV83qXdNJIk4S9/+QtuvvlmmEwmjB07Ftu2bQvmae5URLeMeDTi2nLZ2aJwJUREpLRWlweT/vsDRY791a9WwKTr+Sv5qaeewsmTJzFlyhT86le/AgAcO3YMAPCTn/wETzzxBEaPHo3Y2FicP38eK1euxKOPPgqDwYBXX30Vq1atQlFREUaOHNnlMR555BH89re/xeOPP44//OEPWL16NUpKShAfHx+cH7YTEd0y4tUYxRMXpwImIqLBz2KxQKfTwWQyITU1FampqVCr1QCAX/3qV1i2bBnGjBmDhIQETJ8+Hd///vcxdepUjB07Fo8++ihGjx7dY0vHmjVrcMcddyAnJwf/8z//g5aWFnzxxRch/bkiumVE1oqWEYlhhIgo4hm1anz1qxWKHXug5syZE7De0tKCRx55BP/6179QUVEBt9uN1tZWlJaWdvs506ZN8z+PioqC2WxGdXX1gOvrToSHETGCmGGEiIgkSepVV8lgdelVMT/+8Y/xwQcf4IknnkBOTg6MRiNuu+02OJ3Obj9Hq9UGrEuSBK/XG/R6Oxq6Zz0IJJ1oGVG7WxWuhIiIqHd0Oh08Hk+P++3evRtr1qzBzTffDABobm5GcXFxiKvrn4geMyLpRIpUe9gyQkREQ0NWVhY+//xzFBcXo6ampstWi5ycHGzZsgWHDh3C4cOH8a1vfSvkLRz9FdFhRKUXYUTjYcsIERENDQ8++CDUajUmTZqEpKSkLseA/P73v0dcXBzmz5+PVatWYcWKFZg1a1aYq+2diO6mUfvCiNZjV7gSIiKi3hk3bhz27NkTsG3NmjWX7ZeVlYUdO3YEbFu3bl3A+qXdNp3Nd9LQ0NCvOvsioltG/GHEyzBCRESklIgOIxpjNABAxzBCRESkmIgOIzqDCCN6mWGEiIhIKQMKIxs3boQkSbj//vu73a+goACzZ8+GwWDA6NGj8dxzzw3ksEGjN4kwYgDDCBERkVL6HUb27duHF154IWCmts6cO3cOK1euxMKFC1FYWIiHHnoI9957L/Ly8vp76KDRGc0AAKPsAHp5kyIiIiIKrn6FkebmZqxevRp//vOfERcX1+2+zz33HEaOHIknn3wSEydOxPe+9z1897vfxRNPPNGvgoPJYBJhRCXJ8Dp5eS8REZES+hVG1q1bhxtuuAHXXnttj/vu2bMHy5cvD9i2YsUK7N+/Hy6Xq9P3OBwOWK3WgCUUjFFm/3N7a1NIjkFERETd63MY2bx5Mw4ePIiNGzf2av+qqiqkpKQEbEtJSYHb7UZNTU2n79m4cSMsFot/yczM7GuZvWLQ6eCQxRz89haGESIiIiX0KYyUlZXhvvvuw6ZNm2AwGHr9PkmSAtbbJlW5dHubDRs2oLGx0b+UlZX1pcxeU6kktEIPAHDYGEaIiGj4y8rKwpNPPql0GQH6NAPrgQMHUF1djdmzZ/u3eTwe7Nq1C8888wwcDgfU6sDbIKempqKqqipgW3V1NTQaDRISEjo9jl6vh16v70tp/dYqGRCLZjjtzWE5HhEREQXqUxhZunQpjh49GrDtO9/5DiZMmICf/vSnlwURAMjNzcU///nPgG0ffvgh5syZc9ltipXgkPSADLhaGUaIiIiU0KduGrPZjClTpgQsUVFRSEhIwJQpUwCILpa77rrL/561a9eipKQEDzzwAI4fP46XXnoJL774Ih588MHg/iT95JCMABhGiIho8Hv++ecxYsSIy+6++/Wvfx133303zpw5gxtvvBEpKSmIjo7G3Llz8dFHHylUbe8FfQbWysrKgDsIZmdn491330V+fj5mzJiBX//613j66adx6623BvvQ/eJSibEvHgfDCBFRRJNlwNmizNLLua6+8Y1voKamBjt37vRvq6+vxwcffIDVq1ejubkZK1euxEcffYTCwkKsWLECq1at6vLOvoPFgO/am5+fH7D+yiuvXLbPokWLcPDgwYEeKiScaiPgBtz2FqVLISIiJblswP+kK3PshyoAXVSPu8XHx+O6667DG2+8gaVLlwIA/v73vyM+Ph5Lly6FWq3G9OnT/fs/+uij2Lp1K7Zt24b169eHrPyBiuh70wCARy1aRrwOhhEiIhr8Vq9ejby8PDgcDgDAX//6V3zzm9+EWq1GS0sLfvKTn2DSpEmIjY1FdHQ0Tpw4MfxbRoY6t9oEAJBdNoUrISIiRWlNooVCqWP30qpVq+D1evHOO+9g7ty52L17N373u98BAH784x/jgw8+wBNPPIGcnBwYjUbcdtttcDqdoao8KCI+jHg0YgCr7GTLCBFRRJOkXnWVKM1oNOKWW27BX//6V5w+fRrjxo3zT7mxe/durFmzBjfffDMAcfuW4uJiBavtnYgPI3JbGnWyZYSIiIaG1atXY9WqVTh27Bi+/e1v+7fn5ORgy5YtWLVqFSRJwn/9139dduXNYBTxY0bgaxlRuRlGiIhoaFiyZAni4+NRVFSEb33rW/7tv//97xEXF4f58+dj1apVWLFiBWbNmqVgpb3DlhGtaJKTOGaEiIiGCLVajYqKy8e3ZGVlYceOHQHb1q1bF7A+GLttIr5lRNKLbhq1u1XhSoiIiCITw4hvsJLawzBCRESkhIgPIypfGNEwjBARESki4sOI2iDCiJZhhIiISBERH0a0hmgAgM5rV7gSIiKiyBTxYUTjCyN6mWGEiCjSyL28QR11LRjnMOLDiNbIMEJEFGm0Wi0AwGbjtA4D1XYO285pf0T8PCN6kxkAYATDCBFRpFCr1YiNjUV1dTUAwGQyQZIkhasaWmRZhs1mQ3V1NWJjY6FWq/v9WQwjvpYRLTyAxwWo+5/siIho6EhNTQUAfyCh/omNjfWfy/5iGPG1jACA19EMlSlOwWqIiChcJElCWloakpOT4XK5lC5nSNJqtQNqEWkT8WHEZDTCLaugkbyw25phYhghIoooarU6KF+o1H8RP4DVoNXABj0AwN7arHA1REREkSfiw4hKJcEOAwDAaWtSuBoiIqLIE/FhBAAckmgZcdrYMkJERBRuDCMA7JJoGXHZGUaIiIjCjWEEgFPVFkbYTUNERBRuDCMAXL4w4rG3KFwJERFR5GEYAeBSGQEAHgfDCBERUbgxjABwq0UY8ToZRoiIiMKNYQSARyPCiMyWESIiorBjGAHg0ZjEExfv3khERBRuDCMAZC3DCBERkVIYRtAeRlQudtMQERGFG8MIAPjCiORqVbgQIiKiyMMwAkDSRQEA1B6GESIionBjGAGg0ouWEQ3DCBERUdgxjABQ6aMBABqPXeFKiIiIIg/DCACNL4xovWwZISIiCjeGEQBaoxgzoveyZYSIiCjcGEYAaPQijOhkhhEiIqJwYxgBoDOaAQAGhhEiIqKwYxgBoDeJMSNGOACvV+FqiIiIIgvDCNpbRgAAbg5iJSIiCieGEQDGqGj/cy/v3EtERBRWDCMATHodWmUdAMBua1K4GiIiosjCMAJAr1HBBj0AhhEiIqJwYxgBoFJJsMMAAHC1NitcDRERUWRhGPGxS6JlxMkwQkREFFYMIz4OSbSMOFvZTUNERBRODCM+TpURAOC2s2WEiIgonBhGfFwq0TLi4aW9REREYcUw4uNSi5YRj8OmcCVERESRhWHEx+MLIzJbRoiIiMKqT2Hk2WefxbRp0xATE4OYmBjk5ubivffe63L//Px8SJJ02XLixIkBFx5sHo0JACC7GEaIiIjCSdOXnTMyMvDYY48hJycHAPDqq6/ixhtvRGFhISZPntzl+4qKihATE+NfT0pK6me5oePVipYRONlNQ0REFE59CiOrVq0KWP/Nb36DZ599Fnv37u02jCQnJyM2NrZfBYaLrBFhRHIxjBAREYVTv8eMeDwebN68GS0tLcjNze1235kzZyItLQ1Lly7Fzp07e/xsh8MBq9UasIScVnTTMIwQERGFV5/DyNGjRxEdHQ29Xo+1a9di69atmDRpUqf7pqWl4YUXXkBeXh62bNmC8ePHY+nSpdi1a1e3x9i4cSMsFot/yczM7GuZfaeLAgCo3QwjRERE4STJsiz35Q1OpxOlpaVoaGhAXl4e/vKXv6CgoKDLQHKpVatWQZIkbNu2rct9HA4HHA6Hf91qtSIzMxONjY0BY0+CacebT2HJ8f9GUdQcjP/xxyE5BhERUSSxWq2wWCw9fn/3acwIAOh0Ov8A1jlz5mDfvn146qmn8Pzzz/fq/fPmzcOmTZu63Uev10Ov1/e1tAFR6UXLiMbTGtbjEhERRboBzzMiy3JAK0ZPCgsLkZaWNtDDBp1aHw2AYYSIiCjc+tQy8tBDD+H6669HZmYmmpqasHnzZuTn5+P9998HAGzYsAHl5eV47bXXAABPPvkksrKyMHnyZDidTmzatAl5eXnIy8sL/k8yQG1hROe1K1wJERFRZOlTGLlw4QLuvPNOVFZWwmKxYNq0aXj//fexbNkyAEBlZSVKS0v9+zudTjz44IMoLy+H0WjE5MmT8c4772DlypXB/SmCQGsU3TQMI0REROHV5wGsSujtAJiBOHRoH2a8dS2aYUL0LytDcgwiIqJI0tvvb96bxkdrMAMADLADgz+fERERDRsMIz4Gk28AK7yAx6lwNURERJGDYcRHbzK3r3AWViIiorBhGPExGgxwymoAgMfBO/cSERGFC8OIj0mnQSvERGt2W5PC1RAREUUOhhEfg1YFGwwAAKetWeFqiIiIIgfDiI8kSXD4WkacbBkhIiIKG4aRDuySL4zYGUaIiIjChWGkA6dKdNO47BzASkREFC4MIx04VUYAgMfOMSNEREThwjDSgcvXMuJmywgREVHYMIx04FaLlhGvk2GEiIgoXBhGOnCrTQAAmZOeERERhQ3DSAdejWgZkdkyQkREFDYMIx14taJlROK9aYiIiMKGYaQDmWGEiIgo7BhGOmoLI26GESIionBhGOlIJ8KI2m1XuBAiIqLIwTDSgUoXBQBQe1oVroSIiChyMIx0oNKLMKJhGCEiIgobhpEO1PpoAICWYYSIiChsGEY60BhEGNF5OWaEiIgoXBhGOtAYRTeNTmYYISIiCheGkQ50vpYRA8MIERFR2DCMdKAzijCihxPwehSuhoiIKDIwjHRgMJnbVzgLKxERUVgwjHSgN0bBK0tixckwQkREFA4MIx2Y9Fq0QgcA8Dh4514iIqJwYBjpwKRTwwY9AMDR2qRwNURERJGBYaQDvUaFVl8YsdusCldDREQUGRhGOpAkCXYYAAAuG7tpiIiIwoFh5BIOyRdG7OymISIiCgeGkUs4VG1hhC0jRERE4cAwcgmXyggA8DCMEBERhQXDyCVcal8Y4aW9REREYcEwcgm3WnTTeJ0MI0REROHAMHIJj9oEAJDZMkJERBQWDCOX8GpENw1cDCNEREThwDByCVnbFkZ4bxoiIqJwYBi5hKwV3TQSwwgREVFYMIxcShcFAFC5WxUuhIiIKDIwjFxC0oowonazZYSIiCgcGEYuIelFN43Gw5YRIiKicGAYuYRaHw0A0HjsCldCREQUGRhGLqHWi24arZctI0REROHAMHIJjUG0jOi9bBkhIiIKB4aRS2iNIozoGEaIiIjCgmHkEjpfy4gBdkCWFa6GiIho+OtTGHn22Wcxbdo0xMTEICYmBrm5uXjvvfe6fU9BQQFmz54Ng8GA0aNH47nnnhtQwaGmM4kwooIMuB0KV0NERDT89SmMZGRk4LHHHsP+/fuxf/9+LFmyBDfeeCOOHTvW6f7nzp3DypUrsXDhQhQWFuKhhx7Cvffei7y8vKAUHwoGk7l9hbOwEhERhZwkywPri4iPj8fjjz+Of//3f7/stZ/+9KfYtm0bjh8/7t+2du1aHD58GHv27On1MaxWKywWCxobGxETEzOQcntU3tCKxN9nQi+5gPu/BGIzQ3o8IiKi4aq339/9HjPi8XiwefNmtLS0IDc3t9N99uzZg+XLlwdsW7FiBfbv3w+Xy9XlZzscDlit1oAlXExaNWzQAwA8Dt65l4iIKNT6HEaOHj2K6Oho6PV6rF27Flu3bsWkSZM63beqqgopKSkB21JSUuB2u1FTU9PlMTZu3AiLxeJfMjPD1zph1LWHEUdrU9iOS0REFKn6HEbGjx+PQ4cOYe/evfjBD36Au+++G1999VWX+0uSFLDe1it06faONmzYgMbGRv9SVlbW1zL7Ta9RwS7rAAAOG8MIERFRqGn6+gadToecnBwAwJw5c7Bv3z489dRTeP755y/bNzU1FVVVVQHbqqurodFokJCQ0OUx9Ho99Hp9X0sLCkmSYJcMAABna7MiNRAREUWSAc8zIssyHI7OL4HNzc3F9u3bA7Z9+OGHmDNnDrRa7UAPHTIOXxhx2xlGiIiIQq1PYeShhx7C7t27UVxcjKNHj+LnP/858vPzsXr1agCie+Wuu+7y77927VqUlJTggQcewPHjx/HSSy/hxRdfxIMPPhjcnyLInCoRRlwMI0RERCHXp26aCxcu4M4770RlZSUsFgumTZuG999/H8uWLQMAVFZWorS01L9/dnY23n33XfzoRz/CH//4R6Snp+Ppp5/GrbfeGtyfIshcKiPgBTx2Xk1DREQUan0KIy+++GK3r7/yyiuXbVu0aBEOHjzYp6KU5lIbATcv7SUiIgoH3pumEx61EQAgOxlGiIiIQo1hpBMejS+MsGWEiIgo5BhGOuHRmMQT3puGiIgo5BhGOiFrRRiRGEaIiIhCjmGkM/4w0qpwIURERMMfw0hndCKMqNxsGSEiIgo1hpFOSLooAIDaw5YRIiKiUGMY6YRKL8KIhmGEiIgo5BhGOqH2hREtwwgREVHIMYx0Qq2PBgBovXaFKyEiIhr+GEY6oTWIlhG9ly0jREREocYw0gmtUbSM6GW2jBAREYUaw0gndAZfNw3cgMetcDVERETDG8NIJ/Qmc/uKi/enISIiCiWGkU4YjCZ4ZEmsODnxGRERUSgxjHTCqNfABoNY4f1piIiIQophpBMmrRqt0AMAPI5mhashIiIa3hhGOmHUqWGTRRhx2JoUroaIiGh4YxjphF6j8reMOG1sGSEiIgolhpFOSJIEuyTGjDjtDCNEREShxDDSBadKhBGXnZf2EhERhRLDSBfawoibLSNEREQhxTDSBZfKCADwOtgyQkREFEoMI11wq0UY8TCMEBERhRTDSBfawoiX84wQERGFFMNIF7waEUY4AysREVFoMYx0wR9GeG8aIiKikGIY6YKsNQEAJLaMEBERhRTDSFd8YUTlZhghIiIKJYaRruiiAAAqd6vChRAREQ1vDCNdUOlFGNGwZYSIiCikGEa6oPK1jGi8doUrISIiGt4YRrqgNogwovWwm4aIiCiUGEa6oDFEAwB0XoYRIiKiUGIY6YJGL8KIVnYoXAkREdHwxjDSBa1RhBG97ABkWeFqiIiIhi+GkS7oTWYAgAoy4GJXDRERUagwjHRB72sZAcD70xAREYUQw0gXjHotWmWdWHG2KFsMERHRMMYw0gWTTg0b9GKFLSNEREQhwzDSBZNOjVZfGHHbmxWuhoiIaPhiGOmCQatGqyzCiLO1SeFqiIiIhi+GkS7oNSp/y4izlS0jREREocIw0gVJkuCQDAAYRoiIiEKJYaQbDpUIIxwzQkREFDoMI91w+sMIL+0lIiIKFYaRbrjVRgCA18EwQkREFCoMI91wqRhGiIiIQo1hpBsejS+McAZWIiKikOlTGNm4cSPmzp0Ls9mM5ORk3HTTTSgqKur2Pfn5+ZAk6bLlxIkTAyo8HLwak3ji5AysREREodKnMFJQUIB169Zh79692L59O9xuN5YvX46Wlp5bDoqKilBZWelfxo4d2++iw8UfRtwMI0RERKGi6cvO77//fsD6yy+/jOTkZBw4cABXX311t+9NTk5GbGxsnwtUlE6EEYn3piEiIgqZAY0ZaWxsBADEx8f3uO/MmTORlpaGpUuXYufOnd3u63A4YLVaAxZFaEUYUTGMEBERhUy/w4gsy3jggQewYMECTJkypcv90tLS8MILLyAvLw9btmzB+PHjsXTpUuzatavL92zcuBEWi8W/ZGZm9rfMgdFFAQDU7lZljk9ERBQBJFmW5f68cd26dXjnnXfwySefICMjo0/vXbVqFSRJwrZt2zp93eFwwOFw+NetVisyMzPR2NiImJiY/pTbL2/94zXc9OU9qDCMRfrP9oftuERERMOB1WqFxWLp8fu7Xy0j99xzD7Zt24adO3f2OYgAwLx583Dq1KkuX9fr9YiJiQlYlKDWi5YRjYctI0RERKHSpwGssizjnnvuwdatW5Gfn4/s7Ox+HbSwsBBpaWn9em84tYURrZdhhIiIKFT6FEbWrVuHN954A2+//TbMZjOqqqoAABaLBUajmCBsw4YNKC8vx2uvvQYAePLJJ5GVlYXJkyfD6XRi06ZNyMvLQ15eXpB/lODTGEQY0XntCldCREQ0fPUpjDz77LMAgMWLFwdsf/nll7FmzRoAQGVlJUpLS/2vOZ1OPPjggygvL4fRaMTkyZPxzjvvYOXKlQOrPAy0hmgAgF5mGCEiIgqVfg9gDafeDoAJtk+/PI2r/jFbrPziIqDRhe3YREREQ11IB7BGCp2vZQQA4OL9aYiIiEKBYaQbRqMRLlktVnh/GiIiopBgGOmGUadGK/RihbOwEhERhQTDSDeMWjVsbWHEyW4aIiKiUGAY6YZJp4ZNFmHE7WhWuBoiIqLhiWGkGx27aRytbBkhIiIKBYaRbujUKn8YcbeyZYSIiCgUGEa6IUkSHJKYWdbJMEJERBQSDCM9cKoMAAC3nWGEiIgoFBhGeuAPIw6OGSEiIgoFhpEeuNWim8bLMEJERBQSDCM9aA8j7KYhIiIKBYaRHng1IozInA6eiIgoJBhGeuDRmAAAKodV4UqIiIiGJ4aRHtSYRgMAkmq/ALxehashIiIafhhGelBquRJNshFRjmqg/IDS5RAREQ07DCM90BmM+Ng7U6x89ZaitRAREQ1HDCM9MOrUeM9zpVj5ahsgy8oWRERENMwwjPTApFUj3zsdDpURaCwFKgqVLomIiGhYYRjpQaxJCwd0OKSfKzZ89bayBREREQ0zDCM9yB2TCAB4o8k3buQ4u2qIiIiCiWGkB2OSojAy3oTt7unwqPRA3VngwpdKl0VERDRsMIz0QJIkLJmQDBsMOB7dNpCVXTVERETBwjDSC0smJAMA3myZJTYwjBAREQUNw0gvXDk6HiadGltbpsCr0gE1J4HqE0qXRURENCwwjPSCXqPGVTmJaIYJJbHsqiEiIgomhpFeauuq2ebiJb5ERETBxDDSS9eMF2HkpZoJkFUaoPoYUHNK4aqIiIiGPoaRXkq1GDA5PQaNcjQuJMwTG9k6QkRENGAMI33Q1lXzseQLI8e3KVgNERHR8MAw0gfX+MLIcxcmQJbUQOVhoO6cwlURERENbQwjfTA9IxYJUTqUOUxoTPFdVcPWESIiogFhGOkDtUrCovFJAIA9hgViI8eNEBERDQjDSB+1jRt58eIkABJQfgBoKFO2KCIioiGMYaSPFo5NglolYX+tDvb0tq6afypbFBER0RDGMNJHFqMWc0bFAQAOmReJjeyqISIi6jeGkX5YOlF01fytaYbYULYXsFYoVxAREdEQxjDSD23jRt4rkeAZcYXYePxfClZEREQ0dDGM9MOYpGhkxhvh9HhxOnGJ2MhLfImIiPqFYaQfJEnCEt+9at5yzBYbSz4FmqsVrIqIiGhoYhjppyUTUwAAW8+qIafPAmQvcIJdNURERH3FMNJPV2bHw6hVo8pqx4WMFWLjkb8DsqxsYUREREMMw0g/GbRqXJWTCAB4T74KUOuA0s+AE+8oXBkREdHQwjAyAG1X1WwrUQHz7xEb3/8Z4LQpWBUREdHQwjAyANdMEPepOVTWgNqZ6wFLJtBYBuz+vwpXRkRENHQwjAxAmsWISWkxkGWgoNgGXLdRvPDZ00DNaWWLIyIiGiIYRgaoratmx4lqYMLXgJxrAY8TeO/HHMxKRETUCwwjA3SNL4wUnLwIl1cGrv+tGMx6ZgdvoEdERNQLfQojGzduxNy5c2E2m5GcnIybbroJRUVFPb6voKAAs2fPhsFgwOjRo/Hcc8/1u+DBZkZmLOKjdGiyu3GgpB5IGANcdZ948f0NgLNF2QKJiIgGuT6FkYKCAqxbtw579+7F9u3b4Xa7sXz5crS0dP2Fe+7cOaxcuRILFy5EYWEhHnroIdx7773Iy8sbcPGDgVolYfE4MZB15wnfDKwLHgAsIwHreWDXEwpWR0RENPhJstz/gQ0XL15EcnIyCgoKcPXVV3e6z09/+lNs27YNx48f929bu3YtDh8+jD179vTqOFarFRaLBY2NjYiJielvuSHzz8MVuOdvhciIM2Lng4uhVavEjfPeXA2otMAP9wKJOUqXSUREFFa9/f4e0JiRxsZGAEB8fHyX++zZswfLly8P2LZixQrs378fLpdrIIcfNJZOTEZClA7n61vx9qEKsXHCDUDOMsDr4mBWIiKibvQ7jMiyjAceeAALFizAlClTutyvqqoKKSkpAdtSUlLgdrtRU1PT6XscDgesVmvAMpiZdBr8x9WjAQDP7DgFt8cLSBJw/f92GMzKu/oSERF1pt9hZP369Thy5Aj+9re/9bivJEkB6209Q5dub7Nx40ZYLBb/kpmZ2d8yw+bOeaMQH6VDca0N/zziax1JGANcdb94zsGsREREnepXGLnnnnuwbds27Ny5ExkZGd3um5qaiqqqqoBt1dXV0Gg0SEhI6PQ9GzZsQGNjo38pKyvrT5lhFaXX4HsLswEAf9hxGh6vr1tmwY98g1nLgV2PK1ghERHR4NSnMCLLMtavX48tW7Zgx44dyM7O7vE9ubm52L59e8C2Dz/8EHPmzIFWq+30PXq9HjExMQHLUHBXbhZiTVqcvdiCf7W1juhMwPWPieefPQNcPKlcgURERINQn8LIunXrsGnTJrzxxhswm82oqqpCVVUVWltb/fts2LABd911l3997dq1KCkpwQMPPIDjx4/jpZdewosvvogHH3wweD/FIBGt1+B7CzppHRm/Ehi7XAxmffc/Aa9XwSqJiIgGlz6FkWeffRaNjY1YvHgx0tLS/Mubb77p36eyshKlpaX+9ezsbLz77rvIz8/HjBkz8Otf/xpPP/00br311uD9FIPI3fOzEGPQ4HR1M949Wik2+gez6oFzu4CC/1W2SCIiokFkQPOMhMtgn2fkUk99dAq//+gkxqVE4/37roZK5RuoW7gJeHudeP5vrwGTblSuSCIiohALyzwj1Lk1V2XBbNDg5IVmvH+sw+Ddmd8GrvyBeL51LVD1pTIFEhERDSIMIyFgMWrx3avE2JGnPz4Fr7dD49PyR4HRiwGXDdh8B9BSq0yRREREgwTDSIh896psmPUanKhqwodfdWgdUWuA214G4rKBhlLg73cDnuExEy0REVF/MIyEiMWkxZqrsgAAT318GgFDc0zxwB1/A3TRQPFuMSEaERFRhGIYCaF/X5CNKJ0axyut2P7VhcAXkycCt/wZgATs+zNw4BUlSiQiIlIcw0gIxZp0uHt+FgDgqY9P4bILlyasBJb8XDx/50GgpHd3MSYiIhpOGEZC7HsLR8OkU+NYhRU7TlRfvsPCB4FJN4kJ0d78NtAw+Ke+JyIiCiaGkRCLj9LhrtwsAF20jkgScNOfgNSpgK0G2PwtwGkLf6FEREQKYRgJg/9YmA2jVo0j5xuRf/Li5TvoooBvvgGYEoGqI8DbPwQ87vAXSkREpACGkTBIiNbjztxRAIAnP7pk3pE2sSOB218HVBrg2FbgpeVA9YkwV0pERBR+DCNh8h8LR8OoVeNwWQP+vPts5zuNmg/c9hKgtwDlB4DnrwY+eRLwesJaKxERUTgxjIRJklmP/141CQDw+AdFOFTW0PmOk24E1u0FcpYBHgfw0cPAi8uBiyfDVywREVEYMYyE0TfnZuKGqWlwe2Xc+7dCNNm7mHk1Jh1Y/Xfg688A+higfD/w3ALg06fYSkJERMMOw0gYSZKE/7llKkbEGlFaZ8Mv3vry8qtr2ncGZt0J/HAPMGapaCXZ/t/AS9cBNafCWzgREVEIMYyEmcWoxdN3zIRaJeHtQxX4x4HzPbwhA/h2HvD1P4hWkvNfiFaSz/4AuB3hKZqIiCiEGEYUMHtUHB5YNg4A8PC2Yzhzsbn7N0gSMOsuXyvJEsBtBz78BfC7icCH/wXUnglD1URERKHBMKKQtYvGIHd0AmxOD+79WyEc7l6MBbFkAN/eAqx6GjCnAbZa4LOngT/MAl5dBXy5BXA7Q188ERFREElyl4MWBg+r1QqLxYLGxkbExMQoXU7QXLDacf1Tu1HX4sR3rsrCw6sm9/7NHjdw6gNg/8vA6Y8A+H6NUUnAjNXA7LuB+NEhqZuIiKg3evv9zTCisB0nLuC7r+wHALx49xwsnZjS9w9pKAUOvgYcfB1ormrfPnoxMG8dMHaZ6OohIiIKI4aRIeSRfx7Dy58WIz5Kh/fuW4iUGEP/PsjjAk6+L1pLzuyAv7UkZSqw8AExh4lKHbS6iYiIusMwMoQ43B7c8qfPcKzCitzRCdj0vSuhVg2wJaO+GPjizyKYuFrEtoQcYMGPgKn/Bmh0A66biIioO739/uYA1kFAr1HjD3fMhEmnxp6ztXiuIAhXx8RlASt+A/zoS2DRzwBDLFB7Gnh7HfD0TODz53l3YCIiGhTYMjKI/H1/GX78jyNQqyT84oaJWDM/C1Kwxno4mkQryZ5ngOYLYpspEcj9ITDpJjHrq9YYnGMRERGB3TRDkizLeGjrl/jbF6UAgBumpuGxW6fCbNAG7yAuO3Dor8CnT4qBrx2ZEoCYEWKxjBABJSZDPKZMBkzxwauDiIiGPYaRIUqWZbz6WTF+8+5xuDwyshOj8Oy3Z2FCapB/bo8L+DIP+Pw54GIR4Oqhy0ZrEuNNctcDOlNwayEiomGJYWSIO1haj/V/PYiKRjsMWhUevWkqbpudEZqDyTLQWg9YKwBruVgay33r54G6YqDR14oSkwEsewSYcisvFyYiom4xjAwDdS1O3P/mIew6eREAcPucTDxy42QYtGG+PFeWgWNbgO0PA41lYlvGFcB1G4GMOeGthYiIhgyGkWHC65Xxx52n8buPTkKWgYlpMXh29SxkJUaFvxhXqxgAu/v37ZcLT7sdWPqwGGNCRETUAcPIMPPJqRrct7kQtS1OmPUaPP6NabhuSpoyxVgrgR2/FgNhAUBjBBbcD8y/l+NJiIjIj2FkGKpqtGP9Gwexv6QeAHDtxGTcf+04TBlhUaagikLg/Q1A6R6xHp0KzFwt7o2TMEaZmoiIaNBgGBmmXB4vnvigCH/efRZe329u2aQU3Ld0rDKhRJaBr94Gtv9X4KXCI3NFKJl8E6A39/7zvF5xfx21HjBYALUm6CUTEVF4MIwMc2cuNuOZHafx9qFyfyhZPikF9107FpPTFQglbgdQ9C5Q+FfgzMeA7BXbtVHinjgzVwOjrgq8AsfZAlQfB6qOAhe+BKq+BC4cA5xN7fvoLYDRAhjjLl9i0sWdiePHAJYM3neHiGiQYRiJEKerm/GHHaew7XAF2n6TKyan4P5rx2FimkLnyloBHN4sxpTUnm7fHpcFjL9BXDp84Uug9gz8N/PrSFIDsqdvx1RpxefHjxZdRPGjgfhswJwmZpbVRvkeTWxtISIKE4aRCHO6uglPfXwa/zrSHkqun5KKe5aMxaR0hc6ZLANlXwCHNgFfbg1s8WgTlQykTgVSp4i7C6dOARLGitfsjWL+k7bF3tD+3FYnLjOuPQPUnwM8zt7XpdKKUKI1igG3MSPEXY3HLAnKj01ERALDSIQ6eaEJT398Cu8crfSHkmsnJmPdNTmYOTJOucKcLcDxfwJln4sWjJQpIoREJw/8s70e0RpTdwaoOyuW2rNivaVGXJLssqHTVpiOcq4Flv0aSJk08JqIiIhhJNIVVTXhDzsCQ8mCnESsX5KDK7Pjg3cDvqFClsW4FpfNF058AcVlEwNwv/gz4HUBkgqYeSdwzUOAOVXpqomIhjSGEQIgBro+m38GbxWWw+0b6TpnVBzWL8nBonFJkRdKulJ7Bvjol8DxbWJdGwVcdR8wfz2gG8AEc7Isgo+zBXA2i+fmVN50kIgiAsMIBSirs+H5XWfw//afh9MtrnSZOsKCddfkYPmkFKhUDCUAgNK9wAc/B8r3i/XoVGDJL4AZ3xJX68iyGK9iPS/u39N4vv15UyVgt4rZaZ0dls66h+KygPRZwIjZwIhZQNr0gYUeIqJBiGGEOnXBasefd53FXz8vRatLXLEyNjkaP7xmDFZNS4dGrVK4wkFAloFjW0VLSUOJ2BaXLcJIYzngbu3f52pNgMYAtNZd/pqkApImAiNmipCSNh1IyAGMsf39KYiIFMcwQt2qa3Hi5U/P4ZXPitFkdwMAMuONWLtoDG6dlRH+m/ENRm4H8MULwK7HxZU9HUUliblNYkaIR0uGmPfEYAF00aKVQ2sKfK7yBb3WeqDiEFB+QMxiW34QaKrovAZTIpA4VlyunDBWBJTEsaJlRaMHHM2idaaxTCwNZR3Wz4tWnPhsIHkikDQBSJ4EJE8AYrPa67mULAMtF4G6c+JKpbpzYkK7mHRgwkogbWbX7yUi6oBhhHrFanfh9T0leOmTc6htEZfHJpv1+D9Xj8YdV4xElJ5zcsBWBxTvFhOtWTIAczqgNQT3GNZKoOKgCCYVB8VkcE2VXe8vqcTMtpeGpN7SmoDEcSKcJOaIgFR3DqgvFo9tN0LsjDkNGL8SmHADkLUQ0Oi6P5Ysi8+tPCRCmMsGTL5ZzNLLMUtEwxrDCPVJq9ODN/eV4vldZ1HZaAcAxJq0+M78bKyZnwWLSatwhRHI0SQG1taeFkvNqfbnzub2/fQWIDbT10KT2d5SEzsSMMSKS5yrvwKqT4iQU3MS8Dh6OLgkPiMuS7SsWEaKiepOf3TJsWOAsctEOBm7TKw3lIjQUVHYHkDsDZcfInEcMOsuYPodQFTiAE9WH7XUAOf3i7FB5QcAtQ6YfAsw8Wscu0MURAwj1C9OtxdvFZbj2YIzOFcj/nccpVPjuilpGBFnRJrFgFSLAakxBqRZDLAYtbwiJ9xkGWi+IFoz2rqG+sLjFt0v1cfFUncGMMaL0BGXLR5jR4puoEu57KKV6MS/gKL3RB1tVFpAHy3qupRKC6RMBtJniOMf29re+qLSihAwew2QdXXwu4DcDnHLgfP72gNIfXHn+2qjgImrgOm3A9mLen+LAWuFmOCv/hwwYg4wch6gZoAnYhihAfF4Zbx7tBJ/3HkaJ6o6mTnVx6BVITVGBJQRsSaMTYnG+BQzxqWakW4xMKgMZ16vaFUoegc48Y5ocQECg0faDPGYPCkw3NitwJd5wMFXRQtKm7gs0VoyYzUQndK7bhxHk+jmsvquaLKWi3BgrRRXOl0s6nyG3sRxIjhkzAZaaoHDfxNhoo05DZh6GzDtm2Jm4DZuB1B5BDj/hQgg5/eJY3aktwA5S4Fx14nJ9KISev45/J/vFGN0opP6HjSHi7Z5gTR6duUNcQwjFBSyLGPXqRocKWtApdWOC412VDbaUWW1o66l+ynYo/Wa9nCSYsb4VDMmpJqREN3J/7hp6Ks9I7pwkiZ03qrSlcrDwIFXgaN/BxzWwNdUGhFu1FrRSuF/rhHjZmy1l7+nM6YEX/DwLemzLr9SSZZFsDi8GTi2JbCFJ2UKkHklUHVE1HtpuJHUIoDFZQElnwG2mg6vqYCMucC4FSKcJPtm+LXVigBXcwqoPSUea06JVhvZI+5cPf56cVn5mKV9u6eS1yvGHh3fBtSXiMvHR84XwTBULTa1Z4Cj/xCTByaMFWOREsYChh7+zZZlESIrCgMXW63oPjNYRHejwSJ+Z23rxljxe828EkifGZobZbrsoqux7HMRPFtqfMdvW2IuWbeIQe1J44NfyxDFMEIhZ3d5UG11oMpqR2VjK0prbThZ3YyTVU04c7HZP8laR5IE3DxzBH563QSkxAR5ECgNbc4W4NhbwIFXRKtDX+gtQEya6LYyp4vHmDTxPGmc6H7qy/+w3U7g1IfAkc3AyQ8uDx+mBCDjCiBzrngcMat9rElbEDj5vliqjga+15wmJr/rbBxNG7U+cFxPVBIw9d+AGXeI2yh0xuMCSj4Fjv9LdKN1NgBaaxLBaNRVwKhcEdB0ph5PR5ecvhmMC18Xx+5MdKrvijDflWCJ48RdvTsGj47dff1hjANGLxahLWep+P33R1NVe/Ao+1yMd/K6+v45GXOB3PWiyy/C7ybOMEKKcrq9KK5tQVFVE05eaFua/eNQTDo1frh4DL63cDQvI6bL2a2imd7rEl+yXrfvsW3dI7ZFJYovd3106GpprRdjXGrPAKnTRADpS7hpPC+CzckPgLP5gNvue0ESA48Txnb4sh4nnpvTRCvM4c3Akf8X2NKSMlWEkqnfEFdUndnpG8PzbmBrji4aGLtcdDGVHxRh4dLxPCqtr9Vknmj9aTt+d4N4ZVmErYOvi662tpYpSSXCgGUEUHNatPq0VPfuHElq0aKWPlO03qTPAhJGi4Da2iCuGrP7Hlsb2p83lAHFnwCOS64qS5ooQsmYJSJ4afTiPS014rL15mrx6F+/IM53Q+nltUUlidaXzCvF78vRJP582hsDF4dvW83J9vAaOwqY90Ng5rdD+2d0EGMYoUHpUFkDfvXPYzhY2gAAGBFrxEMrJ2Ll1FSOL6Hhz2kTX+SGWDF3jNbY83s8LnEV06E3REtL2xedpBZfsi5b+76mBHFl08RVYgBux0vQvV6gpkiEkpLPxNLV5eOWTBFMksa3P5rTxPEPvg5UH2vfNy5LfNlO/5YIIh21NogQV3OyvSuq9rQINOkzfOFjpghC/W2h8bjFoOTTHwNnPhbBq+Osx2qdOF5vWjgkFZA8Gci8whdArhA/X1/+bWquFve62veX9gkODRZg9neAK7/f/1abS3m9IoR53KJulUr8mZBUojVG8q2r1OLPjN3aHpgc1g7rHR5nrhbdjUHEMEKDlizL2Ha4Ao+9d8J/GfEV2fH4769NwpQRnQ/Y83hlnLzQhP0l9ThYUo/D5xsQb9LhqpxELBybiOmZsdBy9lga7mx1YjzLob+137IgJkNcjTRxFZA5r/djS2RZDNgt2SO6xS6eFGHFVtvzezUGYOLXgVl3AqMWDK5J8Gx1wNmdwOkdIpx0DFx6i2hNi0oSj9HJ4rkpUYxxGTGn5zEuveW0iUHRe/8kAhggxjpNuVWEElOiaCVztbY/ulrFDM8uu3hsbRCtWbY6EWzaHlvrxSJ7g1Nrm1tfFIO2gyhkYWTXrl14/PHHceDAAVRWVmLr1q246aabutw/Pz8f11xzzWXbjx8/jgkTJvTqmAwjw5PN6cbzBWfx/K4zsLu8kCTg32Zn4sEV42HUqXGotAH7S+pwoKQeh0ob0ORwd/lZ0XoN5o2Ox4KcRCwYm4gxSdFsaaHhrfaM+PJKmRzcK05aakUouVjkG1RbJIJKYxmQNk3c1XrqN4bGrQraJtxT60T46MvA6mDxeoFTHwCfPQOUfBL+43ekixZzAbUNvtXHBD5OuVXciiKIQhZG3nvvPXz66aeYNWsWbr311l6HkaKiooBCkpKSoFb3bqwAw8jwVt7Qiv997wS2HRZTous1Krg8Xlw6/jVKp8bMkXGYNSoOM0fGoqrRjk9O1+Cz0zWotwU2waZZDLgqJxFLJiRj6cRk6DUcl0I0IF5PxA/GHLDyg8CeP4oxPpBEN5rG2MWjQXTnmeLEPECmePFojGt/bor3dUN5xe9H9oorsQLWveL3po9R5PcXlm4aSZJ6HUbq6+sRGxvbr+MwjESGAyV1eOSfX+HIeTEYLSPOiNmj4jBnlAggE1JjoO7k7sJer4xjFVZ8croGn5y+iH3F9f47EwNAnEmLm2dm4Pa5mRifau5zXY02F8rqbZiQauaNBImI+qC3399hu/HIzJkzYbfbMWnSJPziF7/otOumjcPhgMPRflmb1dqLeQRoyJs9Kh5v/fAqfFVpRZJZ3+tLf1UqCVMzLJiaYcEPFo+B3eXBvuI67D5Vg22HKlBlteOlT8/hpU/PYUZmLL45NxNfm56O6C7uu2NzurG/uB6fnqnBnjO1+LK8EV5ZDLZdPW8kbp+TyblSiIiCKOQtI0VFRdi1axdmz54Nh8OB119/Hc899xzy8/Nx9dVXd/qeX/7yl3jkkUcu286WEeorj1fGrpMXsXlfKT4+Xu2f+8SkU+Nr09Jw+9xMTB0Ri8PnG/Dp6Rp8dqYWhaX1cHkC/1oYtCrYXaK1RadW4WvT03BXbhZmZMaG+0ciIhoyBk03TWdWrVoFSZKwbdu2Tl/vrGUkMzOTYYQG5GKTA1sOnseb+8tw9mL7XWk1KumyCdrSLQbMz0nEVTkJyB2diFiTFu8cqcRre4px+Hz7nAbTMyy4KzcLN0xL43wpRESXGNRh5De/+Q02bdqE48eP92p/jhmhYJJlGftL6vHmvjK8c6QSrS4P4qN0yB2TgKvGJGL+mASMSjB1eTXOobIGvLanGP86XAmnR7SWxEfp8G9zMnFldjwy4owYEWeESRe2XlAiokFpUIeR2267DXV1ddixY0ev9mcYoVBpdrhxscmBUfEmqDoZHNud2mYH3txfhk17SlDRaL/s9fgoHTLijCKcxBqREWdCRpwRmfEmjIw3hbQlxe7y4Hy9DQ02F6ZlxEKn4cBbIgq/kA1gbW5uxunTp/3r586dw6FDhxAfH4+RI0diw4YNKC8vx2uvvQYAePLJJ5GVlYXJkyfD6XRi06ZNyMvLQ15eXj9+LKLgitZruhzI2pOEaD1+uDgH/2fhaHx8ohpvFZajuNaG83U2NDncqGtxoq7F6b866FIpMXqMSojCqHgTRiWYxPMEE0bFR8Fi6vlmZs0ON0pqW1Baa0NxrQ0ltS0o8T1WWu1o+29GYrQed1yRiTuuGIn02F7M+HmJxlYX9p6txYhYY5eT0hERDUSfW0a6msTs7rvvxiuvvII1a9aguLgY+fn5AIDf/va3eOGFF1BeXg6j0YjJkydjw4YNWLlyZa+PyZYRGmoaW10or2/F+Xobzte3oryh/XlpnQ1N9q4ncAPgv4S57a9n21/SvvxtjdZroFFLaPDNwaKSgGWTUnDnvCzMH5PQbUtQTbMD27+6gPe/rMJnZ2r8A3rnjY7H9xeNweJxSZxUjoh6xOngiQYpWZZRb3OJVo06G0pqbSj2tXCU1NlwscnR84f4xJm0GJUQhawEE0b6HttaWBKidHB7ZXx47AJe31uMvWfr/O8bnRiF1fNG4bZZGf5WmIqGVnxwrArvf1mFfcV1AZPOjUowoby+1T/Qd0KqGd9fNBpfm5beq2n4nW4vDpTUY9epi6hpcmDhuCQsmZDc71YpIhoaGEaIhiib0w1rqxuSBPjbHqS2B/FEksRMtWZDz905bU5eaMKmvSXYcrAczb6p9Q1aFVZMTkVxTUvAVUIAMHWEBddNScWKySnISTajsrEVL31yDm98XooWpweAmHvl3xdk4/a5mYjqECxkWca5mhbsPlWDXScvYs/ZWth872mjU6uwcGwiVkxJxbKJKYiL0vXhLBHRUMAwQkSdana48VZhOV7fU4KiC03+7ZIEzB0VjxW+AJIR1/ldVBttLmz6vAQvf1qMmmbRimMxanF37ihMTIvB7tMigJyvbw14X2K0DgvHJiHJrMdHX13A2Zr2y6vVKglXZsfj+impWD45tdcT3hHR4MYwQkTdarvE+cNjVchOjMaySSlIMvd+Zlm7y4MtB8vxwq4zKK61Xfa6Tq3CnKw4LBybhKvHJWJiaox/nIosyzhV3Yz3vxTdQl9VBs6yPGtkLJZMSMaiccmYnB7T5yudgsnl8WL3qYsoqbXhiux4TEqL4XgZol5iGCGisPB4ZXx4rAovf1qMhlYn5o9JxNXjEjFvdEKv51oprbWJ8SrHqnCgpD7gtYQoHa4el4RF45KwcGxir6fibxubE6VX9/lGibIs42h5I7YcLMc/D1egtsXpfy0lRo/F45JxzYQkXJWT2KeuMqJIwzBCREPSBasdHx2/gF0nL+LT07X+8S2A6EqaNsKCReOSsGBsEgCgymrHhUY7qqz2gOfVVgecHi+0agmT0mIwIzMW0zNjMSMzFlkJUZ22tpyvt+HtQxXYcvA8znSYpTcxWoeJaTHYX1yPVlf72BeNSsLcrHgsHp+EayYkY2xyNFtNiDpgGCGiIc/p9uJgaT0KTl5EQdHFy7pz+ivGoMH0zFjM9AWU2mYn8g6ex+fn2q840mtUWD45FbfMHIGFYxOhUatgd3nwxbk65BddRH5RdcC4FwBIsxgwPtWMrLYrmxKjkJUQhYw4Y6+uOiIabhhGiGjYqbbaRTA5eRFfnKuDUadGSowBqTEGpFraH1N8j0nRelyw2lFY1oBDpQ04fL4BX5Y3wuH2dvr5kgTMy07AzbNG4PopqT12wRTXtCC/qBr5Jy9iz5naLj9XrZIwItaIrEQxyV18lA5mg8a3aBGt11y2btSqFR0r01FVox1HzjfA4fZCrZKgkgCVJEElSVCrJEgSfNslxJl0yIw3svuKADCMEBF1yuXxoqiqKSCgaNUqrJqehptmjOjXLLUA0Or04FBZA4prW1Bc24KSGjF/TEmtLaBrpy/UKglatQStWuVbxHOdb12lktonxpMBGbLvEb5tMgxaNXKSozE+1YwJqWaMT41BusXQZXeSxyujqKoJB0rqsL+kHvuL61He0Nrpvt2xGLXIjDcis8NtEDLixHqqxYBovYZdWhGAYYSIaBCQZRnVTQ4U1/im669rQYPNhWaHG012N5rtbljt7etNdhe8If5X2azXYFyq2R9Q0ixGHKtoxIGSehSWNgSM0wHE7L3jUsywGLWQZcAjy/DKMrxeGV5ZBBivLMPjlVHT7EC9b9bf7ujUKsRFaRFn0iHOpEN8lA5xUVrEm3SIi9LBYtTCoFVDr1H5H/UaNfRaFQy+R6NODTNDzaDGMEJENATJsgyb0wOH2wu3xwunxwuXR4bL44XT7YXL44XbK8PlFo9icjzpskny2rY12d04eaEJRVViOXOx2T+Tblei9RrMHBmL2aPiMGdUPGaMjO3TbLnNDre4/UFdK8rqbSirE7dDKKtv9d+7KVjMeg0yffd3Ghlvwsi2x3gT0mMvH6vj8cpodXlgc7hhc3rQ4nSj1elBWqy4oWV/uT1eHCxtwAWrHVePTerV/aUiAcMIERFdxun24lxNC05UWVFU1YSTF5pQ3mDH+JRozM6Kx+yRcRifavbfHykUWp0e1NmcqPfdTLLe1vboEttsTlhbXXC4vLC7PXC4vHC4RUCzu9ofe2pBUqskpMYYIEnimC1ON+yuzsf1AEB2YhQW5CTiqpxE5I5JgMXYfaCobrKjoOgi8osuYvepi7D67jmlU6uwbFIKbpud4R/83FcOtwdNdjdaHG60ODywOd1o9gWoZocbNocbLU4Pks16zBudgIw446BsIWIYISKiYa3V6cH5epv/Hk+ldYGLs4sBxYAYrByl08CkU8OgVaO8oRWeDulGJQHTM2OxMCcRC8YmYUZmLNQqCYfK6pFfdBE7i6rxZXng1V2xJi0So/U4Xd3s35Zk1uPmmSNw2+wMjEsxd1qLLMs4X9+Kg6X1OFBSj4Ol9The2RRQT09GxBpx5eh4zMtOwLzRCciMHxzhhGGEiIgiltcrxuqcr7dBpZL8wcOkUyNKr4Feowr4srbaXdh7phafnK7BJ6dqLrtsO0qnhlaj8t8Fu820DAsWj0vC4gnJmJ4hAsuxikb848B5vH2oAnUdJsyblmHBrbMycN2UVJTV2fzB40BJg//WCpdqqzfK/6hBlF4Nk14Dk1aNszUtOFzWcFnXW5rFgHmjE3BldjzmZMUjOzEqpK1dXWEYISIi6qfyhlZ8cuoiPjldi09P1/hDRYxBg6vHJWHx+GQsGpfU7S0UnG4v8ouq8Y8D57HjRHW3Y3W0agmT0i2YPTIOs0bFYtbIOKTGGHp1ebfN6caBknp8frYOe8/W4vD5Brg8gcfSa1QYlyIGLE9IixGPqeZez2jcXwwjREREQeD1yjheZYXT7cXUEZZ+jQGpbXZg2+EK/OPAeRyrsCIxWo/ZvtAxe1QcpoywwKDt220LutLq9OBgaT32nq3F3rO1+LLc2uXl5UlmvT+YfH36CEzNsASlhjYMI0RERINQk90V1nlWPF4ZpXU2FFVZcbyyyT94uaTOho4J4MnbZ+CmmSOCeuzefn/3/lotIiIiGrBwz06rVknIToxCdmIUrpuS5t/e4hCXfZ/wXfY9IzM2rHV1xDBCREQUgaL0GswcGYeZI+OULgW8cxMREREpimGEiIiIFMUwQkRERIpiGCEiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBTFMEJERESKYhghIiIiRTGMEBERkaKGxF17ZVkGAFitVoUrISIiot5q+95u+x7vypAII01NTQCAzMxMhSshIiKivmpqaoLFYunydUnuKa4MAl6vFxUVFTCbzZAkKWifa7VakZmZibKyMsTExATtc6lzPN/hxfMdXjzf4cXzHX79OeeyLKOpqQnp6elQqboeGTIkWkZUKhUyMjJC9vkxMTH8wxxGPN/hxfMdXjzf4cXzHX59PefdtYi04QBWIiIiUhTDCBERESkqosOIXq/Hww8/DL1er3QpEYHnO7x4vsOL5zu8eL7DL5TnfEgMYCUiIqLhK6JbRoiIiEh5DCNERESkKIYRIiIiUhTDCBERESkqosPIn/70J2RnZ8NgMGD27NnYvXu30iUNC7t27cKqVauQnp4OSZLw1ltvBbwuyzJ++ctfIj09HUajEYsXL8axY8eUKXYY2LhxI+bOnQuz2Yzk5GTcdNNNKCoqCtiH5zx4nn32WUybNs0/8VNubi7ee+89/+s816GzceNGSJKE+++/37+N5zu4fvnLX0KSpIAlNTXV/3qoznfEhpE333wT999/P37+85+jsLAQCxcuxPXXX4/S0lKlSxvyWlpaMH36dDzzzDOdvv7b3/4Wv/vd7/DMM89g3759SE1NxbJly/z3IKK+KSgowLp167B3715s374dbrcby5cvR0tLi38fnvPgycjIwGOPPYb9+/dj//79WLJkCW688Ub/P8g816Gxb98+vPDCC5g2bVrAdp7v4Js8eTIqKyv9y9GjR/2vhex8yxHqiiuukNeuXRuwbcKECfLPfvYzhSoangDIW7du9a97vV45NTVVfuyxx/zb7Ha7bLFY5Oeee06BCoef6upqGYBcUFAgyzLPeTjExcXJf/nLX3iuQ6SpqUkeO3asvH37dnnRokXyfffdJ8sy/2yHwsMPPyxPnz6909dCeb4jsmXE6XTiwIEDWL58ecD25cuX47PPPlOoqshw7tw5VFVVBZx7vV6PRYsW8dwHSWNjIwAgPj4eAM95KHk8HmzevBktLS3Izc3luQ6RdevW4YYbbsC1114bsJ3nOzROnTqF9PR0ZGdn45vf/CbOnj0LILTne0jcKC/Yampq4PF4kJKSErA9JSUFVVVVClUVGdrOb2fnvqSkRImShhVZlvHAAw9gwYIFmDJlCgCe81A4evQocnNzYbfbER0dja1bt2LSpEn+f5B5roNn8+bNOHjwIPbt23fZa/yzHXxXXnklXnvtNYwbNw4XLlzAo48+ivnz5+PYsWMhPd8RGUbaSJIUsC7L8mXbKDR47kNj/fr1OHLkCD755JPLXuM5D57x48fj0KFDaGhoQF5eHu6++24UFBT4X+e5Do6ysjLcd999+PDDD2EwGLrcj+c7eK6//nr/86lTpyI3NxdjxozBq6++innz5gEIzfmOyG6axMREqNXqy1pBqqurL0t8FFxto7J57oPvnnvuwbZt27Bz505kZGT4t/OcB59Op0NOTg7mzJmDjRs3Yvr06Xjqqad4roPswIEDqK6uxuzZs6HRaKDRaFBQUICnn34aGo3Gf055vkMnKioKU6dOxalTp0L65zsiw4hOp8Ps2bOxffv2gO3bt2/H/PnzFaoqMmRnZyM1NTXg3DudThQUFPDc95Msy1i/fj22bNmCHTt2IDs7O+B1nvPQk2UZDoeD5zrIli5diqNHj+LQoUP+Zc6cOVi9ejUOHTqE0aNH83yHmMPhwPHjx5GWlhbaP98DGv46hG3evFnWarXyiy++KH/11Vfy/fffL0dFRcnFxcVKlzbkNTU1yYWFhXJhYaEMQP7d734nFxYWyiUlJbIsy/Jjjz0mWywWecuWLfLRo0flO+64Q05LS5OtVqvClQ9NP/jBD2SLxSLn5+fLlZWV/sVms/n34TkPng0bNsi7du2Sz507Jx85ckR+6KGHZJVKJX/44YeyLPNch1rHq2lkmec72P7zP/9Tzs/Pl8+ePSvv3btX/trXviabzWb/d2OoznfEhhFZluU//vGP8qhRo2SdTifPmjXLfykkDczOnTtlAJctd999tyzL4vKwhx9+WE5NTZX1er189dVXy0ePHlW26CGss3MNQH755Zf9+/CcB893v/td/78bSUlJ8tKlS/1BRJZ5rkPt0jDC8x1ct99+u5yWliZrtVo5PT1dvuWWW+Rjx475Xw/V+ZZkWZYH1rZCRERE1H8ROWaEiIiIBg+GESIiIlIUwwgREREpimGEiIiIFMUwQkRERIpiGCEiIiJFMYwQERGRohhGiIiISFEMI0RERKQohhEiIiJSFMMIERERKYphhIiIiBT1/wHPyF3gFhmxJwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama2(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama2_L{model_config.n_layers}xH{model_config.n_heads}xN{model_config.max_seq_len}xD{model_config.dim}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama2, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 16])\n",
      "model params: 541824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama2(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is all or ignoble hear\n",
      "Your highly indred: for they villain,\n",
      "Sit will the plague my furn sweetly\n",
      "Tha\n",
      "\n",
      "tarding thee of our more end summers presence.\n",
      "\n",
      "BRUTUS:\n",
      "Are you any gentle shut, then days ob a thun\n",
      "\n",
      "Is prayers, something of that? Corioli young to a persont.\n",
      "\n",
      "JULIET:\n",
      "Then fruing the prayer-hand not \n",
      "\n",
      "\n",
      "Hath he? Cry Richard, and bridable, with\n",
      "Rhom when I'ld thousand'st to burst be or no\n",
      "he sad ne'er \n",
      "\n",
      "GLOUCESTER:\n",
      "What said his will stay he willed with the palment of God Camillo:\n",
      "You no: and shall do \n",
      "\n",
      "Sirrt what you man ill the glove deads,\n",
      "But fear in ready. The king on that best him;\n",
      "Stay, sorrow s\n",
      "\n",
      "EAll, let for itsenchering to the hope that we?\n",
      "And I'ld please of your son, where it in these\n",
      "that \n",
      "\n",
      "Shocad incents loves come to deserves,\n",
      "With whom for my brawn, pleasured and the\n",
      "Lone him ateth thee\n",
      "\n",
      "\n",
      "AGBENVOLIO:\n",
      "Did Clifford, and traords, steed, which hands,\n",
      "Thou hadst stain me him and the prince,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "That's nothing?\n",
      "\n",
      "STANLEY:\n",
      "My change ha!\n",
      "What steps noble where I did, garlant;\n",
      "The hold what he sp\n",
      "\n",
      "\n",
      "\n",
      "CAMILLO:\n",
      "Nay, good, sir; some noble proud not engthing,\n",
      "Save the lie ouncels\n",
      "Is plots murder's war\n",
      "\n",
      "Wherefore, thou cannot be nightness be the cried\n",
      "Hypprincely tended carried me on your hands;\n",
      "Be cal\n",
      "\n",
      "Second Sir. Tut, sir.\n",
      "\n",
      "VIRGILIA:\n",
      "Come his tongue?\n",
      "\n",
      "BISHOP VERS:\n",
      "Your sickles thee no from Agalisfied\n",
      "\n",
      "\n",
      "FLORIZEL:\n",
      "Yes, I never me and we well, Have in like my reign,\n",
      "By minior\n",
      "With them and us; my lord. \n",
      "\n",
      "\n",
      "Than spirit, time marriagenet me resols,\n",
      "But hum it, which gapes car of then my Lady,\n",
      "That hath all\n",
      "\n",
      "JULIET:\n",
      "It even my pray, since ease, I must would I centrey\n",
      "Is throat he doth cause audies' than the\n",
      "\n",
      "Is become of man, most his famentage untoisly\n",
      "With told will cheerly and in the beast,\n",
      "His clean tha\n",
      "\n",
      "arm left ask, my light. Go, gillingnified sadvantage\n",
      "not see no put yours. But I well is love,\n",
      "For l\n",
      "\n",
      "\n",
      "Brear all doubt me twenched ale a under both,\n",
      "Yourself, and him told we'll be fly powerly;\n",
      "On humbe\n",
      "\n",
      "the guod do\n",
      "Distress designs name hollow this heaven;\n",
      "It do look up hope's wandering to go.\n",
      "\n",
      "LADY CA\n",
      "\n",
      "\n",
      "MOPSA:\n",
      "Is sturns your brother inclaim, the man.\n",
      "\n",
      "SICINIUS:\n",
      "What's the kindness' be much:\n",
      "A visagens\n",
      "\n",
      "\n",
      "Had bent and pursuities with thy subject andeed.\n",
      "\n",
      "First Lewis as hand no man eam\n",
      "Methinks:\n",
      "Fie, he \n",
      "\n",
      "POMPrdisday us.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Gladies, be when he had pleaserous are that sage!\n",
      "\n",
      "YORK:\n",
      "If did a-\n",
      "\n",
      "Here ducall'd twice that party in the sweetdering:\n",
      "And yet both'd to end.\n",
      "\n",
      "ELBOW:\n",
      "Your look, with th\n",
      "\n",
      "than this cut\n",
      "The ord wherefore gentle cloud. Lady York take,\n",
      "High fag the prithee with uffer house;\n",
      "\n",
      "YORK:\n",
      "'Twas my lady water.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "There is open my trewpetion sorrow's had ears other's r\n",
      "\n",
      "I cannot disposition pretty of that he stand, for crums the heart!\n",
      "\n",
      "KING RICHARD III:\n",
      "Oh, forget's t\n",
      "\n",
      "Is.\n",
      "\n",
      "CORIOLANUS:\n",
      "My aim! my good lord!\n",
      "Give me a:\n",
      "Where's make provide clean deliver,\n",
      "My crown, or a\n",
      "\n",
      "Good lunge, and that right; but though\n",
      "The looks erefore thee, then made of all.\n",
      "\n",
      "YORK:\n",
      "If Lord Lanc\n",
      "\n",
      "Such is but My newded!\n",
      "\n",
      "BRUTUS:\n",
      "Then, my lord, sweet is labours,\n",
      "Somer ignoran, ow dark on goodly fa\n",
      "\n",
      "KING HENRY VI:\n",
      "How are here ere daughter'd, dispersed great the earm.\n",
      "\n",
      "MOPSA:\n",
      "It fit with thy soul n\n",
      "\n",
      "Watchmin: like it not? I would not to it.\n",
      "And my soul, and\n",
      "mine into your at once.\n",
      "\n",
      "PENE EDWARD:\n",
      "I k\n",
      "CPU times: user 1.77 s, sys: 9.84 ms, total: 1.78 s\n",
      "Wall time: 173 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
