{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=4, n_heads=8, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=10000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]].view(*shape)\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            hidden_dim: int,\n",
    "            multiple_of: int,\n",
    "            ffn_dim_multiplier: Optional[float],\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim, requires_grad=False)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k, requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)        \n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     dropout_p = dropout_p, \n",
    "                                                     is_causal = is_causal)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        h_heads = [head(x, start_pos) for head in self.heads]\n",
    "        h = torch.cat(h_heads, dim=-1)\n",
    "        h = self.linear(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.attention_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_swiglu = FFN_SwiGLU(config.dim, config.dim * 4, config.multiple_of, config.ffn_dim_multiplier)\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = x + self.attention(self.attention_norm(x), start_pos)\n",
    "        out = x + self.ffn_swiglu(self.ffn_norm(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", Llama2Block(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        h = self.embeddings(idx)\n",
    "\n",
    "        for block in self.llama_blocks:\n",
    "            h = block(h, start_pos)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama2):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama2, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 2902400\n",
      "Epoch 0 | val loss 3.695 | Time 1.119 | ETA in seconds 111.903\n",
      "Epoch 100 | val loss 2.135 | Time 46.217 | ETA in seconds 4575.514\n",
      "Epoch 200 | val loss 1.868 | Time 50.304 | ETA in seconds 4929.834\n",
      "Epoch 300 | val loss 1.755 | Time 50.696 | ETA in seconds 4917.508\n",
      "Epoch 400 | val loss 1.705 | Time 50.300 | ETA in seconds 4828.776\n",
      "Epoch 500 | val loss 1.651 | Time 50.494 | ETA in seconds 4796.916\n",
      "Epoch 600 | val loss 1.580 | Time 49.719 | ETA in seconds 4673.563\n",
      "Epoch 700 | val loss 1.589 | Time 50.105 | ETA in seconds 4659.754\n",
      "Epoch 800 | val loss 1.555 | Time 49.644 | ETA in seconds 4567.286\n",
      "Epoch 900 | val loss 1.576 | Time 49.620 | ETA in seconds 4515.433\n",
      "Epoch 1000 | val loss 1.542 | Time 50.282 | ETA in seconds 4525.358\n",
      "Epoch 1100 | val loss 1.527 | Time 49.825 | ETA in seconds 4434.388\n",
      "Epoch 1200 | val loss 1.538 | Time 50.874 | ETA in seconds 4476.875\n",
      "Epoch 1300 | val loss 1.523 | Time 50.090 | ETA in seconds 4357.863\n",
      "Epoch 1400 | val loss 1.467 | Time 49.892 | ETA in seconds 4290.671\n",
      "Epoch 1500 | val loss 1.471 | Time 49.706 | ETA in seconds 4224.993\n",
      "Epoch 1600 | val loss 1.490 | Time 49.967 | ETA in seconds 4197.193\n",
      "Epoch 1700 | val loss 1.488 | Time 52.749 | ETA in seconds 4378.193\n",
      "Epoch 1800 | val loss 1.470 | Time 52.773 | ETA in seconds 4327.395\n",
      "Epoch 1900 | val loss 1.477 | Time 49.137 | ETA in seconds 3980.085\n",
      "Epoch 2000 | val loss 1.485 | Time 49.526 | ETA in seconds 3962.080\n",
      "Epoch 2100 | val loss 1.489 | Time 49.604 | ETA in seconds 3918.740\n",
      "Epoch 2200 | val loss 1.509 | Time 49.150 | ETA in seconds 3833.680\n",
      "Epoch 2300 | val loss 1.485 | Time 49.454 | ETA in seconds 3807.971\n",
      "Epoch 2400 | val loss 1.508 | Time 51.085 | ETA in seconds 3882.493\n",
      "Epoch 2500 | val loss 1.483 | Time 50.847 | ETA in seconds 3813.513\n",
      "Epoch 2600 | val loss 1.485 | Time 52.355 | ETA in seconds 3874.306\n",
      "Epoch 2700 | val loss 1.470 | Time 53.318 | ETA in seconds 3892.202\n",
      "Epoch 2800 | val loss 1.482 | Time 50.653 | ETA in seconds 3647.037\n",
      "Epoch 2900 | val loss 1.482 | Time 52.638 | ETA in seconds 3737.287\n",
      "Epoch 3000 | val loss 1.481 | Time 50.059 | ETA in seconds 3504.157\n",
      "Epoch 3100 | val loss 1.446 | Time 47.244 | ETA in seconds 3259.847\n",
      "Epoch 3200 | val loss 1.473 | Time 45.563 | ETA in seconds 3098.306\n",
      "Epoch 3300 | val loss 1.495 | Time 54.525 | ETA in seconds 3653.202\n",
      "Epoch 3400 | val loss 1.465 | Time 49.701 | ETA in seconds 3280.236\n",
      "Epoch 3500 | val loss 1.472 | Time 58.362 | ETA in seconds 3793.559\n",
      "Epoch 3600 | val loss 1.462 | Time 59.320 | ETA in seconds 3796.490\n",
      "Epoch 3700 | val loss 1.461 | Time 58.758 | ETA in seconds 3701.776\n",
      "Epoch 3800 | val loss 1.464 | Time 55.824 | ETA in seconds 3461.092\n",
      "Epoch 3900 | val loss 1.452 | Time 57.222 | ETA in seconds 3490.534\n",
      "Epoch 4000 | val loss 1.499 | Time 54.765 | ETA in seconds 3285.882\n",
      "Epoch 4100 | val loss 1.482 | Time 52.067 | ETA in seconds 3071.955\n",
      "Epoch 4200 | val loss 1.455 | Time 51.964 | ETA in seconds 3013.901\n",
      "Epoch 4300 | val loss 1.458 | Time 55.665 | ETA in seconds 3172.887\n",
      "Epoch 4400 | val loss 1.456 | Time 54.361 | ETA in seconds 3044.212\n",
      "Epoch 4500 | val loss 1.464 | Time 58.183 | ETA in seconds 3200.091\n",
      "Epoch 4600 | val loss 1.497 | Time 53.687 | ETA in seconds 2899.111\n",
      "Epoch 4700 | val loss 1.487 | Time 55.514 | ETA in seconds 2942.233\n",
      "Epoch 4800 | val loss 1.442 | Time 50.786 | ETA in seconds 2640.862\n",
      "Epoch 4900 | val loss 1.477 | Time 47.884 | ETA in seconds 2442.083\n",
      "Epoch 5000 | val loss 1.475 | Time 49.164 | ETA in seconds 2458.179\n",
      "Epoch 5100 | val loss 1.491 | Time 47.223 | ETA in seconds 2313.950\n",
      "Epoch 5200 | val loss 1.499 | Time 47.299 | ETA in seconds 2270.371\n",
      "Epoch 5300 | val loss 1.466 | Time 48.351 | ETA in seconds 2272.487\n",
      "Epoch 5400 | val loss 1.519 | Time 48.061 | ETA in seconds 2210.802\n",
      "Epoch 5500 | val loss 1.483 | Time 46.953 | ETA in seconds 2112.904\n",
      "Epoch 5600 | val loss 1.486 | Time 47.899 | ETA in seconds 2107.548\n",
      "Epoch 5700 | val loss 1.461 | Time 47.546 | ETA in seconds 2044.480\n",
      "Epoch 5800 | val loss 1.466 | Time 46.936 | ETA in seconds 1971.304\n",
      "Epoch 5900 | val loss 1.489 | Time 48.000 | ETA in seconds 1967.996\n",
      "Epoch 6000 | val loss 1.497 | Time 48.611 | ETA in seconds 1944.458\n",
      "Epoch 6100 | val loss 1.479 | Time 47.752 | ETA in seconds 1862.347\n",
      "Epoch 6200 | val loss 1.491 | Time 47.370 | ETA in seconds 1800.043\n",
      "Epoch 6300 | val loss 1.507 | Time 48.343 | ETA in seconds 1788.696\n",
      "Epoch 6400 | val loss 1.489 | Time 48.115 | ETA in seconds 1732.153\n",
      "Epoch 6500 | val loss 1.505 | Time 47.257 | ETA in seconds 1653.994\n",
      "Epoch 6600 | val loss 1.472 | Time 47.317 | ETA in seconds 1608.794\n",
      "Epoch 6700 | val loss 1.472 | Time 48.401 | ETA in seconds 1597.218\n",
      "Epoch 6800 | val loss 1.511 | Time 47.566 | ETA in seconds 1522.109\n",
      "Epoch 6900 | val loss 1.512 | Time 47.072 | ETA in seconds 1459.229\n",
      "Epoch 7000 | val loss 1.518 | Time 48.067 | ETA in seconds 1442.021\n",
      "Epoch 7100 | val loss 1.512 | Time 48.181 | ETA in seconds 1397.252\n",
      "Epoch 7200 | val loss 1.484 | Time 47.244 | ETA in seconds 1322.823\n",
      "Epoch 7300 | val loss 1.480 | Time 47.306 | ETA in seconds 1277.254\n",
      "Epoch 7400 | val loss 1.496 | Time 48.261 | ETA in seconds 1254.799\n",
      "Epoch 7500 | val loss 1.484 | Time 47.568 | ETA in seconds 1189.197\n",
      "Epoch 7600 | val loss 1.527 | Time 47.206 | ETA in seconds 1132.946\n",
      "Epoch 7700 | val loss 1.514 | Time 47.751 | ETA in seconds 1098.266\n",
      "Epoch 7800 | val loss 1.521 | Time 48.645 | ETA in seconds 1070.179\n",
      "Epoch 7900 | val loss 1.511 | Time 48.407 | ETA in seconds 1016.548\n",
      "Epoch 8000 | val loss 1.505 | Time 47.583 | ETA in seconds 951.654\n",
      "Epoch 8100 | val loss 1.512 | Time 48.441 | ETA in seconds 920.376\n",
      "Epoch 8200 | val loss 1.535 | Time 47.645 | ETA in seconds 857.607\n",
      "Epoch 8300 | val loss 1.521 | Time 47.481 | ETA in seconds 807.177\n",
      "Epoch 8400 | val loss 1.514 | Time 48.289 | ETA in seconds 772.621\n",
      "Epoch 8500 | val loss 1.518 | Time 48.207 | ETA in seconds 723.107\n",
      "Epoch 8600 | val loss 1.546 | Time 47.578 | ETA in seconds 666.093\n",
      "Epoch 8700 | val loss 1.554 | Time 47.797 | ETA in seconds 621.358\n",
      "Epoch 8800 | val loss 1.526 | Time 48.613 | ETA in seconds 583.351\n",
      "Epoch 8900 | val loss 1.517 | Time 47.661 | ETA in seconds 524.275\n",
      "Epoch 9000 | val loss 1.525 | Time 47.563 | ETA in seconds 475.635\n",
      "Epoch 9100 | val loss 1.562 | Time 48.323 | ETA in seconds 434.903\n",
      "Epoch 9200 | val loss 1.576 | Time 48.422 | ETA in seconds 387.377\n",
      "Epoch 9300 | val loss 1.548 | Time 47.591 | ETA in seconds 333.138\n",
      "Epoch 9400 | val loss 1.529 | Time 47.781 | ETA in seconds 286.688\n",
      "Epoch 9500 | val loss 1.505 | Time 48.846 | ETA in seconds 244.232\n",
      "Epoch 9600 | val loss 1.549 | Time 48.246 | ETA in seconds 192.985\n",
      "Epoch 9700 | val loss 1.579 | Time 47.432 | ETA in seconds 142.296\n",
      "Epoch 9800 | val loss 1.574 | Time 47.951 | ETA in seconds 95.902\n",
      "Epoch 9900 | val loss 1.555 | Time 48.493 | ETA in seconds 48.493\n",
      "validation loss:  1.5551607251167296\n",
      "CPU times: user 14h 40min 26s, sys: 3min 24s, total: 14h 43min 50s\n",
      "Wall time: 1h 28min 23s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama2(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama2_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama2, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 2902400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama2(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I have enter'd a morning's father:\n",
      "But if you veRoment drunk.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "If my uncle Henry is \n",
      "\n",
      "But all i' the world.\n",
      "O, that thy earnest together! All thou never long\n",
      "Of pity may break off down: \n",
      "\n",
      "The city is drawn so.\n",
      "\n",
      "CAMILLO:\n",
      "It is alone; most proclaim your inforces;\n",
      "So it is the rest of that \n",
      "\n",
      "\n",
      "If I was lurtier. Still, affective?\n",
      "\n",
      "First Citizen:\n",
      "We pray not; so worth some pity of my daughter,\n",
      "\n",
      "MENENIUS:\n",
      "I prithee, good Camillo.\n",
      "\n",
      "AL Overd:\n",
      "Here's in the other spices.\n",
      "\n",
      "Of no\n",
      "Attend whose though\n",
      "\n",
      "Jesu, my gentle heart's blood, frist thee\n",
      "Unto thy sins; thy follow'd to all?\n",
      "\n",
      "PEMINIUS:\n",
      "For this th\n",
      "\n",
      "What are the gentle day of more joys\n",
      "To sue than doubteth myself: how upon his pity.\n",
      "Fairest Lord Au\n",
      "\n",
      "That power in my counsel, the dearest man\n",
      "As ornament was to bid mine arm,\n",
      "I would show. Send me to \n",
      "\n",
      "I might remember, the best endeavours of the king.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "I will be so, no pity in pinnou\n",
      "\n",
      "JULIET:\n",
      "Lost of fire, I should\n",
      "I drop our informer hunder the Volscian,\n",
      "'Tis peril too; when 'tis it\n",
      "\n",
      "A' the war so and larkness of a tabour eye,\n",
      "And had rather heart than the fairest Paris;\n",
      "Thy busines\n",
      "\n",
      "A man. For this?\n",
      "You'll make him to aem the child-cock,\n",
      "But when you see, but you pardon him all\n",
      "The\n",
      "\n",
      "AUFIDIUS:\n",
      "Good morrow, what will attend the market, for alliand\n",
      "With Venuments, fall on their odds.\n",
      "\n",
      "\n",
      "O, hear me from the minds of that when was both\n",
      "not great desiring.\n",
      "\n",
      "CAMILLO:\n",
      "If I will hour, if you\n",
      "\n",
      "\n",
      "\n",
      "NOJck, Embrace be the fires of her two more\n",
      "But triumphant in this led tired did\n",
      "But your hell-bed\n",
      "\n",
      "\n",
      "AUFIDIUS:\n",
      "O boy, 'tis hower!\n",
      "\n",
      "CORIOLANUS:\n",
      "Bear thee she was about the world,\n",
      "With no good marriage \n",
      "\n",
      "While we will both use from the suit: he hath\n",
      "But mine thou didst continue it.\n",
      "I have proceeded in t\n",
      "\n",
      "\n",
      "Your graces must else.\n",
      "\n",
      "ISABELLA:\n",
      "Doth a buse that stand for you: that was\n",
      "I am a gagged cut off us\n",
      "\n",
      "The beauty of the judge to die for their wealth,\n",
      "But I dare so rink in York and heart\n",
      "That what is w\n",
      "\n",
      "Pray you? If thine refuse this night?\n",
      "Say grief, that, to aid your heart to be,\n",
      "And that the people \n",
      "\n",
      "Than thou shalt suspect my birth as done!\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Depress my soul begin so shallow it.\n",
      "\n",
      "DUK\n",
      "\n",
      "think,\n",
      "Since, strike, clept to any thing I crieved in thy just.\n",
      "Thou wast set down, and thou art sha\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "His grace to the king. Welcome, prosper, pray\n",
      "You that your house with your right-on\n",
      "\n",
      "\n",
      "First Senator:\n",
      "Why, counsel, now, hath Margaret had been,\n",
      "The burthens of thy truth: so five I thou\n",
      "\n",
      "\n",
      "Pardon these o's my father's lady's love\n",
      "Down'd how the greater powerful friend,\n",
      "One that when it w\n",
      "\n",
      "O that the wealth goest heavy vessels\n",
      "To help this doom by aim. Bathh, less abody,\n",
      "The very lamb abo\n",
      "\n",
      "Of your queen and coward; I must not take\n",
      "From vice about\n",
      "Is goted fortune of my guilting hand.\n",
      "Our \n",
      "\n",
      "MERCUTIO:\n",
      "I emperians with a wisher gently vengeance:\n",
      "When it was still or I will not for your hands\n",
      "\n",
      "to the each of an officer.\n",
      "\n",
      "MENENIUS:\n",
      "Stand upon my weary. Mark your wife's awar,\n",
      "That ever was was \n",
      "\n",
      "Of the world, and more than will to thee;\n",
      "But by the sea, I will not keep.\n",
      "\n",
      "First Murderer:\n",
      "Reside o\n",
      "\n",
      "Yes are the other function in arms,\n",
      "Being warliken of my sight! King of my weapon,\n",
      "Add the rebel sun\n",
      "\n",
      "HERMIONE:\n",
      "Never, not so, nor dream,\n",
      "That beaumons them well\n",
      "That holy man in the war, the tops,\n",
      "If t\n",
      "CPU times: user 9.77 s, sys: 9.94 ms, total: 9.78 s\n",
      "Wall time: 974 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
