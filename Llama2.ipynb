{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=5000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]].view(*shape)\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            hidden_dim: int,\n",
    "            multiple_of: int,\n",
    "            ffn_dim_multiplier: Optional[float],\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim, requires_grad=False)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k, requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)        \n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     dropout_p = dropout_p, \n",
    "                                                     is_causal = is_causal)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        h_heads = [head(x, start_pos) for head in self.heads]\n",
    "        h = torch.cat(h_heads, dim=-1)\n",
    "        h = self.linear(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.attention_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_swiglu = FFN_SwiGLU(config.dim, config.dim * 4, config.multiple_of, config.ffn_dim_multiplier)\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = x + self.attention(self.attention_norm(x), start_pos)\n",
    "        out = x + self.ffn_swiglu(self.ffn_norm(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", Llama2Block(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        h = self.embeddings(idx)\n",
    "\n",
    "        for block in self.llama_blocks:\n",
    "            h = block(h, start_pos)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama2):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama2, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 935296\n",
      "Epoch 0 | val loss 3.979 | Time 0.213 | ETA in seconds 10.639\n",
      "Epoch 100 | val loss 2.214 | Time 10.886 | ETA in seconds 533.396\n",
      "Epoch 200 | val loss 1.961 | Time 10.726 | ETA in seconds 514.869\n",
      "Epoch 300 | val loss 1.852 | Time 12.341 | ETA in seconds 580.014\n",
      "Epoch 400 | val loss 1.788 | Time 13.491 | ETA in seconds 620.567\n",
      "Epoch 500 | val loss 1.727 | Time 14.014 | ETA in seconds 630.648\n",
      "Epoch 600 | val loss 1.715 | Time 13.986 | ETA in seconds 615.381\n",
      "Epoch 700 | val loss 1.689 | Time 13.879 | ETA in seconds 596.794\n",
      "Epoch 800 | val loss 1.658 | Time 13.803 | ETA in seconds 579.728\n",
      "Epoch 900 | val loss 1.631 | Time 13.913 | ETA in seconds 570.427\n",
      "Epoch 1000 | val loss 1.632 | Time 13.880 | ETA in seconds 555.219\n",
      "Epoch 1100 | val loss 1.645 | Time 14.286 | ETA in seconds 557.164\n",
      "Epoch 1200 | val loss 1.626 | Time 13.944 | ETA in seconds 529.869\n",
      "Epoch 1300 | val loss 1.596 | Time 14.219 | ETA in seconds 526.115\n",
      "Epoch 1400 | val loss 1.610 | Time 14.276 | ETA in seconds 513.925\n",
      "Epoch 1500 | val loss 1.597 | Time 14.270 | ETA in seconds 499.465\n",
      "Epoch 1600 | val loss 1.579 | Time 14.090 | ETA in seconds 479.043\n",
      "Epoch 1700 | val loss 1.553 | Time 13.965 | ETA in seconds 460.846\n",
      "Epoch 1800 | val loss 1.577 | Time 13.914 | ETA in seconds 445.234\n",
      "Epoch 1900 | val loss 1.552 | Time 13.832 | ETA in seconds 428.807\n",
      "Epoch 2000 | val loss 1.572 | Time 13.855 | ETA in seconds 415.643\n",
      "Epoch 2100 | val loss 1.540 | Time 13.933 | ETA in seconds 404.054\n",
      "Epoch 2200 | val loss 1.554 | Time 13.934 | ETA in seconds 390.165\n",
      "Epoch 2300 | val loss 1.553 | Time 14.073 | ETA in seconds 379.971\n",
      "Epoch 2400 | val loss 1.528 | Time 14.012 | ETA in seconds 364.317\n",
      "Epoch 2500 | val loss 1.535 | Time 14.245 | ETA in seconds 356.126\n",
      "Epoch 2600 | val loss 1.525 | Time 14.355 | ETA in seconds 344.527\n",
      "Epoch 2700 | val loss 1.539 | Time 14.136 | ETA in seconds 325.118\n",
      "Epoch 2800 | val loss 1.547 | Time 14.070 | ETA in seconds 309.535\n",
      "Epoch 2900 | val loss 1.551 | Time 13.959 | ETA in seconds 293.133\n",
      "Epoch 3000 | val loss 1.515 | Time 13.933 | ETA in seconds 278.653\n",
      "Epoch 3100 | val loss 1.552 | Time 13.850 | ETA in seconds 263.145\n",
      "Epoch 3200 | val loss 1.520 | Time 13.878 | ETA in seconds 249.800\n",
      "Epoch 3300 | val loss 1.536 | Time 13.891 | ETA in seconds 236.155\n",
      "Epoch 3400 | val loss 1.552 | Time 13.953 | ETA in seconds 223.254\n",
      "Epoch 3500 | val loss 1.531 | Time 13.890 | ETA in seconds 208.354\n",
      "Epoch 3600 | val loss 1.539 | Time 13.986 | ETA in seconds 195.806\n",
      "Epoch 3700 | val loss 1.523 | Time 14.188 | ETA in seconds 184.442\n",
      "Epoch 3800 | val loss 1.526 | Time 14.106 | ETA in seconds 169.271\n",
      "Epoch 3900 | val loss 1.525 | Time 14.191 | ETA in seconds 156.096\n",
      "Epoch 4000 | val loss 1.529 | Time 14.306 | ETA in seconds 143.057\n",
      "Epoch 4100 | val loss 1.528 | Time 14.360 | ETA in seconds 129.241\n",
      "Epoch 4200 | val loss 1.489 | Time 14.318 | ETA in seconds 114.541\n",
      "Epoch 4300 | val loss 1.506 | Time 14.104 | ETA in seconds 98.725\n",
      "Epoch 4400 | val loss 1.517 | Time 13.877 | ETA in seconds 83.261\n",
      "Epoch 4500 | val loss 1.496 | Time 13.900 | ETA in seconds 69.499\n",
      "Epoch 4600 | val loss 1.536 | Time 13.861 | ETA in seconds 55.444\n",
      "Epoch 4700 | val loss 1.490 | Time 14.299 | ETA in seconds 42.897\n",
      "Epoch 4800 | val loss 1.518 | Time 14.486 | ETA in seconds 28.973\n",
      "Epoch 4900 | val loss 1.524 | Time 13.990 | ETA in seconds 13.990\n",
      "validation loss:  1.5235283732414246\n",
      "CPU times: user 2h 2min 53s, sys: 14.5 s, total: 2h 3min 8s\n",
      "Wall time: 12min 19s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama2(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama2_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama2, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 935296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama2(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The fatal.\n",
      "\n",
      "MARCIUS:\n",
      "He's treverenous hand more my flattering is\n",
      "That have me deadly that your compa\n",
      "\n",
      "And both truth: if he\n",
      "Sound of my age; and that commanded myself:\n",
      "There's; the Volscianst has have t\n",
      "\n",
      "Is the surderer: sin on this I we call like,\n",
      "Will to me do look bruised bear a tout.\n",
      "Come, shalt wit\n",
      "\n",
      "to gambour eyes, or men, whose desire good,\n",
      "By eving to wonder; he made all sir.\n",
      "\n",
      "KING RICHARD II:\n",
      "T\n",
      "\n",
      "She honour'd arms of our procla?\n",
      "\n",
      "Second Murderer:\n",
      "I am leave I am a tender wrong death man!\n",
      "\n",
      "HASTIN\n",
      "\n",
      "Have life break friends Shall be a king.\n",
      "Madam, my treason: tuton the fear!\n",
      "\n",
      "CAMILLO:\n",
      "Happine so?\n",
      "\n",
      "M\n",
      "\n",
      "it will fither durst, for supposers in heaven,\n",
      "To speech he has pity; but I speak with his\n",
      "To still.\n",
      "\n",
      "I was your half; be become royal son:\n",
      "All marraise is too my youth\n",
      "By mean! Kink in Pentence to Buck\n",
      "\n",
      "First Gentleman:\n",
      "My lord, of shame much joints to't, dost thou me welcome,\n",
      "To much out on! and Lay t\n",
      "\n",
      "There have by sister slaughters.\n",
      "\n",
      "LEONTES:\n",
      "I will have done.\n",
      "\n",
      "ANGELO:\n",
      "Master Marry. She had you\n",
      "'Tis\n",
      "\n",
      "KING RICHARD III:\n",
      "I prithee, think his minive and sudden,\n",
      "Thou stay 'shestelling, praysix on French,\n",
      "\n",
      "If all think orsaked or fortunes in sweet death,\n",
      "Makes the secrets and trown, and not remember,\n",
      "Is a\n",
      "\n",
      "Iume speak beseech your blood, that makes yourselves.\n",
      "\n",
      "PETER:\n",
      "By Thursday be guiltion men, write-wor\n",
      "\n",
      "RICHARD:\n",
      "My tongue our knee, farewell and were\n",
      "But ye at unre than confess womb, for you:\n",
      "Thou seems\n",
      "\n",
      "arm'd and stole? Camillo, though those two my home,\n",
      "whereon-virging; and what answer thus;\n",
      "For I, an\n",
      "\n",
      "That hath limemon his horse!-you'll heaven evouchstanified\n",
      "to know you do secure made. What's time,\n",
      "\n",
      "\n",
      "Of thy liptier England's royal good his;\n",
      "From their life biregonts accuse.\n",
      "\n",
      "JULIET:\n",
      "Come, sir, if yo\n",
      "\n",
      "CORIOLANUS:\n",
      "I have speaked court\n",
      "that time hath done? natural now they little youth\n",
      "Is be retired th\n",
      "\n",
      "Seem, here he behind, dispboth here?\n",
      "\n",
      "LEONTES:\n",
      "! York was little young tent to my joy'd\n",
      "How arbour n\n",
      "\n",
      "TYBALT:\n",
      "Pray to the heaven, home, lay Bround,\n",
      "Show loves trunk and to thy plebeians.\n",
      "\n",
      "BUSHY:\n",
      "Good pa\n",
      "\n",
      "And make off.\n",
      "\n",
      "NORTHUMBERLAND:\n",
      "No, sir, see come to sighs and affords\n",
      "And sweeted vinege, and were f\n",
      "\n",
      "But, nor thou art the present to the silent blood:\n",
      "they say.\n",
      "\n",
      "Shepherd:\n",
      "Give me rites, my letter!\n",
      "Wh\n",
      "\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "This Lord! O, in hath farewell:\n",
      "I'll corrut may soldiers, art go, to lead,\n",
      "Dight\n",
      "\n",
      "MENENIUS:\n",
      "And my heart: if I give thee, and sorrow,\n",
      "No proud for a grave will pamp, and it come,\n",
      "And\n",
      "\n",
      "Tales Titus More't thou wrath to lost press'd bless,\n",
      "Of blooding father Grey shall met;\n",
      "Have fellowl\n",
      "\n",
      "Thou shalt prick be point.\n",
      "\n",
      "CAMILLO:\n",
      "Marry, as I have a secured the gaze us\n",
      "Of what your affairs, fa\n",
      "\n",
      "\n",
      "\n",
      "thought to him thunder. Have made usurpicturely up a Edward!\n",
      "\n",
      "JOHN OF GAUNT:\n",
      "What then. York!\n",
      "Let \n",
      "\n",
      "It with us. Will your usurper highnessful dancing withdriUp.\n",
      "\n",
      "THASS GONTAGUE:\n",
      "I say you proud some g\n",
      "\n",
      "That seem to his love entreat to friends;\n",
      "As 'tis a for a heard and a palas there treachmon be with \n",
      "\n",
      "Is cannot are repised, I'll seem in my prince been,\n",
      "Of Warwick; these being from your wring.\n",
      "\n",
      "JOHN O\n",
      "\n",
      "EDWARD:\n",
      "And warns that Clarence sinherit his\n",
      "That clouds be my vohile: lords, I go's news?\n",
      "Think the\n",
      "\n",
      "Lourio? a gage, and blad hath here,--\n",
      "And I pray the honours of the issue,\n",
      "But says the signaled swo\n",
      "CPU times: user 2.38 s, sys: 0 ns, total: 2.38 s\n",
      "Wall time: 234 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
