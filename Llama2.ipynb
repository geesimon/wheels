{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=4, n_heads=8, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=10000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 4\n",
    "    n_heads: int = 8\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]].view(*shape)\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            hidden_dim: int,\n",
    "            multiple_of: int,\n",
    "            ffn_dim_multiplier: Optional[float],\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)        \n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     dropout_p = dropout_p, \n",
    "                                                     is_causal = is_causal)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        heads = [h(x, start_pos) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.attention(x, start_pos)\n",
    "\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.feedforward(x)\n",
    "        return x\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "            nn.Linear(config.dim, config.vocab_size),\n",
    "        )\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        x = self.embeddings(idx)\n",
    "        for block in self.llama_blocks:\n",
    "            x = block(x, start_pos)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 2362561\n",
      "Epoch 0 | val loss 4.134 | Time 0.612 | ETA in seconds 61.223\n",
      "Epoch 100 | val loss 2.196 | Time 41.312 | ETA in seconds 4089.919\n",
      "Epoch 200 | val loss 1.919 | Time 51.595 | ETA in seconds 5056.324\n",
      "Epoch 300 | val loss 1.800 | Time 48.191 | ETA in seconds 4674.541\n",
      "Epoch 400 | val loss 1.732 | Time 49.415 | ETA in seconds 4743.859\n",
      "Epoch 500 | val loss 1.689 | Time 53.831 | ETA in seconds 5113.905\n",
      "Epoch 600 | val loss 1.662 | Time 51.201 | ETA in seconds 4812.848\n",
      "Epoch 700 | val loss 1.645 | Time 52.451 | ETA in seconds 4877.898\n",
      "Epoch 800 | val loss 1.639 | Time 54.185 | ETA in seconds 4985.045\n",
      "Epoch 900 | val loss 1.660 | Time 52.060 | ETA in seconds 4737.473\n",
      "Epoch 1000 | val loss 1.644 | Time 51.913 | ETA in seconds 4672.148\n",
      "Epoch 1100 | val loss 1.631 | Time 51.707 | ETA in seconds 4601.927\n",
      "Epoch 1200 | val loss 1.623 | Time 53.856 | ETA in seconds 4739.368\n",
      "Epoch 1300 | val loss 1.595 | Time 53.569 | ETA in seconds 4660.529\n",
      "Epoch 1400 | val loss 1.632 | Time 51.515 | ETA in seconds 4430.260\n",
      "Epoch 1500 | val loss 1.613 | Time 49.594 | ETA in seconds 4215.513\n",
      "Epoch 1600 | val loss 1.560 | Time 48.724 | ETA in seconds 4092.809\n",
      "Epoch 1700 | val loss 1.592 | Time 48.829 | ETA in seconds 4052.829\n",
      "Epoch 1800 | val loss 1.569 | Time 49.202 | ETA in seconds 4034.548\n",
      "Epoch 1900 | val loss 1.556 | Time 49.208 | ETA in seconds 3985.827\n",
      "Epoch 2000 | val loss 1.544 | Time 48.774 | ETA in seconds 3901.928\n",
      "Epoch 2100 | val loss 1.554 | Time 49.176 | ETA in seconds 3884.867\n",
      "Epoch 2200 | val loss 1.521 | Time 48.801 | ETA in seconds 3806.451\n",
      "Epoch 2300 | val loss 1.564 | Time 48.858 | ETA in seconds 3762.073\n",
      "Epoch 2400 | val loss 1.517 | Time 48.563 | ETA in seconds 3690.751\n",
      "Epoch 2500 | val loss 1.549 | Time 49.020 | ETA in seconds 3676.465\n",
      "Epoch 2600 | val loss 1.562 | Time 48.709 | ETA in seconds 3604.493\n",
      "Epoch 2700 | val loss 1.590 | Time 50.093 | ETA in seconds 3656.772\n",
      "Epoch 2800 | val loss 1.546 | Time 50.413 | ETA in seconds 3629.717\n",
      "Epoch 2900 | val loss 1.554 | Time 47.787 | ETA in seconds 3392.883\n",
      "Epoch 3000 | val loss 1.541 | Time 45.775 | ETA in seconds 3204.235\n",
      "Epoch 3100 | val loss 1.535 | Time 45.684 | ETA in seconds 3152.163\n",
      "Epoch 3200 | val loss 1.544 | Time 46.736 | ETA in seconds 3178.036\n",
      "Epoch 3300 | val loss 1.505 | Time 46.072 | ETA in seconds 3086.833\n",
      "Epoch 3400 | val loss 1.517 | Time 48.226 | ETA in seconds 3182.893\n",
      "Epoch 3500 | val loss 1.535 | Time 44.632 | ETA in seconds 2901.073\n",
      "Epoch 3600 | val loss 1.539 | Time 44.328 | ETA in seconds 2836.961\n",
      "Epoch 3700 | val loss 1.533 | Time 44.418 | ETA in seconds 2798.356\n",
      "Epoch 3800 | val loss 1.558 | Time 45.262 | ETA in seconds 2806.238\n",
      "Epoch 3900 | val loss 1.556 | Time 44.070 | ETA in seconds 2688.260\n",
      "Epoch 4000 | val loss 1.518 | Time 43.913 | ETA in seconds 2634.805\n",
      "Epoch 4100 | val loss 1.554 | Time 44.131 | ETA in seconds 2603.742\n",
      "Epoch 4200 | val loss 1.580 | Time 42.878 | ETA in seconds 2486.902\n",
      "Epoch 4300 | val loss 1.550 | Time 42.694 | ETA in seconds 2433.538\n",
      "Epoch 4400 | val loss 1.524 | Time 42.701 | ETA in seconds 2391.241\n",
      "Epoch 4500 | val loss 1.587 | Time 44.232 | ETA in seconds 2432.787\n",
      "Epoch 4600 | val loss 1.542 | Time 41.950 | ETA in seconds 2265.289\n",
      "Epoch 4700 | val loss 1.561 | Time 41.928 | ETA in seconds 2222.169\n",
      "Epoch 4800 | val loss 1.519 | Time 43.549 | ETA in seconds 2264.560\n",
      "Epoch 4900 | val loss 1.539 | Time 42.169 | ETA in seconds 2150.644\n",
      "Epoch 5000 | val loss 1.547 | Time 43.440 | ETA in seconds 2172.003\n",
      "Epoch 5100 | val loss 1.549 | Time 42.426 | ETA in seconds 2078.874\n",
      "Epoch 5200 | val loss 1.543 | Time 42.694 | ETA in seconds 2049.324\n",
      "Epoch 5300 | val loss 1.543 | Time 42.182 | ETA in seconds 1982.542\n",
      "Epoch 5400 | val loss 1.532 | Time 47.003 | ETA in seconds 2162.145\n",
      "Epoch 5500 | val loss 1.523 | Time 52.207 | ETA in seconds 2349.315\n",
      "Epoch 5600 | val loss 1.562 | Time 52.604 | ETA in seconds 2314.567\n",
      "Epoch 5700 | val loss 1.522 | Time 51.571 | ETA in seconds 2217.550\n",
      "Epoch 5800 | val loss 1.506 | Time 45.312 | ETA in seconds 1903.094\n",
      "Epoch 5900 | val loss 1.509 | Time 42.383 | ETA in seconds 1737.704\n",
      "Epoch 6000 | val loss 1.546 | Time 42.088 | ETA in seconds 1683.528\n",
      "Epoch 6100 | val loss 1.521 | Time 42.106 | ETA in seconds 1642.138\n",
      "Epoch 6200 | val loss 1.541 | Time 49.861 | ETA in seconds 1894.703\n",
      "Epoch 6300 | val loss 1.514 | Time 47.762 | ETA in seconds 1767.188\n",
      "Epoch 6400 | val loss 1.491 | Time 45.368 | ETA in seconds 1633.235\n",
      "Epoch 6500 | val loss 1.552 | Time 46.332 | ETA in seconds 1621.636\n",
      "Epoch 6600 | val loss 1.517 | Time 47.369 | ETA in seconds 1610.548\n",
      "Epoch 6700 | val loss 1.545 | Time 43.383 | ETA in seconds 1431.627\n",
      "Epoch 6800 | val loss 1.515 | Time 46.155 | ETA in seconds 1476.970\n",
      "Epoch 6900 | val loss 1.549 | Time 45.674 | ETA in seconds 1415.900\n",
      "Epoch 7000 | val loss 1.554 | Time 48.649 | ETA in seconds 1459.482\n",
      "Epoch 7100 | val loss 1.521 | Time 45.661 | ETA in seconds 1324.171\n",
      "Epoch 7200 | val loss 1.525 | Time 45.533 | ETA in seconds 1274.929\n",
      "Epoch 7300 | val loss 1.542 | Time 45.738 | ETA in seconds 1234.928\n",
      "Epoch 7400 | val loss 1.513 | Time 45.820 | ETA in seconds 1191.317\n",
      "Epoch 7500 | val loss 1.507 | Time 46.220 | ETA in seconds 1155.488\n",
      "Epoch 7600 | val loss 1.501 | Time 45.483 | ETA in seconds 1091.583\n",
      "Epoch 7700 | val loss 1.505 | Time 45.420 | ETA in seconds 1044.671\n",
      "Epoch 7800 | val loss 1.558 | Time 44.312 | ETA in seconds 974.868\n",
      "Epoch 7900 | val loss 1.533 | Time 48.954 | ETA in seconds 1028.033\n",
      "Epoch 8000 | val loss 1.490 | Time 48.533 | ETA in seconds 970.655\n",
      "Epoch 8100 | val loss 1.534 | Time 48.323 | ETA in seconds 918.136\n",
      "Epoch 8200 | val loss 1.505 | Time 47.491 | ETA in seconds 854.844\n",
      "Epoch 8300 | val loss 1.523 | Time 48.282 | ETA in seconds 820.788\n",
      "Epoch 8400 | val loss 1.541 | Time 46.541 | ETA in seconds 744.655\n",
      "Epoch 8500 | val loss 1.526 | Time 44.038 | ETA in seconds 660.573\n",
      "Epoch 8600 | val loss 1.517 | Time 46.703 | ETA in seconds 653.847\n",
      "Epoch 8700 | val loss 1.539 | Time 47.211 | ETA in seconds 613.744\n",
      "Epoch 8800 | val loss 1.529 | Time 46.898 | ETA in seconds 562.777\n",
      "Epoch 8900 | val loss 1.477 | Time 45.532 | ETA in seconds 500.853\n",
      "Epoch 9000 | val loss 1.534 | Time 43.988 | ETA in seconds 439.879\n",
      "Epoch 9100 | val loss 1.553 | Time 44.237 | ETA in seconds 398.134\n",
      "Epoch 9200 | val loss 1.514 | Time 45.763 | ETA in seconds 366.102\n",
      "Epoch 9300 | val loss 1.522 | Time 46.273 | ETA in seconds 323.911\n",
      "Epoch 9400 | val loss 1.507 | Time 44.922 | ETA in seconds 269.529\n",
      "Epoch 9500 | val loss 1.523 | Time 45.003 | ETA in seconds 225.016\n",
      "Epoch 9600 | val loss 1.524 | Time 47.304 | ETA in seconds 189.214\n",
      "Epoch 9700 | val loss 1.523 | Time 46.314 | ETA in seconds 138.943\n",
      "Epoch 9800 | val loss 1.523 | Time 50.573 | ETA in seconds 101.146\n",
      "Epoch 9900 | val loss 1.539 | Time 49.293 | ETA in seconds 49.293\n",
      "validation loss:  1.5388972759246826\n",
      "CPU times: user 13h 50min 25s, sys: 3min 36s, total: 13h 54min 2s\n",
      "Wall time: 1h 23min 24s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABR6UlEQVR4nO3dd3xUVd4/8M+dmp6QnpCEJIQaQCCAIiIoCgqi2LuwrvuIP7Asj2XRbbq6uKvPim3BjorKrgKK0pGqgnQILbSEhBRCSG/T7v39cWYmGdKTydyE+bxfr3nB3Lkzc+YScj9zzvecKymKooCIiIhIJRq1G0BERETejWGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSlU7tBrSGLMvIy8tDYGAgJElSuzlERETUCoqioKKiArGxsdBomu7/6BZhJC8vD/Hx8Wo3g4iIiNohJycHcXFxTT7eLcJIYGAgAPFhgoKCVG4NERERtUZ5eTni4+Od5/GmdIsw4hiaCQoKYhghIiLqZloqsWABKxEREamKYYSIiIhUxTBCREREquoWNSNERESdQVEUWK1W2Gw2tZvSLWm1Wuh0ug4vu8EwQkREXslsNiM/Px/V1dVqN6Vb8/PzQ0xMDAwGQ7tfg2GEiIi8jizLyMzMhFarRWxsLAwGAxfVbCNFUWA2m3H+/HlkZmaiT58+zS5s1hyGESIi8jpmsxmyLCM+Ph5+fn5qN6fb8vX1hV6vx5kzZ2A2m+Hj49Ou12EBKxERea32fpOnOu44hvxXICIiIlUxjBAREZGqGEaIiIi8VGJiIubPn692M1jASkRE1J2MHz8eQ4cOdUuI2LVrF/z9/TveqA7y6jByct37sOXug+9ltyJh+ES1m0NERNRhiqLAZrNBp2v5FB8REeGBFrXMq4dpytJXo9+ZL5F37Fe1m0JERCpTFAXVZqsqN0VRWtXGGTNmYMuWLXjzzTchSRIkScKiRYsgSRLWrl2LESNGwGg0Ytu2bTh16hRuueUWREVFISAgACNHjsSGDRtcXu/iYRpJkvDhhx/i1ltvhZ+fH/r06YMVK1a48zA3yqt7RmSN/ePbLOo2hIiIVFdjsWHgn9eq8t5HXpoEP0PLp+Q333wTx48fx6BBg/DSSy8BAA4fPgwAePbZZ/H6668jOTkZISEhOHv2LCZPnoyXX34ZPj4++PTTTzF16lRkZGQgISGhyfd48cUX8c9//hOvvfYa3n77bdx///04c+YMQkND3fNhG+HVPSOK5AgjVnUbQkRE1ArBwcEwGAzw8/NDdHQ0oqOjodVqAQAvvfQSrr/+evTu3RthYWG47LLL8Oijj2Lw4MHo06cPXn75ZSQnJ7fY0zFjxgzce++9SElJwd///ndUVVVh586dnfq5vLpnRNHa19G3mdVtCBERqc5Xr8WRlyap9t4dNWLECJf7VVVVePHFF/HDDz8gLy8PVqsVNTU1yM7ObvZ1hgwZ4vy7v78/AgMDUVhY2OH2Nce7w4hjmEbmMA0RkbeTJKlVQyVd1cWzYp555hmsXbsWr7/+OlJSUuDr64s77rgDZnPzX8D1er3LfUmSIMuy29tbX/c96m6gaOwHXOYwDRERdQ8GgwE2m63F/bZt24YZM2bg1ltvBQBUVlYiKyurk1vXPt5dM2IPIxKHaYiIqJtITEzEr7/+iqysLBQVFTXZa5GSkoJly5Zh//79OHDgAO67775O7+FoL68OI5LWHkbYM0JERN3E008/Da1Wi4EDByIiIqLJGpA33ngDPXr0wJVXXompU6di0qRJGD58uIdb2zpePUwD5zANa0aIiKh76Nu3L7Zv3+6ybcaMGQ32S0xMxMaNG122zZo1y+X+xcM2ja13Ulpa2q52toVX94wo9p4RDcMIERGRarw6jHCYhoiISH0dCiPz5s2DJEl46qmnmt1vy5YtSEtLg4+PD5KTk7Fw4cKOvK372NcZkRSGESIiIrW0O4zs2rUL77//vsviKI3JzMzE5MmTMXbsWOzbtw/PP/88nnjiCSxdurS9b+02klaUzHCYhoiISD3tCiOVlZW4//778cEHH6BHjx7N7rtw4UIkJCRg/vz5GDBgAB555BE8/PDDeP3119vVYHeS7D0jDCNERETqaVcYmTVrFqZMmYLrrruuxX23b9+OiRMnumybNGkSdu/eDYtF3RAg6exhhMM0REREqmnz1N4lS5Zg79692LVrV6v2LygoQFRUlMu2qKgoWK1WFBUVISYmpsFzTCYTTCaT8355eXlbm9kqknM2DcMIERGRWtrUM5KTk4Mnn3wSixcvho+PT6ufJ0mSy33HPOaLtzvMmzcPwcHBzlt8fHxbmtlqGkcYYc8IERGRatoURvbs2YPCwkKkpaVBp9NBp9Nhy5YteOutt6DT6RpdKz86OhoFBQUu2woLC6HT6RAWFtbo+8ydOxdlZWXOW05OTlua2WqOYRotwwgREXmJxMREzJ8/X+1muGjTMM2ECROQnp7usu03v/kN+vfvj+eeew5abcNLII8ePRrff/+9y7Z169ZhxIgRDa4M6GA0GmE0GtvStHbR6MT7axUWsBIREamlTWEkMDAQgwYNctnm7++PsLAw5/a5c+ciNzcXn332GQBg5syZeOeddzBnzhz87ne/w/bt2/HRRx/hq6++ctNHaD+Nlj0jREREanP7Cqz5+fkuF+1JSkrCqlWrsHnzZgwdOhR/+9vf8NZbb+H2229391u3mUZvDyNo+VLMREREanvvvffQs2fPBlffvfnmmzF9+nScOnUKt9xyC6KiohAQEICRI0diw4YNKrW29Tp8obzNmze73F+0aFGDfcaNG4e9e/d29K3cTmsfptFxmIaIiBQFsFSr8956P6CJSR313XnnnXjiiSewadMmTJgwAQBQUlKCtWvX4vvvv0dlZSUmT56Ml19+GT4+Pvj0008xdepUZGRkICEhobM/Rbt59VV7Nc4CVvaMEBF5PUs18PdYdd77+TzA4N/ibqGhobjhhhvw5ZdfOsPI119/jdDQUEyYMAFarRaXXXaZc/+XX34Zy5cvx4oVKzB79uxOa35HefWF8jQ6USSrBWtGiIioe7j//vuxdOlS53pcX3zxBe655x5otVpUVVXh2WefxcCBAxESEoKAgAAcO3bMpXyiK/LqnhGtvWZEzzBCRER6P9FDodZ7t9LUqVMhyzJWrlyJkSNHYtu2bfjXv/4FAHjmmWewdu1avP7660hJSYGvry/uuOMOmM3mzmq5W3h3GHHUjLCAlYiIJKlVQyVq8/X1xW233YYvvvgCJ0+eRN++fZGWlgYA2LZtG2bMmIFbb70VgLiWXFZWloqtbR2vDiN6vX2YhlN7iYioG7n//vsxdepUHD58GA888IBze0pKCpYtW4apU6dCkiT86U9/ajDzpivy6poRxzCNQbKJKmoiIqJu4Nprr0VoaCgyMjJw3333Obe/8cYb6NGjB6688kpMnToVkyZNwvDhw1Vsaet4dc+I1j6bBgAUm8W5PDwREVFXptVqkZfXsL4lMTERGzdudNk2a9Ysl/tdcdjGq3tGdIa68GGzmJrZk4iIiDqLd4cRfd31b6xWLnxGRESkBi8PI3U9I1b2jBAREanCq8OIXqeDVRGHwNLF52ATERFdqrw6jGg1EqzQAmDNCBERkVq8OowAgNU+oYg1I0RE3kfhsg4d5o5j6PVhxGIPIzJ7RoiIvIZeL1bgrq5W6Sq9lxDHMXQc0/bw6nVGAMAqiWEaq5U1I0RE3kKr1SIkJASFhYUAAD8/P0iSpHKruhdFUVBdXY3CwkKEhIRAq9W2+7W8PozYHD0jDCNERF4lOjoaAJyBhNonJCTEeSzby+vDSN0wDcMIEZE3kSQJMTExiIyMhMXCusH20Ov1HeoRcfD6MGKTtIDCYRoiIm+l1WrdckKl9vP6AlabJApuZM6mISIiUgXDiCQ6hxQO0xAREamCYcSx6JmNPSNERERqYBixD9MoNvaMEBERqcHrw4is4dReIiIiNXl9GHHUjLCAlYiISB1eH0Zk+zANOExDRESkCoYR+zCNwgJWIiIiVXh9GFEcU3s5TENERKQKrw8jssY+TCMzjBAREanB68OI4hymYc0IERGRGrw+jDhqRsCaESIiIlV4fRiBxiD+ZBghIiJShdeHEYU9I0RERKpiGGEBKxERkaq8PoxAK8KIJFtVbggREZF38vow4ugZkdgzQkREpAqvDyOSvWaEYYSIiEgdXh9GFJ2YTcNhGiIiInV4fRiROExDRESkKoYRewGrhmGEiIhIFV4fRpyzaRQO0xAREanB68OIZK8ZYc8IERGROhhGnMM07BkhIiJSA8OI1t4zwmEaIiIiVXh9GNHoRM+IlmGEiIhIFQwjWoYRIiIiNXl9GJH0RgAcpiEiIlJLm8LIggULMGTIEAQFBSEoKAijR4/G6tWrm9x/8+bNkCSpwe3YsWMdbri7OIZpdApn0xAREalB15ad4+Li8OqrryIlJQUA8Omnn+KWW27Bvn37kJqa2uTzMjIyEBQU5LwfERHRzua6n8ZewKpVbCq3hIiIyDu1KYxMnTrV5f4rr7yCBQsWYMeOHc2GkcjISISEhLSrgZ1NY19nRAsO0xAREamh3TUjNpsNS5YsQVVVFUaPHt3svsOGDUNMTAwmTJiATZs2tfjaJpMJ5eXlLrfOotU7hmkYRoiIiNTQ5jCSnp6OgIAAGI1GzJw5E8uXL8fAgQMb3TcmJgbvv/8+li5dimXLlqFfv36YMGECtm7d2ux7zJs3D8HBwc5bfHx8W5vZalp7ASt7RoiIiNQhKYqitOUJZrMZ2dnZKC0txdKlS/Hhhx9iy5YtTQaSi02dOhWSJGHFihVN7mMymWAymZz3y8vLER8fj7KyMpfaE3c4cewQ+iwZg1oY4PPX8259bSIiIm9WXl6O4ODgFs/fbaoZAQCDweAsYB0xYgR27dqFN998E++9916rnn/FFVdg8eLFze5jNBphNBrb2rR20dmHabRgASsREZEaOrzOiKIoLr0YLdm3bx9iYmI6+rZuo9H7AAD0sAFt6yQiIiIiN2hTz8jzzz+PG2+8EfHx8aioqMCSJUuwefNmrFmzBgAwd+5c5Obm4rPPPgMAzJ8/H4mJiUhNTYXZbMbixYuxdOlSLF261P2fpJ209nVGAAA2C2CfXUNERESe0aYwcu7cOTz44IPIz89HcHAwhgwZgjVr1uD6668HAOTn5yM7O9u5v9lsxtNPP43c3Fz4+voiNTUVK1euxOTJk937KTpAr683HCRbADCMEBEReVKbC1jV0NoCmPa4UFaBsDfiAADKc1mQfHu49fWJiIi8VWvP315/bRpdvZ4Ri9msYkuIiIi8k9eHEb1OA6siDoPN2vpCXCIiInIPrw8jOo0GFnvpjMXCi+URERF5mteHEb1WggVaAIDNzJ4RIiIiT/P6MCJJEqyOMGJlzQgREZGneX0YAQArxFojVgvDCBERkacxjADsGSEiIlIRwwgAqyTCiMyeESIiIo9jGEHdMI3Nytk0REREnsYwAsDm6BnhOiNEREQexzACwCaJdUbYM0JEROR5DCMAbPZFz2QWsBIREXkcwwjqekYUhhEiIiKPYxgBIGvsPSM2hhEiIiJPYxgBYJPEbBrZalW5JURERN6HYQSAbB+mAWfTEBEReRzDCOrCiCxzNg0REZGnMYwAsGnEMI3Cqb1EREQexzACQHEM07CAlYiIyOMYRgAo9tk0io09I0RERJ7GMAJAtg/TgGGEiIjI4xhGACgMI0RERKphGEHdMA04m4aIiMjjGEZQv2eEi54RERF5GsMIADjDCGfTEBEReRrDCABFawAASAp7RoiIiDyNYQQAtI51RlgzQkRE5GkMI4BzmEbDAlYiIiKPYxgBAK0IIxLDCBERkccxjACQHGGENSNEREQexzACOHtGOExDRETkeQwjAOCYTSOzZ4SIiMjTGEYAaHT2nhEO0xAREXkcwwgASSN6RjhMQ0RE5HkMIwAke8+Ilj0jREREHscwAg7TEBERqYlhBIBkL2BlzwgREZHnMYwA0OgYRoiIiNTCMAJA66wZYQErERGRpzGMoH7PiE3llhAREXkfhhHUCyPgMA0REZGnMYwA0NrDiI41I0RERB7HMAJAo7eHEfaMEBEReRzDCACtzggA0IE1I0RERJ7GMAJAqxezaXSwAYqicmuIiIi8S5vCyIIFCzBkyBAEBQUhKCgIo0ePxurVq5t9zpYtW5CWlgYfHx8kJydj4cKFHWpwZ9DZa0YAADZO7yUiIvKkNoWRuLg4vPrqq9i9ezd2796Na6+9FrfccgsOHz7c6P6ZmZmYPHkyxo4di3379uH555/HE088gaVLl7ql8e6iMxjr7vBieURERB4lKUrHxiVCQ0Px2muv4be//W2Dx5577jmsWLECR48edW6bOXMmDhw4gO3bt7f6PcrLyxEcHIyysjIEBQV1pLmNyr1Qhp5vJ4g7z2UBvj3c/h5ERETeprXn73bXjNhsNixZsgRVVVUYPXp0o/ts374dEydOdNk2adIk7N69GxZL0z0QJpMJ5eXlLrfOpHcZpuGMGiIiIk9qcxhJT09HQEAAjEYjZs6cieXLl2PgwIGN7ltQUICoqCiXbVFRUbBarSgqKmryPebNm4fg4GDnLT4+vq3NbBOdTguLogUA2KzmTn0vIiIictXmMNKvXz/s378fO3bswGOPPYbp06fjyJEjTe4vSZLLfceo0MXb65s7dy7Kysqct5ycnLY2s010WglWiDBitZg69b2IiIjIla6tTzAYDEhJSQEAjBgxArt27cKbb76J9957r8G+0dHRKCgocNlWWFgInU6HsLCwJt/DaDTCaDQ2+bi7GbQamKGFLwCrxQzPvTMRERF1eJ0RRVFgMjXemzB69GisX7/eZdu6deswYsQI6O1re3QFOo0Eiz2XcZiGiIjIs9oURp5//nls27YNWVlZSE9PxwsvvIDNmzfj/vvvByCGVx566CHn/jNnzsSZM2cwZ84cHD16FB9//DE++ugjPP300+79FB2k1dQN09jMDCNERESe1KZhmnPnzuHBBx9Efn4+goODMWTIEKxZswbXX389ACA/Px/Z2dnO/ZOSkrBq1Sr8/ve/x7vvvovY2Fi89dZbuP322937KTpIkiRYHT0jNtaMEBEReVKH1xnxhM5eZwQAzvylH3pJBTh3x3eIGjS+U96DiIjIm3T6OiOXGqvE2TRERERqYBixs0IU1MpWLgdPRETkSQwjdjaJi54RERGpgWHEzmYvYFUYRoiIiDyKYcTOJokwIjOMEBEReRTDiF1dGGHNCBERkScxjNg5wohiY88IERGRJzGM2MmSYzYNwwgREZEnMYzYyRr7MI3NqnJLiIiIvAvDiJ1sH6YBe0aIiIg8imHETtaIYRpFZgErERGRJzGM2Dl6RhTOpiEiIvIohhE7xV4zAs6mISIi8iiGETvnMI2NPSNERESexDBip9jDCFgzQkRE5FEMIw7OYRqGESIiIk9iGLFjzwgREZE6GEbsFK0IIxIXPSMiIvIohhEHe8+IJHM2DRERkScxjNg5ekYgs2eEiIjIkxhGHJw9I6wZISIi8iSGETvJ3jOiYRghIiLyKIYRB60BACBxmIaIiMijGEbsJK1YZ4RhhIiIyLMYRuwcwzRahcM0REREnsQw4sBhGiIiIlUwjNhpdOwZISIiUgPDiJ1k7xnRKOwZISIi8iSGETuNzh5GOExDRETkUQwjdhymISIiUgfDiJ1jmEYLm8otISIi8i4MI3YavT2MsGaEiIjIoxhG7LT2YRodwwgREZFHMYzYSVojAPaMEBEReRrDiJ3OMUwDhhEiIiJPYhixcwzT6BlGiIiIPIphxE6jtw/TQAYUReXWEBEReQ+GETtHzwgAwMa1RoiIiDyFYcROZ+8ZAQDYzOo1hIiIyMswjNhp7QWsAACZPSNERESewjBip9fVCyM2FrESERF5CsOInV6vgUXRijscpiEiIvIYhhE7nUYDC3QAAIVhhIiIyGMYRuz0WglWiJ4Rm5U1I0RERJ7CMGKn02pgcYQRi0nl1hAREXmPNoWRefPmYeTIkQgMDERkZCSmTZuGjIyMZp+zefNmSJLU4Hbs2LEONdzddBrJOUxjsXCYhoiIyFPaFEa2bNmCWbNmYceOHVi/fj2sVismTpyIqqqqFp+bkZGB/Px8561Pnz7tbnRn0Gs1HKYhIiJSga4tO69Zs8bl/ieffILIyEjs2bMHV199dbPPjYyMREhISJsb6ClajQSrogUkQOYwDRERkcd0qGakrKwMABAaGtrivsOGDUNMTAwmTJiATZs2deRtO41FEkvCW60cpiEiIvKUNvWM1KcoCubMmYOrrroKgwYNanK/mJgYvP/++0hLS4PJZMLnn3+OCRMmYPPmzU32pphMJphMdb0T5eXl7W1mm9jswzQywwgREZHHtDuMzJ49GwcPHsRPP/3U7H79+vVDv379nPdHjx6NnJwcvP76602GkXnz5uHFF19sb9PazSqJw2FjASsREZHHtGuY5vHHH8eKFSuwadMmxMXFtfn5V1xxBU6cONHk43PnzkVZWZnzlpOT055mtpnNns1kLnpGRETkMW3qGVEUBY8//jiWL1+OzZs3IykpqV1vum/fPsTExDT5uNFohNFobPLxzmKTdIACyBbOpiEiIvKUNoWRWbNm4csvv8R3332HwMBAFBQUAACCg4Ph6+sLQPRq5Obm4rPPPgMAzJ8/H4mJiUhNTYXZbMbixYuxdOlSLF261M0fpeOcYYQ1I0RERB7TpjCyYMECAMD48eNdtn/yySeYMWMGACA/Px/Z2dnOx8xmM55++mnk5ubC19cXqampWLlyJSZPntyxlncCm71mROY6I0RERB7T5mGalixatMjl/rPPPotnn322TY1SiyzxQnlERESexmvT1FPXM8IwQkRE5CkMI/XIGrHomWLjMA0REZGnMIzUUzdMwzBCRETkKQwj9TjDCIdpiIiIPIZhpB5Fax+mkdkzQkRE5CkMI/U4ekbAYRoiIiKPYRipR7EXsDKMEBEReQ7DSD0MI0RERJ7HMFKPouFsGiIiIk9jGKnHsc6IxAJWIiIij2EYqc8RRtgzQkRE5DEMI/XZp/ZCtqrbDiIiIi/CMFKf1jFMw0XPiIiIPIVhpD5nGGHPCBERkacwjNRnrxnRsICViIjIYxhG6pHYM0JERORxDCP1aQ0AAElhzwgREZGnMIzU4+gZ0bBnhIiIyGMYRuqRdPYwwp4RIiIij2EYqYc9I0RERJ7HMFKPxl4zolUYRoiIiDyFYaQeSSfCiIZhhIiIyGMYRurR2GtG2DNCRETkOQwj9ThqRhhGiIiIPIdhpB6d3giAYYSIiMiTGEbq0dhrRrRgGCEiIvIUhpF6HDUjOvaMEBEReQzDSD1anRim0cEKyLLKrSEiIvIODCP1KP5hsChaaCEDFflqN4eIiMgrMIzUo9MbkKNEiDvFp9RtDBERkZdgGKlHr9UgS4kWdy6cVLcxREREXoJhpB6dRoNMJUbcucCeESIiIk9gGKlHr5WQ6ewZYRghIiLyBIaRenRaTV0YYc0IERGRRzCM1KPTSMiSHWEkE5Bt6jaIiIjICzCM1GPQaZCHMJgUPSBbgNJstZtERER0yWMYqSfETw9IGmQpUWIDh2qIiIg6HcNIPUadFjFBPvWm9zKMEBERdTaGkYskhPlxei8REZEHMYxcJCHUjzNqiIiIPIhh5CIJoX51M2q4CisREVGnYxi5SHyoH047ekZKswGrWd0GERERXeIYRi7SK8wf5xGCKvgAigyUZKndJCIioksaw8hFEkL9AEjIlFk3QkRE5AkMIxfp4adHgFHH6b1EREQewjByEUmSXGfUsIiViIioU7UpjMybNw8jR45EYGAgIiMjMW3aNGRkZLT4vC1btiAtLQ0+Pj5ITk7GwoUL291gT0gI9eMwDRERkYe0KYxs2bIFs2bNwo4dO7B+/XpYrVZMnDgRVVVVTT4nMzMTkydPxtixY7Fv3z48//zzeOKJJ7B06dION76zJIT5cZiGiIjIQ3Rt2XnNmjUu9z/55BNERkZiz549uPrqqxt9zsKFC5GQkID58+cDAAYMGIDdu3fj9ddfx+23396+VneyhFA//NexCmt5LmCuBgx+6jaKiIjoEtWhmpGysjIAQGhoaJP7bN++HRMnTnTZNmnSJOzevRsWi6XR55hMJpSXl7vcPCkh1A+lCEQ5AsSGkkyPvj8REZE3aXcYURQFc+bMwVVXXYVBgwY1uV9BQQGioqJctkVFRcFqtaKoqKjR58ybNw/BwcHOW3x8fHub2S5iei+Q6bh6L4tYiYiIOk27w8js2bNx8OBBfPXVVy3uK0mSy31FURrd7jB37lyUlZU5bzk5Oe1tZrvEhvhCIwGnZF4wj4iIqLO1qWbE4fHHH8eKFSuwdetWxMXFNbtvdHQ0CgoKXLYVFhZCp9MhLCys0ecYjUYYjcb2NM0tDDoNYkN8kVUeDWjBMEJERNSJ2tQzoigKZs+ejWXLlmHjxo1ISkpq8TmjR4/G+vXrXbatW7cOI0aMgF6vb1trPYhX7yUiIvKMNoWRWbNmYfHixfjyyy8RGBiIgoICFBQUoKamxrnP3Llz8dBDDznvz5w5E2fOnMGcOXNw9OhRfPzxx/joo4/w9NNPu+9TdALXhc8YRoiIiDpLm8LIggULUFZWhvHjxyMmJsZ5+89//uPcJz8/H9nZ2c77SUlJWLVqFTZv3oyhQ4fib3/7G956660uO63XwWWtkapCoNazM3qIiIi8RZtqRhyFp81ZtGhRg23jxo3D3r172/JWqksI9UMl/FCi6YEecokYqokdpnaziIiILjm8Nk0THNN7uRIrERFR52IYaUKvUH8AwHGLfa2RouMqtoaIiOjSxTDShGA/PYJ8dDig9BYbjq9Vt0FERESXKIaRZiSE+WGNbSQUSQPk7weKT6vdJCIioksOw0gzEkL9UIwg5PUYKTYcXq5ug4iIiC5BDCPNSLDXjez2Hy82HGIYISIicjeGkWY4ZtSsV0YBGh1wLh0oOqFyq4iIiC4tDCPNcISRo2U6IHm82MihGiIiIrdiGGlGrzARRnJKaiAPvFVsPLRMxRYRERFdehhGmhET7AOtRoLZKqOw53WARg+cPwoUHlW7aURERJcMhpFm6LQa9AzxBQCcqdIDKRPEAxyqISIichuGkRY4hmqyi6uB1NvExsPLgVZcp4eIiIhaxjDSgnh7Eevpoiqg342A1iiWhj93WOWWERERXRoYRlowND4EALA54zzgEwT0uV48cJiFrERERO7AMNKC6wZEQSMBR/PLkVNcDaTWm1XDoRoiIqIOYxhpQai/AaOSQgEAaw8XAH1vAPR+QEkmC1mJiIjcgGGkFSalRgMA1h0+BxgDgDFPiQfW/REwVarXMCIioksAw0grXD8wCgCw60wxiipNwJgngJBeQHkusO3/VG4dERFR98Yw0gpxPfwwqGcQFAXYcOQcoPcFbnhVPPjL20DRSXUbSERE1I0xjLTSpIFiqGbt4QKxod+NQMp1gGwB1jzHYlYiIqJ2YhhppUmDRBj5+eQFVNRaAEkCbviHWCL+5AYgY7XKLSQiIuqeGEZaqU9kAJLC/WG2ydhy/LzYGJ4CXDlb/H3NHwBLjXoNJCIi6qYYRlpJkiRMtBeyrj18ru6BsU8DgbFA6Rngx7+p1DoiIqLui2GkDSbap/huOlYIk9UmNhoDgBv/If6+413OriEiImojhpE2GBYfgshAIypNVvxy6kLdAwNvBq6394r8+BLw63vqNJCIiKgbYhhpA41Gcq45ss4xq8ZhzBPAuOfE31c/C+xb7OHWERERdU8MI23kWI11/ZFzsMkXTecdPxe4Ypb4+4rHuVw8ERFRKzCMtNEVyWEI9tWjqNKMrSfOuz4oScCkV4Dh0wFFBr75rRi2sZrUaSwREVE3wDDSRgadBrcPjwMAfLEju+EOkgTc9AYw7EFAsYmC1oVjgbO7PdxSIiKi7oFhpB3uuzwBALDx2DnklTaytohGC9zyDnD3YsA/EijKAD66XlxYz1zt4dYSERF1bQwj7ZASGYArkkMhK8CSXTlN7zhgKjDrV2DI3WLY5pe3gfmDxNBNWW7Lb1RZCKx9AfjwOvGc4tPu+xBERERdhKQoXf+iKuXl5QgODkZZWRmCgoLUbg4A4PsDeXj8q32IDDTi5z9cC722hVyXsQZY9QxQZh/akbRiSnDab4CYIYBvj7p9q4qAn98Edn4AWC/qeel1FTD8QWDAzYDBz70fioiIyI1ae/5mGGkns1XG6Hk/4kKVGQsfSMMN9mvXNMtmBTJWiXVIzvzk+phvKBCWAgTFACc2AJYqsb1nGnDZvcDxNcDJHwHY/7n0/kDficDAaUCf6wGDvzs/HhERUYcxjHjAP9Ycw4LNpzC2Tzg+/+3lbXtyQboIJSc3ABX5DR+PGQpc84IIGpIktpWdBfZ/Bez7XCw/76D3E1cQjh8FRKUCUYOAgMjm319RRHFtQTow4mEg6eq69yEiInIDhhEPyL5Qjatf2wQA2PLMePQKa2fvhKlS1IMUnwJKsoDIVNcQcjFFAfL2Aoe/BY585xpMHPwjgOTxwMRXgMAo18dsVuD7J4D9X9Rtix0OjHlS1LlotO37HERERPUwjHjIQx/vxNbj5/HouGTMvXGA5xugKED+AeDEeuDcIeDcYeDCSTiHc/zCxcyefjeK+5Ya4JuHxXCRpBXh4/gawForHg9NBqb8H9D7Ws9/FiIiuqQwjHjI2sMFePTzPQjzN+CXudfCqOsCvQrmaiBvn1iW/twhsS3tN2K5+qW/Bc78DGiNwJ2LgP6TRcHsr+8BO98HaksBQwDwP5uB8D4qfggiIuruWnv+5tTeDprQPxJRQUZcqDJjzaGClp/gCQY/IHEM8LuNwJWPi217PgHeSBVBxBgEPLhcBBEA8A8Hrn0B+P1hMVvHXAl8PUP0ojTmwimgutgjH8WtZJvaLSAiokYwjHSQTqvBvaPEImivrj6GosoutPS7zghMfBl4aAUQGCtWhPWPAGasFGHlYsYA4I6PxD7nDgGrn3N9XJaBza8Cbw8H5g8RBbBNBZaupPI88J8HgHnxQPo3areme8veAfzwe1FMTUTkJhymcYNKkxW3vPMTTp2vwhXJoVj828uha2ndEU+rLgYOLwP6TAJC4pvf9/Rm4LNpABTgtg+AIXcBteXA8plAxkrXfYPjgQl/BgbdAWi62GcGxPouK2YDVfbrCGl0wL1LRIGwt1IUIP1rYNu/gJQJ4t9PZ2z5eVVFwDsjgZpiILwv8PBawC+089tLRN0Wa0Y87GRhBW5552dUmW343dgkvDBloNpN6phN84Atr4r1TO74CFj/F7GsvdYATPmXOHlteBEot39Djh0m6lIGTHXvCUqWAZsJ0Pu27XmmSmDdC8CeReJ+5ECgR5IIUzpfYPoKMRXa25RkAT/MAU79WLcteoioHwrr3fxzlz4iQoxD3Cjgoe+4+B4RNYlhRAVrDuVj5uK9AIC37x2GqZfFqtyiDpBtwOfTgMytddsCY8X1duLSxH1LDbD9XeCnN0SdCSBm6CSPB1JvBfpP6VgwqS4GPr9VTF1+YKlYAK45igIUHhFTng98BZTZl+ofPRu49k+ApAG+ukeciH1CgIfXAJEqzIDqiMpC4NhKMf3aGAT4BAHGYFFs7NPM/w2bFfh1AbDp74ClWhQwp80Q4aKmWBQt3zQfGHJn488/vhb48i5xDG/5N7DmOaC2DOg3Gbjrc0CrE/tZaoD9XwKZW4CxT4vVhbuivZ+Lou1r5oqfU2/j+LXfVdYWslnEMgU9h4sZfXTJYBhRiWMhNF+9Ft/OGoN+0YFqN6n9Ks4BC68CqgqBhNHAnZ82XLMEECfIvZ8BR74Vi6g5SBogbiTQd5IYHopKBUoygayfxO3ML0BgjJh6HNHP9TVNFcBntwC5e8R9/0jgkQ1Aj14N378sV/SAHPkWKDpetz2oJzBtAZA8rm6buUq87tldIlzd/DZQmiXaXZAuaiHiR4nl9vtMBHxD2nXo3E6WgX2fAev/LELAxXxCgPv+CyQ0svhedTHw1b1Azg5xP3GsCB7hKUB5nujxOPOzeGzoA8AN81yDjakCePcK0Qs2ejYw6RXgzHYRVq21wPDpwHV/BXZ9KE7w1UXieb49RH1SVGrLn09RxM9GyRnxb2SuEgHXUgMERotem7AUwOiG/087FoowBYhhuzsXiR699rLUtL3nrjWyfhbXs7ry8cZrvFrLZhX/N84dAooz7WsaZYoasZvmA/1ucFeL2++H3wO7PxYh+epngDFPtG7osLuRbcCxH4Ddn4je2ol/u+TXdWIYUYlNVjD945346WQRooN8cMOgaAzuGYzBccHoHREAraaLfBNpreJMIOdXIPU2QGdoef+ik8CR5aJ3wjGt2MEQUNeD4rI9ELj9w7pfipYa4Is7gaxtYpn8gCjg/FEgor+oU6gfEI6vBZb9ru4ErTWI1WgHThPfeI0BDd+vuhj45Ebg/LHmP4tGL1am7X2tqJEI6w2E9BK9ALVlwNndItSc3SVOaje/3fLKt42RZdE7UVkoei1CEkQRseNba+Ex4IengOzt4n7EAFH3Y6oQtTyV50QA0PsB93wJ9L6m7rXLzgKf3yaG2IzBIkgMe8D1G7FsA7b8E9jyDwCKCGlT/q9uttWqZ8S07x6JwGPb64Zljn4P/PchcRFIjR6QLWJ7cIIIDYWHxef4zeqG08Rlm+h1cxy/3D1A9YWWj1VgDBAcJ463pKm7hfcBksYBiVc13xv381vA+j+Jv4f3E8dFoxNBe8BNLb//xXZ9CKz+g+i1u+kNIKqR4VlTJXB4ORA9SAxntkbVBeDfl4taJ61BhOrBd7juI9uA7e+IADj0fjF139FD5VBbJmbGndrYxBtJwHV/AcY81XIvyfnjwN5Pxc/BiN+6r0bs4NfAskdct4X3A6bOB3pd6Z73qM9SK352FZtYf8knuHXPUxSxhpNfWNt7fC01YpHJ7e+6XvA09TbgtvcBrb5tr+du1cXAoaWit9TNbWEYUVFxlRk3v/MTzpa4zjTxM2jx4BW98Mykfl2vwLUzlOYAJ9YCx9eJbntrrThpxY0QJ424UcDP8+3fyiXg2j8CVz4B/PdBsRCbIRCY8b3oFflwglg2P3k8cP834gS0+VVg6z/Fe8UMBa74fyLQtOaXS3kesPgOEQKiB4sl9KMHi+BzaqP4ZVWU0fB5Gh0QEA2U58K5sJxDWB9RQxHcs/n3tprFN9U9i8Q06arz4hdjfYZAICxZ9O6cWC9O9Hp/MQV71KOuJx1zlZgtdGqjOHHduUgEscKjwOLbRVuDegIPLAMi+zfdrqyfgO9mix4KQAS6wXcA/3lQfNYHv3UNOgCw6yNg5Rzx9+jB4qQ2cBpgrgA+nSp6mwJjgd+sAkKTxLf0Q98AW1+zL85Xj9YAhPYWQcbgL246H9H+ohN1PS7NksTQUNI4cSKLv7zuxLH1NWDjy+LvVz8rTt7LHxXt0eiAuz5r/ZCNooifvy2v1m3T6MQqxlc/I3pKastEiNv+b/FzBgm44jHxc97StaS++a1ol9YoaqYAMTNu9GwRGkqzRUG5o0cLENPyb/9QXN8KEPVBX94tQrfeT1zjKqy3GAbpkSjatvtjse/gu4Cb32rYw6Mo4udqxwLg5Pq67b0nALcubF/4ru98BvD+NeJaXFc/I75wrPlDXcH58IeAG18D9D6NP//EBvF/acyTLa+LVHVBhMddH9S9vtYoAsmQu8WXmKa+cGXvEHVzjt7FyFTx89XrSvGFxT+86fc9/K34P+II2z4hwMBbxHCmbBFDnXcu8nxPkCwDpzeJy4scWwnYzGIYviO9hI3otDCydetWvPbaa9izZw/y8/OxfPlyTJs2rcn9N2/ejGuuuabB9qNHj6J//2Z+MdbT3cIIAJRVW7Ax4xzSz5bjUG4ZDueVocosTjjj+kbg7fuGIchH5TTsSZYaMYQSluL6i9hmEb98dn0o7gfHi1oPnY84eTq6p/MPAB/fKH5pDblb9CKcFkvxY+QjwKS/u/8/8/njwLHvgbz9IjQUn6pbqRYQvSTxo8S33R0LRLtDeoni2B6JDV+vulgEkJ3vN349It9QcTIoz0ODoNP3RmDya03PhLKaxIJ2R78XdTtj/7duEbvwfsCDy0SPQkvM1eIE+8s7rgFp6APAtHcbf87JH0WQSLzK9dt1VRGwaIo4GQYniK73Hf+u+2bo2wNIuV6E054jRM9Bc/+GNSXi36GiQLRNkcXNahaXRzi9pfEAGTFA9DadWCvuX/MCMO5Z8XebFVj+P+JboUYvTuYDb2m+l0C2AauerjuRj3lStOvYD+J+j0Sg/02iLsVk77ELiBI9WIBoy9S3GgY7h2MrgSX3icD98DrRtl8XiMcunyl+3lY9A5jKRUAd8Rvxc2WuFCsu3/6B2L7kPhHgAmPEDLLYoQ3fa+cHYgq/YhOXhLjiMXHSrCoSz83eUa8HURJfBrK3i/8H/pHiW31Tn6Ml5irggwmi1zNxrAjyGq34d97w17ri8/43iZ6ri3t9Tm4AvrzHHtT9xO+AtBkN/+2KToifu/1f1V0FPShO9JrW7x317SGCRcKVQK/R4gvKhZOiUN8xi1CjA2Sr6+vr/cX/r4QrGn7GwmPA++PE8QpJAK6YJXomjQHiS9p/HxSP9b4WuPsL0euoKOJ3Sd5+EcwTxzb87IDo4clYJT6D1STChONPjVb8PGsN4rkaHYB6x8VSLX5XOOrqAFHIfu2fxAVY3ajTwsjq1avx888/Y/jw4bj99ttbHUYyMjJcGhIREQGttnVjZd0xjFzMJitYfSgfz3x9EDUWG1IiA/DR9BHtv57NpWb3J+IXvGwV/4nu/arh9Nvja0UBqiKL+zpfYOqbwGV3e6aNsgxU5Iken7Dert8KS7OBT28WvQqBsSKQhPcRAeTURtH2Yz+IXwKAODmN/B3Q5zrxd/+Iuu5RS634Vlt8SgyTRfQXU3Bb6ka3WYEVjwMHvqzbFjcKuO8/be9WztsvXqvgoGjbrJ3tK0auOCeGxIpP1W3zDRV1EKN+554aEJf3KxDDP5lbxfBi/RoiQNS2XPV71231AwkgwkvadBF6L/7MlloxLHh0BQAJmPK6CMMAcPQHERIq8ur2j+gvCnlTbxVT5n94qu4EMPQB4PoXXb9VVxcD/75CBJcxTwLXvyS2//KOmB1WX9wo4Lb3RE9H0Ung6+n2oVHJftK0ADGXiSAS1Ewx/ekt4rk1JY0/bggAhj0IXP4/4r0KjwJf/0aECEgiZA65WzzW2toZRQG+fUwUmgdEAY9ua1iPdnID8NV9omdoyN3AtIV1Q0PZv4qaJUu1CEVVhWJ7v8liuNQ3VBSq/7pQvI5DzFDxszfwFnGMCg4CB/8rCrkdYdHBGCQCniKLYDjsAWD8XPH7KfsXUfN2coMILAHRwKNbRH2Tg9Ukwta5dBE27vu6Yag4vUX8TrNUizDoFyZWz67fCxgQBQy+UxyD6MGizXs/B9L/23j9WFv4BIteseEPip+VTuCRYRpJklodRkpKShASEtKu97kUwojDodwyPPLpbhSU1yLET4+FD6ThiuQwtZvVNZzZLmbmjHi46aK6nR+I0BLaG7j789YVR3pKRYEojj1/TJzAw/qIE2L9HoboweLb0aDbOqdbVpaBtXPFL+G+NwB3fNL+qbc2q/jmFZXa8rTf5pSdFevW1JaKE8GI3zZey9MZquzf7s/uFN90h9zV+H42q6gl2f1J3bdnRxe+3lecqCoLxZBRTYn4xnnbB0DqNNfXMVWI4Zv8AyKkDLjZtbbCVAH8+JL4OYYi6njGPQuM+h8xRLDsUeDgElGj9Og21+GJQ0vF0IwiA+P+IEJV/ZObpUb0cuz9VNzvf5PouWhpSAgQvVXr/iRObv7hoofFP1z0pg2Y2nDo01IDrH2+rncIACCJns3wFPH/s0di3S0oVnz2ykJxLM/uBH5+U5zkp38vetUak7FaDEHKVvFzM+X/xIy5T24UbU25TtRJ7XxfHFebWYQTn6B6w4CS+L9w5Wyg15jGQ71sA3J21oWMnJ2i58lxHCf8uWGRPSDqgT68TgSzhNHiszi+VKx9QdT0+IUBj/3iGlTqy94hauQc7weIoBQ5UPzfqam32rV/RN0QEyB6ePpcJ3pndAbxM6vVi58Rm0UcD5ulYW+OJIk6pwFTO6f4up4uF0YSExNRW1uLgQMH4o9//GOjQzcOJpMJJlPdSqbl5eWIj4+/JMIIAJwrr8XvPtuNg2fLoJGAK3uH4+bLYjFpUDSCfb1o6Ka9SrJE13NXrLavKhLf2OrPKooYIGYU9ZsshnU8MZ2y4pzouekqUzdlm73YtIu0pyk1peJb8t5PXf8N6/MJFtOZ68/SaqvsHaIXpeCguB+aDAy6XdS1OIZn4kc2fF5JluhVCE1q+rUz1ogT/rAHO38hwiPfiV6booz2fUuf8GcxpNic9G/EjC8oYhgmY7X4fPGXizomR9guSBf7OYZejEHiGIz6XfPHqzGyTVx0VGdsPITUV3QS+OAaESYunwnc+A/g1CbxewAA7vmqrhi8KQXpYkZieF/RQxKVKoKo1Sx6Xw4uEZ/bZhZBuP8U8dmSx3f52ThdJoxkZGRg69atSEtLg8lkwueff46FCxdi8+bNuPrqqxt9zl//+le8+OKLDbZfKmEEAGrMNsxddhDf7q/r0jVoNRjXLwL/c3UyRiZyZctuq6ZEfOsL6immBzc2HZm6NkUR3eXH14h6hIAoEe4CIkVwaE1vQ0tkmyhi/PGlumEGoG76dHeiKKLWpOgEcOGEGF4syaq71RSLb+0BUWI4JiBK9CRc8f9aF5j2fAp8/0Td/chU4DcrRZ1HfZYa0StoDBLDGp7qgTu2Clhyr/j7Df8QPbyVBaKX96Y33PMeNaWiNipmaLda+bjLhJHGTJ06FZIkYcWKFY0+fqn3jNR35kIVfjiYjxX785BxrgKACCXvP5SG8f06WKlORF2fqUKcvH55R3wzfmR9p3ede5ylVvQydKRnbPu7YmioR6KY4t/UsIdaNr4serYcwvoAj271+hWKu3QYeeWVV7B48WIcPXq0VftfSjUjzckoqMBrazOw4eg5GHQafDx9JK7q08yUMSK6dJirRK1AVxx+7CoKj4k6Fk/1eLSFbBOrFJ/cIIpcH9nQ+AwmL9Pa87cqi13s27cPMTExarx1l9YvOhALHhiO6wZEwWyV8chnu7DjdCsWgiKi7s/gzyDSksj+XTOIAKJ247YPRDHyXZ8xiLRRI5OXm1dZWYmTJ+sWK8rMzMT+/fsRGhqKhIQEzJ07F7m5ufjss88AAPPnz0diYiJSU1NhNpuxePFiLF26FEuXLnXfp7iE6LUavHv/MDz6+R5szjiPhxftwmcPj8II1pAQEXVtfqFiPSBqszaHkd27d7vMhJkzR6y+OH36dCxatAj5+fnIzs52Pm42m/H0008jNzcXvr6+SE1NxcqVKzF5cgvVxV7MqNNi4QNpeOTT3fjpZBGmf7wTY/tEoE9UAFIiA9A7IgB9owJh0HnBKq5ERHTJ43LwXViN2YbfLNqJHaeLGzwWHmDEY+N74/7LE+Cj79pTu4iIyDvx2jSXCKtNxvbTF3D8XCVOFlbiZGEFMgoqUF4rFrGJCjJi9jUpuGtkPIw6hhIiIuo6GEYuYRabjG/2nMXbP55AXpm4VkrPEF/cOyoe04b1RFwP16lkNWYbNmcUYl9OKcakhGNc3wg1mk1ERF6GYcQLmKw2/GdXDt7ZeBKFFXXrsoxKCsVtw3oi0EePVen52HisEDWWuiXJx/WNwAtTBqBvlJuvC0JERFQPw4gXqbXYsOJAHpbvzcWOzAto7F80rocvLosPwbrDBbDYFGgk4N5RCfj99X0RHsDphERE5H4MI14qr7QG3+3Pw4oDeTBZbbh+YBSmDI7B4J7BkCQJWUVVeHX1Maw5XAAAMOg0uCI5DNf2i8A1/SN5FWEiInIbhhFq1o7TF/DKyqNIz3W9uFVyhD+uHxCFianRGBYfAo2mi1/YjIiIuiyGEWqRoig4WViJTRmF2HisELuzSmCV634cIgONmJgahUmp0RiVFMrZOkRE1CYMI9Rm5bUWbDtehLWHC7DpWCEqTFbnY34GLcakhGN8vwiM7xeJniGX2IW8iIjI7RhGqENMVht+OXUB6w4XYMPRQpyvN1sHAAKMOvgZtPC3/xnqb8Dtw+MwZUgM9NqGK8PKsoKiKhMiAoyQOnLlTiIi6jYYRshtZFnBkfxybM4oxKaM89iXXQK5iZ+amGAfPDwmCXePioefXoudmcVYc7gAaw8X4Fy5CYlhfrhteBxuHdYT8aHefWltIqJLHcMIdZqKWguKq8yoNFlRbbahymRF+tkyfLr9DIoqRQ9KgFEHvVZCSbWlydcZlRSKSanR6B8diD5RAew1ISK6xDCMkMfVWmxYsT8P7287jZOFlQCAUH8Drh8QhRsGRWNofAg2HivEsn1n8cuphuuhBPvq0S8qEFOGxODOEXHwM7T5Oo5QFAX7c0rx04ki9I0OxMSBUQw4REQqYRgh1ciygl1Z4uJ+ab16QNdIDYljPZS92SU4ca4CZ4qrXcJJiJ8eD13RCw9dmdjiomwmqw1H8yuwKj0fKw/mI7e0xvnYdQMi8fK0wYgO9nHPhyMiolZjGKFupdZiw6nzldiZWYxPfs5CdnE1ALEo29iUcEgSYLLKMFllmK0yKk1WlNdYUF5rQa1FdnktP4MWlyeF4qeTRbDYFAT66PDHKQNw14h49pIQEXkQwwh1WzZZwbrDBXhv62nszylt1XP8DFpc2z8SNw2Jwfh+kfDRa5FRUIFnvzmAA2fFwm6jEkPRNzoARp0WRp0GBp0GNRYbiirMuFBlQlGlCZW1ViRHBCA1Nsh+C0ZcD99ODTGKoiDjXAVW7M/DqfOVeGZSP6RE8rpBRNT9MYxQt6coCvZml+BwXjkMWhEejDotDDoNAow6BPnqEOSjR5CvHoFGXaOrxVptMj7+ORP/t+44TFa5kXdpWZCPDqmxwSKc9BQBpWeIL/yNDWtazFYZeaU1yC+rRZ+ogGaHmDKLqvDDAbF0/wl7jQ0ghqg+njESwxN6tKu9RERdBcMIUT1nLlRhzaECVJltMFtlmKw2mKwyfHRahAcaEO5vRHigAT56LU6cq8ThvDIczivH8XMVsNga/y/ib9AiMsgHEYFGaCUJ2cXVyC+rcU571kjAmJRwTL0sFpNSoxHko8Pxc5VYfSgfaw4V4FhBhfO1DFoNxveLQEF5LQ6eLYOvXot/PzAc1/SL9MThISLqFAwjRG5gtso4fq4CR/LKnQHlWEEFKuutTnsxo06D8ACjSyGtQatBZJARZ0vqtmk1Eq7sHYabL4vFpEHRCPLRo9psxczFe7H1+HnoNBJev/MyTBvWs1M/IxFRZ2EYIepElSYrCstrca7chMKKWthkBQmhfkgI9UNEoFgv5cyFKnx/IA/fH8hHxjnRC2LQaXB1n3BMSo3G9QOjEOJnaPDaZquMZ745gO/25wEAHrkqCTcMisZl8SGNrm5bX63Fhl1ZxTiQU4ogXz3ieviiZ4gfevbwhU4jIae4GlkXqnHmQhVyS2vQM8QXIxJDMTAmCAZd3WuXVJmx/2wpjuaXY2BMEMb1jWDxLxG1GcMIUReSUVCBvNIajEwKRUAjtSYXk2UFf1t5BJ/8nOXc5mfQYlRSKEYlhSLE1wCDvQjXoNUgs6gKP508j11ZJTC3ozbGqNPgsvgQRAX5IP1sKbIuVLs8PiYlDC9MHoiBsS3//1MUBbUWGb4GXliRyNsxjBB1c4qi4IeDor7kl1NFza5mW19MsA9GJYWixmxDbmkNzpbUoKxGPDfAqEOvMD8khvkjNsQHp89XYU92CUobee3kCH+kRARg8/HzMFtlSBJwV1o8/ndiX/gbdag0WVFRa0VFrQWZRVU4ml+OI/nlOJpfgeIqMxLD/HB5UhguTw7F5clh6BniC5uswGS1odYio9ZiQ3mtBeU1VpTVWFBWY4FWA/QK80dSmD96+DfsNSKi7oVhhOgSIssKjhVU4JdTRThwtgw1ZhvMNhlmeyFumL8RY1LCMLZPBHpH+DcYUqmotcBslRHqb2jwmKIoOHW+CnvPlOB8pQmDegZjaFwIgv30AICc4mr8Y80x/HAwv0OfQa+VmiwGbkyInx5J4f64IjkMUwbHIDU2yG1DRUfyyvHd/lz4GrQIDzAiItCI8AAjeob4Iiqo4WUJFEVBTnEN9maXQKeVMHFgtMuwFhE1jmGEiNxqz5kSvLzyCPZllwIQs4UCjDoE+ugRG+KDgTFBGBgbhAExQYgJ9kV6bil+PV2MXzOLkZ5bBttFV1fUayXn1OwgXz2CffUwW23IKqpGQXltg/dPCPXDjYOjMTYlAhqNWI/GKiuw2RQY9Rp7W3QIMOoR4qeHj77hMJHJasM7G09iweZTsDZxtccAow69I/zROzIAscG+yDhXgX3ZJSiqNDv3iQ/1xVMT+mLasJ7Q1ptSXmmyYuvx8zBZbbhxUEyjbSDyJgwjROR2iqKgpNoCH70Gvnptq3sqqs1iKMZHp4VRL9aL0TayLkz9/c9cqEZGQQXWHSnAxmOFDVbabY5WI+HypFDcMCgaEwdGIzrYB/uyS/DsNweda7pM6B+JyCAfnK8QC96drzChoLy2QWhyMGg1SO0ZhJziGucFIVMiA/D4tSkor7Vi/ZFz2HHqAsw20c6+UQH4111DMahncIPXKq0243RRFVJjg2DUMbDQpYthhIguGdVmKzYeK8Sq9HwcK6iATiNBq9FAp5Gg0UgwWWyoNFmddSwXB4r+0YE4fq4CsgKEBxjwt1sG4cbBMQ3ex2S1IftCNU4WVuLU+UqcLalBcoQ/0nr1QGpsMHz0WlSbrfj0lzNYuOWUsxanvuRwf5TXWlBUaYZOI+HJCX3w2Pje0Gk1SD9bhs+2Z2HFgTyYrDICjDpMGBCJGwdFY1zfyEaLfhVFQbXZhuIqM0qrLZAkIDzAiFB/A4eKqMtjGCEir6QoCrKLq7Hu8DmsOVyAvdklzosw3jasJ/5000C3FMeW11rw0bZMfL07B9HBPrh+oJiunRIZgAuVJvzx20NYfagAADAkLhgaSXK5vIG/QYsqs81531evRUKoH6yy7ByCMltllNZYmpwhFeyrR6i/ARoJUAAoivj8AT46DIiuGzbrHx0Ii00Rlz2oMItLH5is0GokaCVJ/KmR4KPXwt+ohZ9BB3+jFj72XhtZUZyvLwqQbag221BjFjVLWo0EnUaCTitCYnGVCacKq3DqvAh1RZVmTB0SgzkT+yHYV9+q45tTXI303DLoNJJz5phRp0VyOIubuxOGESIiAIXltdh2ogjxoX4YlRTqsfdVFAXf7c/Dn787hPJasUieXith8uAYPDS6F4bF98C+nFKsOZSPVekFLovkNcag06CHnx6KAlyoMjc5nNSVhQcYMPfGAbhteM8mh/j2nCnBh9tOY+3hAjT1EftGBThnag2ND0GInwF+em2jl4QgdTGMEBF1AfllNXh740n0DPHFXSPiERHY8HpFiqLgaH4FSqvNopdBWzcMFeInej/q1+jIsoKyGgsuVJlQXGWBrCjQSBIkCZAAFFWacCS/Qky3zitHbmkNJAno4WdAeIABYf5GBProICui18MqK7DJMmotMqpMVlSbbag2W1FjtkFjf1EJgCSJXgo/gxa+ei18DeKik7IMWGUZVlmBxaYgyEeH3pEB6B0RgN4R/rDJCv6+6ihOna8CAIxM7IFZ16TAR6+FTVZgk0WvzeId2dhzpsR5XIbEBUOrkWC2X6272j5dvTGSBPjptfA36pASGYCxfSIwtk84BsYEeSykbDl+Hq+uPoYBMYF47ob+iAry8cj7dmUMI0REBEDU3Bi0GuhaWMG3M5mt4qKVb244gRqLrcn9DFoNbhkai0fGJqNfdMOrV1+oNGFXVjF2nC7GzsxiHCsob7IHBRC9MWP7ROCekfEYlRTaaI/M2ZJqbD91AYPjgtEvKrDRfSw2GSfOVSI+1BeBPq5DTdVmK+atOobPd5xxbvM3aPHkdX0w48okl9qeKpMVB3JKYZUVRAYZERFgRA8/Q5OBSVEU7M8pxeId2dhx+gIuTw7F78YmY0CM67nQZLVh/ZFz+PFoISYOjGq0JkoNDCNERNTl5JXW4J9rjmF/TqmzVkVj73EZ2ycc069MRGRg63sUFEVBjb2AudokFtLbl12KbSfO45dTF1Bdry5nUM8gPDwmCTcNiYVGAjYeK8SXO7Ox5fh5Z11Rn8gATL0sFlMvi0WonwGbjxdiw9FCbMkoRHmtCHVj+4TjhkGiRuh0URX+978HkFkken3uHZWAo/nlzvqg3hH+ePTq3jhVVIlfTxfjUG5Zg2nlOo2EyEAjUqIC0T86EP2iAtEvOhDpuWVYvOMMDueVN/jcV6WE45GxSegZ4ov/7MrBsn25KK6qm34+bWgsXrx5kHO9ILUwjBARkVczW2XszS7BigN5WLb3rHN6eGSgERpJclnPJjU2CCfOVTqnZgNiLZ36ucFHr3GZYq7TSJAVBbICRAf54LU7h2BsnwjIsoJv9p7FP1Yfw4V6AcGhZ4gvAn10KKwwuQSIphh0Gtw0JAbXD4jCyvR8rD5U0GjNUFSQEZcnhWFlej5ssuLSJqtNxoGzZfjpRBF2nylGeIARIxNDMSqpB3pHBHTatacYRoiIiOxKqsz4cmc2Pv0lC4UVYp2YUH8D7kyLw72jEpBon5K99lABvj+Yj59PFsEmK+gbFYAJA6Jw3YBIDI3vgVPnK7E6vQCrD4lp5gBwy9BYvNRIL0RZjQVv/XgCv2ZewIDoIFyeHIbLk0IRH+rn3Mdik1FUaUJeaQ0yCiqRUVCOowUVOH6uAqH+BtwzMh53psW7zCDKKa7Gol+ysGRnNmqtMib0j8Q9o+JxdZ8I6LQa7MsuwZx6vTWjEkNxNL8cFU1cbTzU34ARvXpgxphEXNk73K3HnWGEiIjoImarjI3HzgGQcE3/iCYXnSupMqPWakNMsG+Tr5VVVAWLTUafqIa1LZ5Qa7HBYpMb1LAAoo7l1dXH8Nn2ujqWYF89xqSE4YrkMBRVmLAzqxj7c0qdvT3v3jccU4a4t9aEYYSIiMjL7c4SdSrD7Qv3Xbzysdkq41BeGXZlFuP2tDiEBzSc7dURDCNERESkqtaev7mWMBEREamKYYSIiIhUxTBCREREqmIYISIiIlUxjBAREZGqGEaIiIhIVQwjREREpCqGESIiIlIVwwgRERGpimGEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqnRqN6A1HBcWLi8vV7klRERE1FqO87bjPN6UbhFGKioqAADx8fEqt4SIiIjaqqKiAsHBwU0+LiktxZUuQJZl5OXlITAwEJIkue11y8vLER8fj5ycHAQFBbntdakhHmvP4vH2HB5rz+Gx9hx3HWtFUVBRUYHY2FhoNE1XhnSLnhGNRoO4uLhOe/2goCD+YHsIj7Vn8Xh7Do+15/BYe447jnVzPSIOLGAlIiIiVTGMEBERkaq8OowYjUb85S9/gdFoVLsplzwea8/i8fYcHmvP4bH2HE8f625RwEpERESXLq/uGSEiIiL1MYwQERGRqhhGiIiISFUMI0RERKQqrw4j//73v5GUlAQfHx+kpaVh27Ztajep25s3bx5GjhyJwMBAREZGYtq0acjIyHDZR1EU/PWvf0VsbCx8fX0xfvx4HD58WKUWXxrmzZsHSZLw1FNPObfxOLtXbm4uHnjgAYSFhcHPzw9Dhw7Fnj17nI/zeLuH1WrFH//4RyQlJcHX1xfJycl46aWXIMuycx8e6/bZunUrpk6ditjYWEiShG+//dbl8dYcV5PJhMcffxzh4eHw9/fHzTffjLNnz3a8cYqXWrJkiaLX65UPPvhAOXLkiPLkk08q/v7+ypkzZ9RuWrc2adIk5ZNPPlEOHTqk7N+/X5kyZYqSkJCgVFZWOvd59dVXlcDAQGXp0qVKenq6cvfddysxMTFKeXm5ii3vvnbu3KkkJiYqQ4YMUZ588knndh5n9ykuLlZ69eqlzJgxQ/n111+VzMxMZcOGDcrJkyed+/B4u8fLL7+shIWFKT/88IOSmZmpfP3110pAQIAyf/585z481u2zatUq5YUXXlCWLl2qAFCWL1/u8nhrjuvMmTOVnj17KuvXr1f27t2rXHPNNcpll12mWK3WDrXNa8PIqFGjlJkzZ7ps69+/v/KHP/xBpRZdmgoLCxUAypYtWxRFURRZlpXo6Gjl1Vdfde5TW1urBAcHKwsXLlSrmd1WRUWF0qdPH2X9+vXKuHHjnGGEx9m9nnvuOeWqq65q8nEeb/eZMmWK8vDDD7tsu+2225QHHnhAURQea3e5OIy05riWlpYqer1eWbJkiXOf3NxcRaPRKGvWrOlQe7xymMZsNmPPnj2YOHGiy/aJEyfil19+UalVl6aysjIAQGhoKAAgMzMTBQUFLsfeaDRi3LhxPPbtMGvWLEyZMgXXXXedy3YeZ/dasWIFRowYgTvvvBORkZEYNmwYPvjgA+fjPN7uc9VVV+HHH3/E8ePHAQAHDhzATz/9hMmTJwPgse4srTmue/bsgcVicdknNjYWgwYN6vCx7xYXynO3oqIi2Gw2REVFuWyPiopCQUGBSq269CiKgjlz5uCqq67CoEGDAMB5fBs79mfOnPF4G7uzJUuWYO/evdi1a1eDx3ic3ev06dNYsGAB5syZg+effx47d+7EE088AaPRiIceeojH242ee+45lJWVoX///tBqtbDZbHjllVdw7733AuDPdmdpzXEtKCiAwWBAjx49GuzT0XOnV4YRB0mSXO4ritJgG7Xf7NmzcfDgQfz0008NHuOx75icnBw8+eSTWLduHXx8fJrcj8fZPWRZxogRI/D3v/8dADBs2DAcPnwYCxYswEMPPeTcj8e74/7zn/9g8eLF+PLLL5Gamor9+/fjqaeeQmxsLKZPn+7cj8e6c7TnuLrj2HvlME14eDi0Wm2DJFdYWNggFVL7PP7441ixYgU2bdqEuLg45/bo6GgA4LHvoD179qCwsBBpaWnQ6XTQ6XTYsmUL3nrrLeh0Ouex5HF2j5iYGAwcONBl24ABA5CdnQ2AP9fu9Mwzz+APf/gD7rnnHgwePBgPPvggfv/732PevHkAeKw7S2uOa3R0NMxmM0pKSprcp728MowYDAakpaVh/fr1LtvXr1+PK6+8UqVWXRoURcHs2bOxbNkybNy4EUlJSS6PJyUlITo62uXYm81mbNmyhce+DSZMmID09HTs37/feRsxYgTuv/9+7N+/H8nJyTzObjRmzJgGU9SPHz+OXr16AeDPtTtVV1dDo3E9NWm1WufUXh7rztGa45qWlga9Xu+yT35+Pg4dOtTxY9+h8tduzDG196OPPlKOHDmiPPXUU4q/v7+SlZWldtO6tccee0wJDg5WNm/erOTn5ztv1dXVzn1effVVJTg4WFm2bJmSnp6u3HvvvZyW5wb1Z9MoCo+zO+3cuVPR6XTKK6+8opw4cUL54osvFD8/P2Xx4sXOfXi83WP69OlKz549nVN7ly1bpoSHhyvPPvuscx8e6/apqKhQ9u3bp+zbt08BoPzrX/9S9u3b51zSojXHdebMmUpcXJyyYcMGZe/evcq1117Lqb0d9e677yq9evVSDAaDMnz4cOf0U2o/AI3ePvnkE+c+siwrf/nLX5To6GjFaDQqV199tZKenq5eoy8RF4cRHmf3+v7775VBgwYpRqNR6d+/v/L++++7PM7j7R7l5eXKk08+qSQkJCg+Pj5KcnKy8sILLygmk8m5D491+2zatKnR38/Tp09XFKV1x7WmpkaZPXu2Ehoaqvj6+io33XSTkp2d3eG2SYqiKB3rWyEiIiJqP6+sGSEiIqKug2GEiIiIVMUwQkRERKpiGCEiIiJVMYwQERGRqhhGiIiISFUMI0RERKQqhhEiIiJSFcMIERERqYphhIiIiFTFMEJERESqYhghIiIiVf1/V65wGSegnLgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 2362561\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sequion hath teach my humours. My name:\n",
      "I do require it! Julieted, was, I warrant\n",
      "If find your fathe\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "LADY GREY:\n",
      "Then, to thy charm! O joy! or else you;\n",
      "This mighty fly me to me in such men with me,\n",
      "\n",
      "\n",
      "\n",
      "AUFIDIUS:\n",
      "I pray him too royal spoken;\n",
      "But when his good prithee, let's no prince, the hand,\n",
      "For th\n",
      "\n",
      "Since Gentleman:\n",
      "What, thou wilt once that 'last spoke a tree,--as there\n",
      "before before you their lov\n",
      "\n",
      "For with thy brother house's guess?\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "I prithee, he is ended me in that Edward.\n",
      "\n",
      "NOM\n",
      "\n",
      "To the Capulet!\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "All what changed, but you have there's watch:\n",
      "And these my troth w\n",
      "\n",
      "KING RICHARD II:\n",
      "How you things, soft is with this return:\n",
      "I know you not at began. For bloody distr\n",
      "\n",
      "LADY CAPULET:\n",
      "It hast thou wilt beguile.\n",
      "\n",
      "Nurse:\n",
      "A scold of summer hour's behavior, said\n",
      "Is dissed o\n",
      "\n",
      "I have thought thou wilt be put of slain.\n",
      "Why she will not\n",
      "Shall this good leave, he should not.\n",
      "\n",
      "LA\n",
      "\n",
      "Not so? You must not for a friend desperate friends\n",
      "before the take of forty, the thought of my back\n",
      "\n",
      "BRUTUS:\n",
      "He seeks you now,\n",
      "Unless you will be repress my bed,\n",
      "And deep in me, were you Aufidius;\n",
      "The \n",
      "\n",
      "I honour in curse, no news him?\n",
      "\n",
      "LORD FITZWATER:\n",
      "Are wherein me. And stand him, then you would\n",
      "What \n",
      "\n",
      "A grant with you.\n",
      "\n",
      "BRUTUS:\n",
      "I have been the executioner, welcome;\n",
      "The 'long is both to regal the pati\n",
      "\n",
      "And his poison and breath with Edward\n",
      "To blushing stains? Let it go with a stain\n",
      "Lend all your lower\n",
      "\n",
      "Judged the stiffy, then affection,\n",
      "Have given you injurious relight,\n",
      "But fear our mother, if you'll \n",
      "\n",
      "The fundation of credits.\n",
      "Why art thou, ho!\n",
      "\n",
      "YORK:\n",
      "Harry Rome is, and the great Edward,\n",
      "For make the\n",
      "\n",
      "Stay penvy upon a pleader, I was by,\n",
      "The Duke of York lives.\n",
      "\n",
      "First Murderer:\n",
      "O, that let me hath a \n",
      "\n",
      "\n",
      "Terminal:\n",
      "Why, my quick, he methinks!\n",
      "\n",
      "AUFIDIUS:\n",
      "O Judes much\n",
      "sufficient a fittle, forgiveness, kin\n",
      "\n",
      "YORK:\n",
      "No matter, stay thou; but I do. Lady,\n",
      "Or cur thy father sovereign in heavens,\n",
      "Thought you have\n",
      "\n",
      "How makes his government for your sight.\n",
      "\n",
      "KING EDWARD IV:\n",
      "I virtue Bolingbroke prophesy your father,\n",
      "\n",
      "To us' and her name come him tongues-laid,\n",
      "I will deserve.\n",
      "\n",
      "First Murderer:\n",
      "Here mercy farewell;\n",
      "For\n",
      "\n",
      "Were a foer whither man that crown'd ballad,\n",
      "Than by slaughter to sleep upon for mine.\n",
      "Most lawful t\n",
      "\n",
      "Hath pay'd us?\n",
      "\n",
      "SICINIUS:\n",
      "May not he?\n",
      "\n",
      "COMINIUS:\n",
      "Where he shall be good deny me.\n",
      "\n",
      "MENENIUS:\n",
      "Do you u\n",
      "\n",
      "All madam, go.\n",
      "\n",
      "VOLUMNIA:\n",
      "He'll not have been bloods: the foes in all\n",
      "from even hath word to the fai\n",
      "\n",
      "Doth death be\n",
      "The gentle of my fearful lo, my sons--\n",
      "Bleaded me early tongues' like a bad slander!\n",
      "T\n",
      "\n",
      "\n",
      "Then.\n",
      "\n",
      "MENENIUS:\n",
      "Why, very sir; the bestow the clouds of them?\n",
      "\n",
      "BRUTUS:\n",
      "Very kiss'd which they shal\n",
      "\n",
      "hall still no more to me. For you.\n",
      "\n",
      "LADY CAPULET:\n",
      "I can you pluck-horse take it; her here come,\n",
      "Beca\n",
      "\n",
      "\n",
      "VALERIA:\n",
      "O father, my lord, then, 'tis so; fetch, I should stay.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "If I have my swe\n",
      "\n",
      "tormise me to revenge me to the prince are;\n",
      "And have no secord their sight and savage!\n",
      "\n",
      "QUEEN ELIZAB\n",
      "\n",
      "GLOUCESTER:\n",
      "Do yet the other in this mild and here,\n",
      "Since I hear the king, all helds upon't.\n",
      "\n",
      "BUCKIN\n",
      "\n",
      "Marry, put fresh times--\n",
      "\n",
      "First Servingman:\n",
      "Fear first is sick.\n",
      "\n",
      "VOLUMNIA:\n",
      "A turns to my souls!\n",
      "\n",
      "COR\n",
      "\n",
      "Of Citizens.\n",
      "\n",
      "ABHAM:\n",
      "Not might, if it not to me, farewell!\n",
      "\n",
      "HENRY BOLINGBROKE:\n",
      "What children, out a \n",
      "CPU times: user 8.55 s, sys: 0 ns, total: 8.55 s\n",
      "Wall time: 864 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
