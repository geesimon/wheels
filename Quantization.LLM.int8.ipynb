{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -q\n",
    "%pip install bitsandbytes -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def absmax_quantize_i8(X: torch.Tensor):\n",
    "    absmax = torch.max(torch.abs(X))\n",
    "    X_i8 = ((X * 127) / absmax).to(torch.int8)\n",
    "    return X_i8, X_i8.to(torch.float32) * absmax / 127\n",
    "\n",
    "def zeropoint_quantize_i8(X: torch.Tensor):\n",
    "    r = torch.max(X) - torch.min(X)\n",
    "    r = 1 if r == 0 else r\n",
    "    scale = 255 / r\n",
    "\n",
    "    zeropoint = (-scale * torch.min(X) - 128)\n",
    "    X_i8 =  (X * scale + zeropoint).round().to(torch.int8)\n",
    "    \n",
    "    return X_i8, (X_i8 - zeropoint) / scale\n",
    "\n",
    "# def absmax_quantize(X):\n",
    "#     # Calculate scale\n",
    "#     scale = 127 / torch.max(torch.abs(X))\n",
    "\n",
    "#     # Quantize\n",
    "#     X_quant = (scale * X).round()\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = X_quant / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "# def zeropoint_quantize(X):\n",
    "#     # Calculate value range (denominator)\n",
    "#     x_range = torch.max(X) - torch.min(X)\n",
    "#     x_range = 1 if x_range == 0 else x_range\n",
    "\n",
    "#     # Calculate scale\n",
    "#     scale = 255 / x_range\n",
    "\n",
    "#     # Shift by zero-point\n",
    "#     zeropoint = (-scale * torch.min(X) - 128).round()\n",
    "#     # Scale and round the inputs\n",
    "#     X_quant = torch.clip((X * scale + zeropoint).round(), -128, 127)\n",
    "\n",
    "#     # Dequantize\n",
    "#     X_dequant = (X_quant - zeropoint) / scale\n",
    "\n",
    "#     return X_quant.to(torch.int8), X_dequant\n",
    "\n",
    "def zp_mul(A, B):\n",
    "    # Calculate value range (denominator)\n",
    "    a_range = torch.max(A) - torch.min(A)\n",
    "    b_range = torch.max(B) - torch.min(B)\n",
    "    a_range = 1 if a_range == 0 else a_range\n",
    "    b_range = 1 if b_range == 0 else b_range\n",
    "    \n",
    "    # Calculate scale\n",
    "    a_scale = 255 / a_range\n",
    "    b_scale = 255 / b_range\n",
    "    c_scale = a_scale * b_scale\n",
    "\n",
    "    # Shift by zero-point\n",
    "    a_zp = (-a_scale * torch.min(A) - 128).round()\n",
    "    b_zp = (-b_scale * torch.min(B) - 128).round()\n",
    "    c_zp = a_zp * b_zp\n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    A_quant = torch.clip((A * a_scale + a_zp).round(), -128, 127).to(torch.int8)\n",
    "    B_quant = torch.clip((B * b_scale + b_zp).round(), -128, 127).to(torch.int8)    \n",
    "    \n",
    "    # print(f'c_scale:{c_scale}, c_zp:{c_zp}')\n",
    "    # print(f'A_quant:{A_quant}, B_quant:{B_quant}')\n",
    "    # Multiply\n",
    "    C_quant = (A_quant.to(torch.int16) * B_quant.to(torch.int16)) + c_zp #- A_quant * b_zp.to(torch.float32) - B_quant * a_zp.to(torch.float32) \n",
    "    # print(f'c_quant:{C_quant}')\n",
    "    C = C_quant / c_scale\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_vector_abs_i8(A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n",
    "    A_scale = 127 / torch.max(torch.abs(A), dim=1).values\n",
    "    B_scale = 127 / torch.max(torch.abs(B), dim=0).values\n",
    "    C_scale = torch.matmul(A_scale.unsqueeze(1), B_scale.unsqueeze(0))\n",
    "\n",
    "    A_i8 = torch.clip((A  * A_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    B_i8 = torch.clip((B  * B_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    return torch.matmul(A_i8.to(torch.int32), B_i8.to(torch.int32)) / C_scale\n",
    "\n",
    "def LLM_matmul_abs_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "    W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "    O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)\n",
    "\n",
    "def LLM_matmul_zp_i8(X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "    X_col_filter = torch.max(torch.abs(X), dim = 0).values > alpha\n",
    "    X1 = X[:, X_col_filter]\n",
    "    W1 = W[X_col_filter, :]\n",
    "    X2 = X[:, ~X_col_filter]\n",
    "    W2 = W[~X_col_filter, :]\n",
    "    \n",
    "    O1 = torch.matmul(X1, W1)\n",
    "    print(f'Reserved {(X1.shape[1] / X.shape[1] * 100):.1f}%')\n",
    "    # Calculate value range (denominator)\n",
    "    X2_range = torch.max(X2, dim=1).values - torch.min(X2, dim=1).values\n",
    "    W2_range = torch.max(W2, dim=0).values - torch.min(W2, dim=0).values\n",
    "    \n",
    "    # Calculate scale\n",
    "    X2_scale = 255 / X2_range\n",
    "    W2_scale = 255 / W2_range\n",
    "    O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "    # Shift by zero-point\n",
    "    X2_zp = (-X2_scale * torch.min(X2, dim = 1).values - 128).round()\n",
    "    W2_zp = (-W2_scale * torch.min(W2, dim = 0).values - 128).round()\n",
    "    O_zp = torch.matmul(X2_zp.unsqueeze(1), W2_zp.unsqueeze(0))    \n",
    "    \n",
    "    # Scale and round the inputs\n",
    "    X2_quant = torch.clip((X2 * X2_scale.unsqueeze(1) + X2_zp.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "    W2_quant = torch.clip((W2 * W2_scale.unsqueeze(0) + W2_zp.unsqueeze(0)).round(), -128, 127).to(torch.int8)   \n",
    "    O2_quant = (X2_quant.to(torch.int32) @ W2_quant.to(torch.int32)) \\\n",
    "                - X2_quant.to(X2) @ W2_zp.unsqueeze(0).expand(X2.shape[1], -1) \\\n",
    "                - X2_zp.unsqueeze(1).expand(-1, W2.shape[0]) @ W2_quant.to(W2) \\\n",
    "                + O_zp * X2.shape[1]\n",
    "    O2 = O2_quant / O2_scale\n",
    "    \n",
    "    return O1 + O2.to(O1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reserved 10.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM.int8() absmax -> Acc: 66560.0, Avg: 0.265625\n",
      "Reserved 10.0%\n",
      "LLM.int8() zero-point -> Acc: 68608.0, Avg: 0.275390625\n",
      "tensor(68608., dtype=torch.bfloat16) tensor(0.2754, dtype=torch.bfloat16)\n",
      "int8 abs -> Acc: 70144.0, Avg: 0.28125\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(500, 1000, dtype=torch.bfloat16)\n",
    "W = torch.randn(1000, 500, dtype=torch.bfloat16)\n",
    "X[0, 0: X.shape[1] // 10] = 6\n",
    "\n",
    "error = torch.abs(LLM_matmul_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() absmax -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "\n",
    "error = torch.abs(LLM_matmul_zp_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'LLM.int8() zero-point -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n",
    "print(torch.sum(error), torch.sum(error) / (X.shape[0] * W.shape[1]))\n",
    "\n",
    "error = torch.abs(matmul_vector_abs_i8(X, W) - X @ W)\n",
    "filter = error > 1\n",
    "print(f'int8 abs -> Acc: {torch.sum(error)}, Avg: {torch.sum(error) / (X.shape[0] * W.shape[1])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Quantization Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 510,342,192 bytes\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Set device to CPU for now\n",
    "device = 'cpu'\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_id = 'gpt2'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Print model size\n",
    "print(f\"Model size: {model.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      "tensor([[-0.4738, -0.2614, -0.0978,  ...,  0.0513, -0.0584,  0.0250],\n",
      "        [ 0.0874,  0.1473,  0.2387,  ..., -0.0525, -0.0113, -0.0156],\n",
      "        [ 0.0039,  0.0695,  0.3668,  ...,  0.1143,  0.0363, -0.0318],\n",
      "        ...,\n",
      "        [-0.2592, -0.0164,  0.1991,  ...,  0.0095, -0.0516,  0.0319],\n",
      "        [ 0.1517,  0.2170,  0.1043,  ...,  0.0293, -0.0429, -0.0475],\n",
      "        [-0.4100, -0.1924, -0.2400,  ..., -0.0046,  0.0070,  0.0198]])\n",
      "\n",
      "Absmax quantized weights:\n",
      "\n",
      " tensor([[-0.4702, -0.2463, -0.0896,  ...,  0.0448, -0.0448,  0.0224],\n",
      "        [ 0.0672,  0.1343,  0.2239,  ..., -0.0448,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0672,  0.3583,  ...,  0.1120,  0.0224, -0.0224],\n",
      "        ...,\n",
      "        [-0.2463,  0.0000,  0.1791,  ...,  0.0000, -0.0448,  0.0224],\n",
      "        [ 0.1343,  0.2015,  0.0896,  ...,  0.0224, -0.0224, -0.0448],\n",
      "        [-0.4030, -0.1791, -0.2239,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "\n",
      "Zero-point quantized weights:\n",
      "\n",
      " tensor([[-0.4774, -0.2562, -0.1014,  ...,  0.0534, -0.0572,  0.0313],\n",
      "        [ 0.0976,  0.1419,  0.2303,  ..., -0.0572, -0.0129, -0.0129],\n",
      "        [ 0.0092,  0.0755,  0.3630,  ...,  0.1197,  0.0313, -0.0351],\n",
      "        ...,\n",
      "        [-0.2562, -0.0129,  0.2082,  ...,  0.0092, -0.0572,  0.0313],\n",
      "        [ 0.1419,  0.2082,  0.0976,  ...,  0.0313, -0.0351, -0.0572],\n",
      "        [-0.4110, -0.1899, -0.2341,  ..., -0.0129,  0.0092,  0.0092]])\n"
     ]
    }
   ],
   "source": [
    "# Extract weights of the first layer\n",
    "weights = model.transformer.h[0].attn.c_attn.weight.data\n",
    "print(\"Original weights:\")\n",
    "print(weights)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_abs_quant, weights_abs_dequant = absmax_quantize_i8(weights)\n",
    "print(\"\\nAbsmax quantized weights:\")\n",
    "print('\\n', weights_abs_dequant)\n",
    "\n",
    "# Quantize layer using absmax quantization\n",
    "weights_zp_quant, weights_zp_dequant = zeropoint_quantize_i8(weights)\n",
    "print(\"\\nZero-point quantized weights:\")\n",
    "print('\\n', weights_zp_dequant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "# Store original weights\n",
    "weights = [param.data.clone() for param in model.parameters()]\n",
    "\n",
    "# Create model to quantize\n",
    "model_abs = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_abs = []\n",
    "for param in model_abs.parameters():\n",
    "    _, dequantized = absmax_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_abs.append(dequantized)\n",
    "\n",
    "# Create model to quantize\n",
    "model_zp = deepcopy(model)\n",
    "\n",
    "# Quantize all model weights\n",
    "weights_zp = []\n",
    "for param in model_zp.parameters():\n",
    "    _, dequantized = zeropoint_quantize_i8(param.data)\n",
    "    param.data = dequantized\n",
    "    weights_zp.append(dequantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream, but I'm scared I might fail.\"\n",
      "\n",
      "She did not say what she was scared of, but she certainly wasn't scared of what she will do if she tries any of these things. Even if it is just to\n",
      "--------------------------------------------------\n",
      "Absmax model:\n",
      "I have a dream' to give a the. is the o in it h s to be\n",
      " is to s i g to s s o m th t o p th o sh ti t t t th s s o nt t th th\n",
      "--------------------------------------------------\n",
      "Zeropoint model:\n",
      "I have a dream of my life now and I wish we get an exam tomorrow night, I can tell you I am gonna make a career, that the dreams of my husband would be on the day that morning, I am gonna have a hard time\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, input_text, max_length=50):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)\n",
    "    output = model.generate(inputs=input_ids,\n",
    "                            max_length=max_length,\n",
    "                            do_sample=True,\n",
    "                            top_k=30,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                            attention_mask=input_ids.new_ones(input_ids.shape))\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Generate text with original and quantized models\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "absmax_text   = generate_text(model_abs, \"I have a dream\")\n",
    "zp_text       = generate_text(model_zp, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Absmax model:\\n{absmax_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Zeropoint model:\\n{zp_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  11.11\n",
      "Absmax perplexity:    55.95\n",
      "Zeropoint perplexity: 28.01\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity(model, text):\n",
    "    # Encode the text\n",
    "    encodings = tokenizer(text, return_tensors='pt').to(device)\n",
    "\n",
    "    # Define input_ids and target_ids\n",
    "    input_ids = encodings.input_ids\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "    # Loss calculation    \n",
    "    neg_log_likelihood = outputs.loss\n",
    "\n",
    "    # Perplexity calculation\n",
    "    ppl = torch.exp(neg_log_likelihood)\n",
    "\n",
    "    return ppl\n",
    "\n",
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_abs = calculate_perplexity(model_abs, absmax_text)\n",
    "ppl_zp  = calculate_perplexity(model_zp, zp_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"Absmax perplexity:    {ppl_abs.item():.2f}\")\n",
    "print(f\"Zeropoint perplexity: {ppl_zp.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Int8 Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LinearInt8(nn.Module):\n",
    "    def __init__(self, weight: torch.tensor, bias = None):\n",
    "        super().__init__()\n",
    "        self.zp_quantize_weight_i8(weight)\n",
    "\n",
    "        self.bias = None if bias is None else bias.to(torch.bfloat16)\n",
    "\n",
    "    def zp_quantize_weight_i8(self, weight: torch.Tensor):\n",
    "        r = torch.max(weight, dim=0).values - torch.min(weight, dim=0).values\n",
    "        r = torch.where(r == 0, torch.tensor(1), r)        \n",
    "        \n",
    "        scale = 255 / r\n",
    "        zeropoint = (-scale * torch.min(weight, dim=0).values - 128)        \n",
    "\n",
    "        self.weight_i8 =  (weight * scale.unsqueeze(0) + zeropoint.unsqueeze(0)).round().to(torch.int8)\n",
    "\n",
    "        self.scale = scale.to(torch.bfloat16)\n",
    "        self.zeropoint = zeropoint.to(torch.bfloat16)\n",
    "    \n",
    "    def zp_dequantize_weight_i8(self) -> torch.Tensor:\n",
    "        return (self.weight_i8 - self.zeropoint.unsqueeze(0)) / self.scale.unsqueeze(0)\n",
    "\n",
    "    def LLM_matmul_abs_i8(self, X: torch.Tensor, W: torch.Tensor, alpha = 5) -> torch.Tensor:\n",
    "        X_bf16 = X.to(torch.bfloat16)\n",
    "\n",
    "        X_col_filter = torch.max(torch.abs(X_bf16), dim = -2).values > alpha\n",
    "        X1 = X_bf16[:, X_col_filter]\n",
    "        W1 = W[X_col_filter, :]\n",
    "        X2 = X_bf16[:, ~X_col_filter]\n",
    "        W2 = W[~X_col_filter, :]\n",
    "        \n",
    "        O1 = torch.matmul(X1, W1)\n",
    "        # print(f'Reserved {(X1.shape[1] / X_bf16.shape[1] * 100):.1f}%')\n",
    "        X2_scale = 127 / torch.max(torch.abs(X2), dim=1).values\n",
    "        W2_scale = 127 / torch.max(torch.abs(W2), dim=0).values\n",
    "        O2_scale = torch.matmul(X2_scale.unsqueeze(1), W2_scale.unsqueeze(0))\n",
    "\n",
    "        X2_i8 = torch.clip((X2  * X2_scale.unsqueeze(1)).round(), -128, 127).to(torch.int8)\n",
    "        W2_i8 = torch.clip((W2  * W2_scale.unsqueeze(0)).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "        O2 = torch.matmul(X2_i8.to(torch.int32), W2_i8.to(torch.int32)) / O2_scale\n",
    "        \n",
    "        return (O1 + O2.to(O1)).to(X)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        x_flattened = X.view(-1, X.shape[-1])\n",
    "        ret = self.LLM_matmul_abs_i8(x_flattened, self.zp_dequantize_weight_i8())\n",
    "        if self.bias is not None:\n",
    "            ret += self.bias\n",
    "\n",
    "        return ret.view(X.shape[:-2] + (-1, ret.shape[-1]))\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"in_features={self.weight_i8.shape[0]}, out_features={self.weight_i8.shape[1]}, bias={'true' if self.bias is not None else 'false'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight avg err: 0.0013\n",
      "Activation avg err: 0.0602\n"
     ]
    }
   ],
   "source": [
    "W = model.transformer.h[0].attn.c_attn.weight\n",
    "bias = model.transformer.h[0].attn.c_attn.bias\n",
    "X = torch.randn(2, model.transformer.h[0].attn.c_attn.weight.shape[0])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "print(f'Weight avg err: {torch.sum(torch.abs(l_i8.zp_dequantize_weight_i8() - W))/W.numel():.4f}')\n",
    "print(f'Activation avg err: { torch.sum(torch.abs(X @ W + bias - l_i8(X))) / (X.shape[0] * W.shape[1]):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0578, -0.0065,  0.0057],\n",
       "          [-0.0107, -0.0044, -0.0088],\n",
       "          [ 0.0092, -0.0091,  0.0049]],\n",
       "\n",
       "         [[-0.0385, -0.0160, -0.0037],\n",
       "          [ 0.1062, -0.0068,  0.0057],\n",
       "          [-0.0249,  0.0064, -0.0040]]],\n",
       "\n",
       "\n",
       "        [[[-0.0059, -0.0051, -0.0067],\n",
       "          [ 0.0156,  0.0005, -0.0026],\n",
       "          [-0.0231,  0.0109, -0.0042]],\n",
       "\n",
       "         [[ 0.0277, -0.0095, -0.0014],\n",
       "          [ 0.0626, -0.0264,  0.0023],\n",
       "          [ 0.0294,  0.0059,  0.0100]]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn(5, 3)\n",
    "X = torch.randn(2, 2, 3, 5)\n",
    "bias = torch.randn(W.shape[-1])\n",
    "l_i8 = LinearInt8(W, bias)\n",
    "l_i8(X) -  (X @ W + bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Substitute Vanilla Linear Module with LinearInt8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): LinearInt8(in_features=768, out_features=2304, bias=true)\n",
      "          (c_proj): LinearInt8(in_features=768, out_features=768, bias=true)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): LinearInt8(in_features=768, out_features=3072, bias=true)\n",
      "          (c_proj): LinearInt8(in_features=3072, out_features=768, bias=true)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "--------------------------------------------------\n",
      "Original Model size: 510,342,192 bytes\n",
      "LLM.int8 Model size: 170,271,792 bytes\n"
     ]
    }
   ],
   "source": [
    "model_int8 = deepcopy(model)\n",
    "\n",
    "#Replace attention and ff layers\n",
    "for h in model_int8.transformer.h:\n",
    "    h.attn.c_attn = LinearInt8(h.attn.c_attn.weight, h.attn.c_attn.bias)\n",
    "    h.attn.c_proj = LinearInt8(h.attn.c_proj.weight, h.attn.c_proj.bias)\n",
    "    h.mlp.c_fc = LinearInt8(h.mlp.c_fc.weight, h.mlp.c_fc.bias)\n",
    "    h.mlp.c_proj = LinearInt8(h.mlp.c_proj.weight, h.mlp.c_proj.bias)\n",
    "\n",
    "print(model_int8)\n",
    "# Print model size\n",
    "print(\"-\" * 50)\n",
    "print(f\"Original Model size: {model.get_memory_footprint():,} bytes\")\n",
    "print(f\"LLM.int8 Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model:\n",
      "I have a dream,\" he said. \"You have a dream. The dream of a great guy. You dream that you'll be one day an NFL superstar. You dream that you'll be one day a quarterback in the NFL. You dream that\n",
      "--------------------------------------------------\n",
      "LLM.int8 model:\n",
      "I have a dream,\" he said. \"You have a lot of things you've never been in before,\" but they didn't. He kept talking to his friends, but no one was doing it for him.\n",
      "\n",
      "\"No one did,\"\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "original_text = generate_text(model, \"I have a dream\")\n",
    "torch.manual_seed(0)\n",
    "int8_text = generate_text(model_int8, \"I have a dream\")\n",
    "\n",
    "print(f\"Original model:\\n{original_text}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"LLM.int8 model:\\n{int8_text}\")\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original perplexity:  7.59\n",
      "LLM.int8 perplexity:    13.02\n"
     ]
    }
   ],
   "source": [
    "ppl     = calculate_perplexity(model, original_text)\n",
    "ppl_int8 = calculate_perplexity(model_int8, int8_text)\n",
    "\n",
    "print(f\"Original perplexity:  {ppl.item():.2f}\")\n",
    "print(f\"LLM.int8 perplexity:    {ppl_int8.item():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model_int8 = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "#                                              device_map='auto',\n",
    "#                                              load_in_8bit=True,\n",
    "#                                              )\n",
    "# print(f\"Model size: {model_int8.get_memory_footprint():,} bytes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wheels",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
