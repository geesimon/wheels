{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=5000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        # shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]]\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Networks with SwiGLU\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2002.05202.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class FFN_SwiGLU(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            dim: int,\n",
    "            hidden_dim: int,\n",
    "            multiple_of: int,\n",
    "            ffn_dim_multiplier: Optional[float],\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension of the feedforward layer.\n",
    "            multiple_of (int): Value to ensure hidden dimension is a multiple of this value.\n",
    "            ffn_dim_multiplier (float, optional): Custom multiplier for hidden dimension. Defaults to None.\n",
    "\n",
    "        Attributes:\n",
    "            w1 (ColumnParallelLinear): Linear transformation for the first layer.\n",
    "            w2 (RowParallelLinear): Linear transformation for the second layer.\n",
    "            w3 (ColumnParallelLinear): Linear transformation for the third layer.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        hidden_dim = int(2 * hidden_dim / 3)\n",
    "        # custom dim factor multiplier\n",
    "        if ffn_dim_multiplier is not None:\n",
    "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
    "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "\n",
    "        self.w = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.v = nn.Linear(dim, hidden_dim, bias=False)\n",
    "        self.w_2 = nn.Linear(hidden_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(F.silu(self.w(x)) * self.v(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim, requires_grad=False)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k, requires_grad=False)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)        \n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     dropout_p = dropout_p, \n",
    "                                                     is_causal = is_causal)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        h_heads = [head(x, start_pos) for head in self.heads]\n",
    "        h = torch.cat(h_heads, dim=-1)\n",
    "        h = self.linear(h)\n",
    "        h = self.dropout(h)\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class Llama2Block(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.attention_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_norm = RMSNorm(config.dim, eps = config.norm_eps)\n",
    "        self.ffn_swiglu = FFN_SwiGLU(config.dim, config.dim * 4, config.multiple_of, config.ffn_dim_multiplier)\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = x + self.attention(self.attention_norm(x), start_pos)\n",
    "        out = x + self.ffn_swiglu(self.ffn_norm(x))\n",
    "\n",
    "        return out\n",
    "\n",
    "class Llama2(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", Llama2Block(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "        self.norm = RMSNorm(config.dim)\n",
    "        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        h = self.embeddings(idx)\n",
    "\n",
    "        for block in self.llama_blocks:\n",
    "            h = block(h, start_pos)\n",
    "\n",
    "        h = self.norm(h)\n",
    "        logits = self.output(h)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama2):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama2, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 935296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 3.964 | Time 0.247 | ETA in seconds 12.371\n",
      "Epoch 100 | val loss 2.188 | Time 11.544 | ETA in seconds 565.645\n",
      "Epoch 200 | val loss 1.955 | Time 11.373 | ETA in seconds 545.888\n",
      "Epoch 300 | val loss 1.871 | Time 13.505 | ETA in seconds 634.756\n",
      "Epoch 400 | val loss 1.792 | Time 14.480 | ETA in seconds 666.089\n",
      "Epoch 500 | val loss 1.715 | Time 15.143 | ETA in seconds 681.449\n",
      "Epoch 600 | val loss 1.680 | Time 15.642 | ETA in seconds 688.253\n",
      "Epoch 700 | val loss 1.658 | Time 15.602 | ETA in seconds 670.907\n",
      "Epoch 800 | val loss 1.658 | Time 16.059 | ETA in seconds 674.458\n",
      "Epoch 900 | val loss 1.623 | Time 15.926 | ETA in seconds 652.951\n",
      "Epoch 1000 | val loss 1.627 | Time 15.987 | ETA in seconds 639.468\n",
      "Epoch 1100 | val loss 1.597 | Time 16.139 | ETA in seconds 629.427\n",
      "Epoch 1200 | val loss 1.629 | Time 15.082 | ETA in seconds 573.112\n",
      "Epoch 1300 | val loss 1.603 | Time 14.685 | ETA in seconds 543.357\n",
      "Epoch 1400 | val loss 1.585 | Time 16.344 | ETA in seconds 588.391\n",
      "Epoch 1500 | val loss 1.572 | Time 15.513 | ETA in seconds 542.950\n",
      "Epoch 1600 | val loss 1.567 | Time 16.308 | ETA in seconds 554.470\n",
      "Epoch 1700 | val loss 1.560 | Time 15.739 | ETA in seconds 519.373\n",
      "Epoch 1800 | val loss 1.560 | Time 16.029 | ETA in seconds 512.929\n",
      "Epoch 1900 | val loss 1.560 | Time 15.738 | ETA in seconds 487.879\n",
      "Epoch 2000 | val loss 1.541 | Time 15.136 | ETA in seconds 454.085\n",
      "Epoch 2100 | val loss 1.544 | Time 15.207 | ETA in seconds 440.992\n",
      "Epoch 2200 | val loss 1.586 | Time 16.342 | ETA in seconds 457.574\n",
      "Epoch 2300 | val loss 1.545 | Time 17.505 | ETA in seconds 472.628\n",
      "Epoch 2400 | val loss 1.531 | Time 17.421 | ETA in seconds 452.934\n",
      "Epoch 2500 | val loss 1.543 | Time 15.523 | ETA in seconds 388.072\n",
      "Epoch 2600 | val loss 1.533 | Time 15.612 | ETA in seconds 374.690\n",
      "Epoch 2700 | val loss 1.517 | Time 15.677 | ETA in seconds 360.563\n",
      "Epoch 2800 | val loss 1.533 | Time 15.837 | ETA in seconds 348.420\n",
      "Epoch 2900 | val loss 1.524 | Time 16.685 | ETA in seconds 350.381\n",
      "Epoch 3000 | val loss 1.533 | Time 15.248 | ETA in seconds 304.953\n",
      "Epoch 3100 | val loss 1.518 | Time 15.633 | ETA in seconds 297.019\n",
      "Epoch 3200 | val loss 1.505 | Time 16.175 | ETA in seconds 291.154\n",
      "Epoch 3300 | val loss 1.530 | Time 15.415 | ETA in seconds 262.051\n",
      "Epoch 3400 | val loss 1.533 | Time 15.315 | ETA in seconds 245.037\n",
      "Epoch 3500 | val loss 1.508 | Time 14.843 | ETA in seconds 222.652\n",
      "Epoch 3600 | val loss 1.524 | Time 16.056 | ETA in seconds 224.786\n",
      "Epoch 3700 | val loss 1.501 | Time 14.674 | ETA in seconds 190.757\n",
      "Epoch 3800 | val loss 1.505 | Time 14.561 | ETA in seconds 174.729\n",
      "Epoch 3900 | val loss 1.509 | Time 15.308 | ETA in seconds 168.384\n",
      "Epoch 4000 | val loss 1.553 | Time 15.856 | ETA in seconds 158.556\n",
      "Epoch 4100 | val loss 1.497 | Time 15.314 | ETA in seconds 137.827\n",
      "Epoch 4200 | val loss 1.523 | Time 15.192 | ETA in seconds 121.534\n",
      "Epoch 4300 | val loss 1.499 | Time 15.288 | ETA in seconds 107.017\n",
      "Epoch 4400 | val loss 1.543 | Time 14.970 | ETA in seconds 89.822\n",
      "Epoch 4500 | val loss 1.490 | Time 15.747 | ETA in seconds 78.737\n",
      "Epoch 4600 | val loss 1.531 | Time 15.227 | ETA in seconds 60.907\n",
      "Epoch 4700 | val loss 1.513 | Time 15.045 | ETA in seconds 45.135\n",
      "Epoch 4800 | val loss 1.528 | Time 16.096 | ETA in seconds 32.192\n",
      "Epoch 4900 | val loss 1.502 | Time 15.533 | ETA in seconds 15.533\n",
      "validation loss:  1.5020257234573364\n",
      "CPU times: user 2h 16min 42s, sys: 11.4 s, total: 2h 16min 54s\n",
      "Wall time: 13min 41s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama2(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama2_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama2, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 935296\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama2(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "weeping branch; the earth on, may may;\n",
      "And in thy sons command faith hath doubt a truth\n",
      "Becomest\n",
      "In\n",
      "\n",
      "Say, fearful honour, from then you not a last?\n",
      "\n",
      "Nurse:\n",
      "No, weary could not tell it were it.\n",
      "\n",
      "ROMEO:\n",
      "\n",
      "\n",
      "speak that, peace of Paulina?\n",
      "\n",
      "JULIET:\n",
      "But my renownly banished'? with good old hands!\n",
      "Faith, For an\n",
      "\n",
      "The ousin! whose head, for success reely\n",
      "Than this harses, and yet the many of heart owes.\n",
      "\n",
      "CORIOLAN\n",
      "\n",
      "\n",
      "\n",
      "MENENIUS:\n",
      "Pray forward Henry; I know't.\n",
      "\n",
      "ANGELO:\n",
      "He hatreor the is like Fruth, whither came or wro\n",
      "\n",
      "And come to the Tower.\n",
      "\n",
      "ROMEO:\n",
      "Then, ere, I may stand foence:\n",
      "Prove I thee how cut me: come in here \n",
      "\n",
      "\n",
      "KING RICHARD II:\n",
      "Not take Hy sake, nor as common to hot\n",
      "In this more utterix, a tay beggar,\n",
      "I drivi\n",
      "\n",
      "I am Rost sun, one her bloody of the good\n",
      "Diress to be dispatch'd thee the measure's friends, and le\n",
      "\n",
      "This purse the wealth news while, what thou sir:\n",
      "Then, that tempter with the times were shall know't\n",
      "\n",
      "But I have a king! Coriolanus, what, lords,\n",
      "\n",
      "ANGELO:\n",
      "\n",
      "CLARENCE:\n",
      "Alas,\n",
      "Sweet time be the free.\n",
      "\n",
      "Lord \n",
      "\n",
      "And brief, though I say name, your head to-night,\n",
      "I said thy matterously an any will my wolves\n",
      "Above\n",
      "\n",
      "Though King Lewis is subject'st thou woest up for what sir, swearing blessed\n",
      "From my fathersy king, \n",
      "\n",
      "Consideration straight then his before call.\n",
      "\n",
      "EDWARD:\n",
      "I thoughts I make talongd of the chide in\n",
      "Bres\n",
      "\n",
      "KING EDWARD IV:\n",
      "Lost one were not doing I, your lord?\n",
      "Here lesser that sten and the purposess my gui\n",
      "\n",
      "Fod one Shall come to dead all mine earl the people, your\n",
      "age us plack thus her brought not him.\n",
      "\n",
      "GR\n",
      "\n",
      "They bed, my lord Angelo?\n",
      "\n",
      "LEONTES:\n",
      "The grace afeal!\n",
      "\n",
      "COMINIUS:\n",
      "We shall no lengthen be a presence,\n",
      "\n",
      "\n",
      "By your still.\n",
      "\n",
      "LEONTES:\n",
      "Come, come, there we as it again!\n",
      "\n",
      "CORIOLANUS:\n",
      "O, any know, genite for you.\n",
      "\n",
      "For it?\n",
      "\n",
      "PRINCE EDWARD:\n",
      "Here's the made?\n",
      "O enjoy excell'd, York's cheerful!\n",
      "On good lady!\n",
      "Say, Corio\n",
      "\n",
      "Under to hither: use we once our away, I can not talk,\n",
      "who comes fair man and the allion art\n",
      "Than sw\n",
      "\n",
      "Against their abundainst thou come fram'd could not\n",
      "This never shall sap finly redeeming father's th\n",
      "\n",
      "murder me, what who shall down and harders,\n",
      "That geeneral climate, butterom their woeving\n",
      "With merry\n",
      "\n",
      "With many shall be to be string loves to thee,\n",
      "Me thoughts their tired flattering the off a very\n",
      "pop\n",
      "\n",
      "Clarence haugh aside, as a but to any fay,\n",
      "siccel wanting in her charp-blind the private,\n",
      "noted of i\n",
      "\n",
      "may from you slew dwell undertake with that he did\n",
      "And like gladiers.\n",
      "Wherein I came to go one somet\n",
      "\n",
      "MENENIUS:\n",
      "Bewkeep us are hath sir,\n",
      "Or will repent mountain to see. Now, sir. When, that can peace,\n",
      "c\n",
      "\n",
      "LUCIO: thus do?\n",
      "\n",
      "POMPEY:\n",
      "How, aunt, my name, he hath heard Romes England's heavy\n",
      "look each of nature\n",
      "\n",
      "The poor did right: how look, and down let's\n",
      "Mistressing wanting person of the news honour.\n",
      "For my f\n",
      "\n",
      "\n",
      "PERDITA:\n",
      "Since deliger! But presentence or I\n",
      "\n",
      "Givest of much charity\n",
      "The peace a good time mutinoug\n",
      "\n",
      "To here icamentable.\n",
      "It for your commons had, or qualify, and,\n",
      "I mean, they set my house, a monweary\n",
      "\n",
      "CAPULET:\n",
      "I am to the glove leave follow the love.\n",
      "\n",
      "BUCKINGHAM:\n",
      "So brothers' suits on our war abunduu\n",
      "\n",
      "As since of thee he mischief upon my cheek--\n",
      "Since for Plantagenet\n",
      "And any number's pity brieful of \n",
      "\n",
      "Gently harber the world's kindly: and the enforced wintegate.\n",
      "\n",
      "FLORIZEL:\n",
      "And am infant, hear his win\n",
      "CPU times: user 2.4 s, sys: 0 ns, total: 2.4 s\n",
      "Wall time: 245 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
