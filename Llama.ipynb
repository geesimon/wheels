{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=128, epochs=5000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 8\n",
    "\n",
    "    epochs: int = 5_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f'Initialized RoPE with shape {self.freqs_cis.shape}')\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]].view(*shape)\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([256, 64])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)        \n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = 0.1            \n",
    "        else:            \n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "\n",
    "            dropout_p = 0\n",
    "        \n",
    "        if x.shape[-2] == 1:    # if only one token, then not causal            \n",
    "            is_causal = False\n",
    "        else:\n",
    "            is_causal = True\n",
    "\n",
    "        activations = F.scaled_dot_product_attention(q, k, v, \n",
    "                                                     dropout_p = dropout_p, \n",
    "                                                     is_causal = is_causal)\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        heads = [h(x, start_pos) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "\n",
    "        self.beta = torch.ones(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out\n",
    "\n",
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.attention(x, start_pos)\n",
    "\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.feedforward(x)\n",
    "        return x\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        \n",
    "        Attention.shared_rope = None    # clear the shared rope defined in Attention class\n",
    "\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "            nn.Linear(config.dim, config.vocab_size),\n",
    "        )\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        x = self.embeddings(idx)\n",
    "        for block in self.llama_blocks:\n",
    "            x = block(x, start_pos)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    is_training = model.training\n",
    "\n",
    "    if is_training:\n",
    "        model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    if is_training:\n",
    "        model.train()\n",
    "\n",
    "    return out\n",
    "\n",
    "def train(model: Llama, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())            \n",
    "\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 690113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | val loss 4.155 | Time 0.700 | ETA in seconds 35.011\n",
      "Epoch 100 | val loss 2.248 | Time 32.575 | ETA in seconds 1596.165\n",
      "Epoch 200 | val loss 2.028 | Time 32.818 | ETA in seconds 1575.262\n",
      "Epoch 300 | val loss 1.912 | Time 31.959 | ETA in seconds 1502.093\n",
      "Epoch 400 | val loss 1.857 | Time 24.634 | ETA in seconds 1133.156\n",
      "Epoch 500 | val loss 1.833 | Time 22.934 | ETA in seconds 1032.010\n",
      "Epoch 600 | val loss 1.787 | Time 31.415 | ETA in seconds 1382.247\n",
      "Epoch 700 | val loss 1.766 | Time 33.426 | ETA in seconds 1437.329\n",
      "Epoch 800 | val loss 1.769 | Time 30.794 | ETA in seconds 1293.347\n",
      "Epoch 900 | val loss 1.725 | Time 29.602 | ETA in seconds 1213.689\n",
      "Epoch 1000 | val loss 1.688 | Time 31.384 | ETA in seconds 1255.377\n",
      "Epoch 1100 | val loss 1.711 | Time 25.144 | ETA in seconds 980.629\n",
      "Epoch 1200 | val loss 1.716 | Time 30.633 | ETA in seconds 1164.043\n",
      "Epoch 1300 | val loss 1.665 | Time 31.765 | ETA in seconds 1175.309\n",
      "Epoch 1400 | val loss 1.685 | Time 31.800 | ETA in seconds 1144.803\n",
      "Epoch 1500 | val loss 1.664 | Time 32.891 | ETA in seconds 1151.182\n",
      "Epoch 1600 | val loss 1.662 | Time 36.070 | ETA in seconds 1226.395\n",
      "Epoch 1700 | val loss 1.644 | Time 24.783 | ETA in seconds 817.853\n",
      "Epoch 1800 | val loss 1.694 | Time 15.702 | ETA in seconds 502.464\n",
      "Epoch 1900 | val loss 1.654 | Time 14.950 | ETA in seconds 463.448\n",
      "Epoch 2000 | val loss 1.671 | Time 14.712 | ETA in seconds 441.373\n",
      "Epoch 2100 | val loss 1.644 | Time 14.458 | ETA in seconds 419.279\n",
      "Epoch 2200 | val loss 1.652 | Time 13.556 | ETA in seconds 379.571\n",
      "Epoch 2300 | val loss 1.646 | Time 19.170 | ETA in seconds 517.577\n",
      "Epoch 2400 | val loss 1.633 | Time 30.206 | ETA in seconds 785.348\n",
      "Epoch 2500 | val loss 1.627 | Time 32.691 | ETA in seconds 817.285\n",
      "Epoch 2600 | val loss 1.612 | Time 28.220 | ETA in seconds 677.283\n",
      "Epoch 2700 | val loss 1.613 | Time 30.075 | ETA in seconds 691.736\n",
      "Epoch 2800 | val loss 1.595 | Time 15.093 | ETA in seconds 332.051\n",
      "Epoch 2900 | val loss 1.612 | Time 14.357 | ETA in seconds 301.505\n",
      "Epoch 3000 | val loss 1.621 | Time 19.178 | ETA in seconds 383.552\n",
      "Epoch 3100 | val loss 1.580 | Time 28.245 | ETA in seconds 536.662\n",
      "Epoch 3200 | val loss 1.618 | Time 31.966 | ETA in seconds 575.381\n",
      "Epoch 3300 | val loss 1.652 | Time 26.973 | ETA in seconds 458.541\n",
      "Epoch 3400 | val loss 1.583 | Time 29.036 | ETA in seconds 464.579\n",
      "Epoch 3500 | val loss 1.606 | Time 32.101 | ETA in seconds 481.522\n",
      "Epoch 3600 | val loss 1.609 | Time 31.644 | ETA in seconds 443.017\n",
      "Epoch 3700 | val loss 1.607 | Time 29.779 | ETA in seconds 387.128\n",
      "Epoch 3800 | val loss 1.628 | Time 29.489 | ETA in seconds 353.863\n",
      "Epoch 3900 | val loss 1.626 | Time 30.111 | ETA in seconds 331.218\n",
      "Epoch 4000 | val loss 1.588 | Time 27.036 | ETA in seconds 270.357\n",
      "Epoch 4100 | val loss 1.585 | Time 31.199 | ETA in seconds 280.793\n",
      "Epoch 4200 | val loss 1.589 | Time 27.149 | ETA in seconds 217.195\n",
      "Epoch 4300 | val loss 1.586 | Time 13.721 | ETA in seconds 96.044\n",
      "Epoch 4400 | val loss 1.588 | Time 13.602 | ETA in seconds 81.609\n",
      "Epoch 4500 | val loss 1.593 | Time 13.051 | ETA in seconds 65.257\n",
      "Epoch 4600 | val loss 1.602 | Time 14.178 | ETA in seconds 56.712\n",
      "Epoch 4700 | val loss 1.595 | Time 23.807 | ETA in seconds 71.422\n",
      "Epoch 4800 | val loss 1.626 | Time 30.132 | ETA in seconds 60.265\n",
      "Epoch 4900 | val loss 1.570 | Time 27.781 | ETA in seconds 27.781\n",
      "validation loss:  1.5698100805282593\n",
      "CPU times: user 3h 43min 40s, sys: 5min 23s, total: 3h 49min 3s\n",
      "Wall time: 22min 58s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama_L{model_config.n_layers}xH{model_config.n_heads}xC{model_config.max_seq_len}_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            # logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized RoPE with shape torch.Size([128, 64])\n",
      "model params: 690113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "llama_infer = Llama(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KING EDWARD IV:\n",
      "And trust my fearful cry concule your loving\n",
      "An great me her making our king;\n",
      "And th\n",
      "\n",
      "A say the people's cares: mine count ho!\n",
      "\n",
      "First Citizen:\n",
      "Here is the Tower.\n",
      "\n",
      "CORIOLANUS:\n",
      "Give head t\n",
      "\n",
      "\n",
      "His like deservation our will\n",
      "your traitor, to Coverbeats, if I say,\n",
      "And stumble an one\n",
      "One must st\n",
      "\n",
      "LADY ANNE:\n",
      "Sir, Corioli, awake but post.\n",
      "\n",
      "SICINIUS:\n",
      "I will bright will\n",
      "Restard -time become in here.\n",
      "\n",
      "RICHARD II:\n",
      "What we this basend tell strike with the right.\n",
      "Ance is the swalls worse Peter'd, she go\n",
      "\n",
      "As that, and breder my proporture Heres as they liems imposes muster.\n",
      "\n",
      "VOLUMNIA:\n",
      "Then tell Richard! \n",
      "\n",
      "But etwo I up, sweet a Dive he Catesby,\n",
      "Than I say, in behold round thee; my else.\n",
      "O, who preventure\n",
      "\n",
      "We married. What it subjects spur pale:\n",
      "You then Juliet purly he discan encourteous lay\n",
      "with thing; \n",
      "\n",
      "A thou for leave to for a gone: dear you the sted\n",
      "it services in my are behold say.\n",
      "\n",
      "Lady:\n",
      "Then your\n",
      "\n",
      "KING EDWARD IV:\n",
      "Shall I, to avoide, and my lord man.\n",
      "\n",
      "GLOUCESTER:\n",
      "Then, God Saint Oxfound, the vast \n",
      "\n",
      "Madam:\n",
      "And burnellike us fyperty not in seen to here?\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "But, but, but I am but hold \n",
      "\n",
      "But be talkouth. She stay the seconcern of thy doars\n",
      "vemperable of the deadly lady with to be.\n",
      "Thy k\n",
      "\n",
      "YORK:\n",
      "No mine to sleep, and Catesby, what wind the joy,\n",
      "That palough of thee eye, and so, be hours.\n",
      "\n",
      "\n",
      "Now wonks prosed in heady's prayers, to do I,--\n",
      "\n",
      "Than most spie\n",
      "Joim is down that upon.\n",
      "What, they w\n",
      "\n",
      "Thid will I am this time. How now say conduction,\n",
      "Who man, as excellows one, O forth my danger.\n",
      "\n",
      "LAD\n",
      "\n",
      "STYORRES:\n",
      "Partly, then, that I have peace object, myself,\n",
      "Your here, with left common; 'sir? what?\n",
      "A\n",
      "\n",
      "Lord, there thy lord, befitsing with the most leasure\n",
      "May, but my die, and mock; that is;\n",
      "Than you i\n",
      "\n",
      "My bosolversety you\n",
      "When means to tears, do you day it.\n",
      "\n",
      "DUKE OF YORK:\n",
      "It one sea-lacks sent one our\n",
      "\n",
      "Clown:\n",
      "Make such lady: when\n",
      "As I had cannot get upon upon my soar,\n",
      "Till not the dead; therething\n",
      "My \n",
      "\n",
      "Parchorse!\n",
      "\n",
      "ARCHBISHOP OF BOLINGBROKE:\n",
      "He cannot forth Henry loversand friends.\n",
      "I pray from my son.\n",
      "\n",
      "\n",
      "A: I west if until painted amend says meet take:\n",
      "There was to just in punish'd Derpetion and\n",
      "Only Do\n",
      "\n",
      "what will dreams. What how Power, the lawful scapes breast,\n",
      "Perjestice out of thy sorrow from town o\n",
      "\n",
      "OHf how now\n",
      "Are ignorant from folly and were her king,\n",
      "Or till well, I shout there Tybalt, and down \n",
      "\n",
      "Must cold downs life accivils to-morrow\n",
      "And for thing, little so, fall York.\n",
      "Either, my lord. There \n",
      "\n",
      "\n",
      "TOMER:\n",
      "Welcome you, case that he departion;\n",
      "Who cannot no serve of fear. And maiden as look,\n",
      "O'er m\n",
      "\n",
      "Fullefore for you king, pray thee were out\n",
      "Again and this traitors upon ricker;\n",
      "and that a grave int\n",
      "\n",
      "Alight a for sleep at Not the king:\n",
      "Shall name sinter-hame, so with sight;\n",
      "Derby be should with his \n",
      "\n",
      "Were indeed.\n",
      "My hree, yet why France you choose Bona,\n",
      "Where or your doth your havenged deid.\n",
      "\n",
      "AUTOLY\n",
      "\n",
      "ELBOW:\n",
      "How now, there?\n",
      "\n",
      "LORD FITZWATER:\n",
      "Why, who goes we this soul talk out, sower, a man;\n",
      "But who a\n",
      "\n",
      "\n",
      "WESTMOREL:\n",
      "If what of Will, being them an there: but in the haturely.\n",
      "\n",
      "LUCIO:\n",
      "O, give me was but de\n",
      "\n",
      "\n",
      "One prince, if they strength and her.\n",
      "Makes we save a the faming it minius thy lagger,\n",
      "Ergath, ere \n",
      "\n",
      "SAMPSON:\n",
      "Now hath sword: by new my prince of thee,\n",
      "where is I know her! I get to hither like of my s\n",
      "CPU times: user 3.1 s, sys: 138 s, total: 3.11 s\n",
      "Wall time: 313 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for s in generate(llama_infer, 100):\n",
    "    print(s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
