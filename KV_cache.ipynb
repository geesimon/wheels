{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelArgs(dim=128, n_layers=2, n_heads=4, n_kv_heads=None, vocab_size=-1, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, max_batch_size=32, max_seq_len=64, epochs=10000)\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    dim: int = 128\n",
    "    n_layers: int = 2\n",
    "    n_heads: int = 4\n",
    "    n_kv_heads: Optional[int] = None\n",
    "    vocab_size: int = -1  # defined later by tokenizer\n",
    "    multiple_of: int = 256  # make SwiGLU hidden layer size multiple of large power of 2\n",
    "    ffn_dim_multiplier: Optional[float] = None\n",
    "    norm_eps: float = 1e-5\n",
    "\n",
    "    max_batch_size: int = 32\n",
    "    max_seq_len: int = 16 * 4\n",
    "\n",
    "    epochs: int = 10_000    \n",
    "\n",
    "model_config = ModelArgs()\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: 1115394\n"
     ]
    }
   ],
   "source": [
    "# simple tokenization by characters\n",
    "def encode(s):\n",
    "    return [stoi[ch] for ch in s]\n",
    "\n",
    "def decode(l):\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "\n",
    "lines = open('./data/Shakespeare.txt', 'r').read()\n",
    "vocab = sorted(list(set(lines)))\n",
    "itos = {i:ch for i, ch in enumerate(vocab)}\n",
    "stoi = {ch:i for i, ch in enumerate(vocab)}\n",
    "dataset = torch.tensor(encode(lines), dtype=torch.int8)\n",
    "print(f'Sentences: {dataset.shape[0]}')\n",
    "\n",
    "model_config.vocab_size = len(vocab)\n",
    "\n",
    "def get_batches(data, split, batch_size, context_window):\n",
    "    train = data[:int(.8 * len(data))]\n",
    "    val = data[int(.8 * len(data)): int(.9 * len(data))]\n",
    "    test = data[int(.9 * len(data)):]\n",
    "\n",
    "    if split == 'train':\n",
    "        batch_data = train\n",
    "    elif split == 'test':\n",
    "        batch_data = test\n",
    "    else:\n",
    "        batch_data = val\n",
    "\n",
    "    # pick random starting points\n",
    "    ix = torch.randint(0, batch_data.size(0) - context_window - 1, (batch_size,))\n",
    "    x = torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
    "    y = torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1115394])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMS Normalization \n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/1910.07467.pdf)\n",
    "- [Reference implementation](https://github.com/facebookresearch/llama/blob/54d44631054deae836aec8ceff92dcf8f20ca9e7/llama/model.py#L34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the RMSNorm normalization layer.\n",
    "\n",
    "        Args:\n",
    "            dim (int): The dimension of the input tensor.\n",
    "            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "\n",
    "        Attributes:\n",
    "            eps (float): A small value added to the denominator for numerical stability.\n",
    "            weight (nn.Parameter): Learnable scaling parameter.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x : torch.tensor) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Apply the RMSNorm normalization to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The normalized tensor.\n",
    "\n",
    "        \"\"\"\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the RMSNorm layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor after applying RMSNorm.\n",
    "\n",
    "        \"\"\"        \n",
    "        return self._norm(x.float()).type_as(x) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoPE\n",
    "\n",
    "- [Paper](https://arxiv.org/pdf/2104.09864.pdf)\n",
    "- [Reference Implementation](https://github.com/facebookresearch/llama/blob/dccf644213a2771a81fc4a754eed9623ea7f8444/llama/model.py#L80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE:\n",
    "    def __init__(self, dim: int, max_seq_len: int, theta: float = 10000.0):\n",
    "        \"\"\"\n",
    "        Precompute the frequency tensor for complex exponentials (cis, defined as 'm*theta_i' in the paper) \n",
    "        with given dimensions.\n",
    "\n",
    "        Calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
    "        and the max sequence length. The 'theta_base' parameter scales the frequencies.\n",
    "        The returned tensor contains complex values in complex64 data type.\n",
    "\n",
    "        Args:\n",
    "            dim (int): Dimension of the frequency tensor.\n",
    "            max_seq_len (int): Max sequence length.\n",
    "            theta_base (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
    "        \"\"\"\n",
    "        freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "        freqs = torch.outer(torch.arange(max_seq_len), freqs).float()\n",
    "        self.freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "        print(f\"Rope Initialized! dim:{dim} max_seq_len:{max_seq_len}\")\n",
    "        \n",
    "    def __call__(self, x: torch.Tensor, start_pos = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotary embeddings to input tensors using the given frequency tensor.\n",
    "\n",
    "        This function first reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
    "        for the purpose of broadcasting the frequency tensor during element-wise operations. Then, it applies \n",
    "        rotary embeddings to 'x' tensor using frequency tensor 'freqs_cis'.         \n",
    "        \"\"\"\n",
    "        x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
    "\n",
    "        shape = [d if i == 1 or i == x.ndim - 1 else 1 for i, d in enumerate(x_complex.shape)]\n",
    "        freqs_cis = self.freqs_cis[start_pos:start_pos + x.shape[-2]].view(*shape)\n",
    "                \n",
    "        x_real = torch.view_as_real(x_complex * freqs_cis).flatten(-2)\n",
    "        \n",
    "        return x_real.type_as(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RoPE Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rope Initialized! dim:128 max_seq_len:256\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "dim = 128\n",
    "max_seq_len = 256\n",
    "\n",
    "def get_rotary_matrix(context_window, embedding_dim):\n",
    "    R = torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
    "    for position in range(context_window):\n",
    "        for i in range(embedding_dim//2):\n",
    "            theta = 10000. ** (-2.*i / embedding_dim)\n",
    "            m_theta = position * theta\n",
    "            R[position, 2*i,2*i] = np.cos(m_theta)\n",
    "            R[position, 2*i,2*i+1] = - np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i] = np.sin(m_theta)\n",
    "            R[position, 2*i+1,2*i+1] = np.cos(m_theta)\n",
    "    return R\n",
    "\n",
    "R = get_rotary_matrix(max_seq_len, dim)\n",
    "\n",
    "X= torch.ones(1, max_seq_len, dim)\n",
    "rope = RoPE(dim=dim, max_seq_len=max_seq_len)\n",
    "X1 = rope(X)\n",
    "X2 = (R @ X.unsqueeze(-1)).flatten(-2)\n",
    "\n",
    "print(X1.allclose(X2, atol=1e-3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    shared_rope : RoPE = None\n",
    "\n",
    "    def __init__(self, config : ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if Attention.shared_rope is None:\n",
    "            Attention.shared_rope = RoPE(config.dim, config.max_seq_len)\n",
    "\n",
    "        self.w_q = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_k = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.w_v = nn.Linear(config.dim, config.dim, bias=False)\n",
    "        self.cache_k = torch.zeros(config.max_batch_size, config.max_seq_len, config.dim)\n",
    "        self.cache_v = torch.zeros_like(self.cache_k)\n",
    "\n",
    "    def forward(self, x: torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        # print(f'start_pos: {start_pos} \\n x: {x[:,:,:5]}')\n",
    "        q = self.w_q(x)\n",
    "        k = self.w_k(x)\n",
    "        v = self.w_v(x)\n",
    "\n",
    "        q = Attention.shared_rope(q, start_pos)\n",
    "        k = Attention.shared_rope(k, start_pos)\n",
    "\n",
    "        if self.training:       # apply dropout only during training\n",
    "            dropout_p = .1\n",
    "        else:\n",
    "            self.cache_k[:, start_pos:start_pos + x.shape[-2]] = k\n",
    "            self.cache_v[:, start_pos:start_pos + x.shape[-2]] = v        \n",
    "            k = self.cache_k[:, :start_pos + x.shape[-2]]\n",
    "            v = self.cache_v[:, :start_pos + x.shape[-2]]\n",
    "            \n",
    "            dropout_p = 0\n",
    "                \n",
    "        activations = F.scaled_dot_product_attention(\n",
    "            q, k, v, dropout_p = dropout_p, is_causal=True\n",
    "        )\n",
    "\n",
    "        return activations\n",
    "\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(config) for _ in range(config.n_heads)\n",
    "        ])\n",
    "        self.linear = nn.Linear(config.n_heads * config.dim, config.dim)\n",
    "        self.dropout = nn.Dropout(.1)\n",
    "\n",
    "    def forward(self, x : torch.tensor, start_pos : int) -> torch.tensor:\n",
    "        heads = [h(x, start_pos) for h in self.heads]\n",
    "        x = torch.cat(heads, dim=-1)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class SwiGLU(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.linear_gate = nn.Linear(size, size)\n",
    "        self.linear = nn.Linear(size, size)\n",
    "\n",
    "        self.beta = torch.ones(1, requires_grad=True)\n",
    "\n",
    "    def forward(self, x): \n",
    "        swish_gate = self.linear_gate(x) * torch.sigmoid(self.beta * self.linear_gate(x))\n",
    "        out = swish_gate * self.linear(x)\n",
    "        return out\n",
    "\n",
    "class LlamaBlock(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.rms = RMSNorm(config.dim)\n",
    "\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, start_pos) -> torch.tensor:\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.attention(x, start_pos)\n",
    "\n",
    "        x = self.rms(x) # rms pre-normalization\n",
    "        x = x + self.feedforward(x)\n",
    "        return x\n",
    "\n",
    "class Llama(nn.Module):\n",
    "    def __init__(self, config: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embeddings = nn.Embedding(config.vocab_size, config.dim)\n",
    "        self.llama_blocks = nn.Sequential(\n",
    "            OrderedDict([(f\"LlamaBlock_{i}\", LlamaBlock(config)) for i in range(config.n_layers)])\n",
    "        )\n",
    "\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(config.dim, config.dim),\n",
    "            SwiGLU(config.dim),\n",
    "            nn.Linear(config.dim, config.vocab_size),\n",
    "        )\n",
    "\n",
    "        print(\"model params:\", sum([m.numel() for m in self.parameters()]))\n",
    "\n",
    "    def forward(self, idx, start_pos = 0, targets = None):\n",
    "        x = self.embeddings(idx)\n",
    "        for block in self.llama_blocks:\n",
    "            x = block(x, start_pos)\n",
    "        logits = self.ffn(x)\n",
    "\n",
    "        if targets is None:\n",
    "            return logits\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits.view(-1, self.config.vocab_size), targets.view(-1))\n",
    "            return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()  # don't compute gradients for this function\n",
    "def evaluate_loss(model:Llama):\n",
    "    config = model.config\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"val\"]:\n",
    "        losses = []\n",
    "        for _ in range(10):\n",
    "            xb, yb = get_batches(dataset, split, config.max_batch_size, config.max_seq_len)\n",
    "            _, loss = model(xb, 0, yb)\n",
    "            losses.append(loss.item())\n",
    "        out[split] = np.mean(losses)\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def train(model: Llama, optimizer:torch.optim.Optimizer, scheduler = None, print_logs = False, log_interval = 100):\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "    config = model.config\n",
    "    for epoch in range(config.epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        xs, ys = get_batches(dataset, 'train', config.max_batch_size, config.max_seq_len)\n",
    "        _, loss = model(xs, 0, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        if epoch % log_interval == 0:\n",
    "            batch_time = time.time() - start_time\n",
    "            x = evaluate_loss(model)\n",
    "            losses += [x]\n",
    "            if print_logs:\n",
    "                print(f\"Epoch {epoch} | val loss {x['val']:.3f} | Time {batch_time:.3f} | ETA in seconds {batch_time * (config.epochs - epoch)/log_interval :.3f}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            if scheduler:\n",
    "                print(\"lr: \", scheduler.get_lr())\n",
    "\n",
    "    # print(pd.DataFrame(losses))\n",
    "    print(\"validation loss: \", losses[-1]['val'])\n",
    "    return pd.DataFrame(losses).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rope Initialized! dim:128 max_seq_len:64\n",
      "model params: 690113\n",
      "Epoch 0 | val loss 4.157 | Time 0.093 | ETA in seconds 9.301\n",
      "Epoch 100 | val loss 2.277 | Time 4.854 | ETA in seconds 480.500\n",
      "Epoch 200 | val loss 2.074 | Time 4.706 | ETA in seconds 461.171\n",
      "Epoch 300 | val loss 1.995 | Time 4.638 | ETA in seconds 449.871\n",
      "Epoch 400 | val loss 1.951 | Time 4.618 | ETA in seconds 443.362\n",
      "Epoch 500 | val loss 1.886 | Time 5.183 | ETA in seconds 492.339\n",
      "Epoch 600 | val loss 1.861 | Time 5.365 | ETA in seconds 504.303\n",
      "Epoch 700 | val loss 1.813 | Time 5.948 | ETA in seconds 553.142\n",
      "Epoch 800 | val loss 1.813 | Time 6.272 | ETA in seconds 577.047\n",
      "Epoch 900 | val loss 1.819 | Time 7.348 | ETA in seconds 668.707\n",
      "Epoch 1000 | val loss 1.796 | Time 6.348 | ETA in seconds 571.332\n",
      "Epoch 1100 | val loss 1.774 | Time 7.038 | ETA in seconds 626.403\n",
      "Epoch 1200 | val loss 1.767 | Time 7.286 | ETA in seconds 641.168\n",
      "Epoch 1300 | val loss 1.743 | Time 6.638 | ETA in seconds 577.516\n",
      "Epoch 1400 | val loss 1.752 | Time 6.402 | ETA in seconds 550.582\n",
      "Epoch 1500 | val loss 1.748 | Time 6.982 | ETA in seconds 593.476\n",
      "Epoch 1600 | val loss 1.730 | Time 6.383 | ETA in seconds 536.197\n",
      "Epoch 1700 | val loss 1.746 | Time 6.401 | ETA in seconds 531.312\n",
      "Epoch 1800 | val loss 1.726 | Time 6.271 | ETA in seconds 514.183\n",
      "Epoch 1900 | val loss 1.720 | Time 6.313 | ETA in seconds 511.365\n",
      "Epoch 2000 | val loss 1.699 | Time 6.339 | ETA in seconds 507.125\n",
      "Epoch 2100 | val loss 1.709 | Time 6.280 | ETA in seconds 496.093\n",
      "Epoch 2200 | val loss 1.735 | Time 7.249 | ETA in seconds 565.388\n",
      "Epoch 2300 | val loss 1.688 | Time 6.761 | ETA in seconds 520.627\n",
      "Epoch 2400 | val loss 1.687 | Time 6.112 | ETA in seconds 464.476\n",
      "Epoch 2500 | val loss 1.702 | Time 5.956 | ETA in seconds 446.716\n",
      "Epoch 2600 | val loss 1.694 | Time 6.041 | ETA in seconds 447.018\n",
      "Epoch 2700 | val loss 1.685 | Time 7.008 | ETA in seconds 511.615\n",
      "Epoch 2800 | val loss 1.684 | Time 7.073 | ETA in seconds 509.220\n",
      "Epoch 2900 | val loss 1.651 | Time 6.377 | ETA in seconds 452.767\n",
      "Epoch 3000 | val loss 1.704 | Time 6.252 | ETA in seconds 437.648\n",
      "Epoch 3100 | val loss 1.681 | Time 6.464 | ETA in seconds 445.982\n",
      "Epoch 3200 | val loss 1.663 | Time 6.336 | ETA in seconds 430.874\n",
      "Epoch 3300 | val loss 1.645 | Time 6.284 | ETA in seconds 421.002\n",
      "Epoch 3400 | val loss 1.639 | Time 6.321 | ETA in seconds 417.175\n",
      "Epoch 3500 | val loss 1.641 | Time 6.359 | ETA in seconds 413.352\n",
      "Epoch 3600 | val loss 1.673 | Time 6.378 | ETA in seconds 408.205\n",
      "Epoch 3700 | val loss 1.678 | Time 6.432 | ETA in seconds 405.194\n",
      "Epoch 3800 | val loss 1.673 | Time 6.461 | ETA in seconds 400.613\n",
      "Epoch 3900 | val loss 1.633 | Time 6.471 | ETA in seconds 394.714\n",
      "Epoch 4000 | val loss 1.675 | Time 6.421 | ETA in seconds 385.234\n",
      "Epoch 4100 | val loss 1.660 | Time 6.604 | ETA in seconds 389.637\n",
      "Epoch 4200 | val loss 1.650 | Time 6.633 | ETA in seconds 384.709\n",
      "Epoch 4300 | val loss 1.671 | Time 6.350 | ETA in seconds 361.945\n",
      "Epoch 4400 | val loss 1.639 | Time 6.389 | ETA in seconds 357.796\n",
      "Epoch 4500 | val loss 1.644 | Time 6.499 | ETA in seconds 357.449\n",
      "Epoch 4600 | val loss 1.635 | Time 6.350 | ETA in seconds 342.901\n",
      "Epoch 4700 | val loss 1.656 | Time 6.327 | ETA in seconds 335.312\n",
      "Epoch 4800 | val loss 1.645 | Time 6.478 | ETA in seconds 336.848\n",
      "Epoch 4900 | val loss 1.599 | Time 6.509 | ETA in seconds 331.960\n",
      "Epoch 5000 | val loss 1.648 | Time 6.433 | ETA in seconds 321.631\n",
      "Epoch 5100 | val loss 1.656 | Time 6.237 | ETA in seconds 305.637\n",
      "Epoch 5200 | val loss 1.616 | Time 6.272 | ETA in seconds 301.077\n",
      "Epoch 5300 | val loss 1.607 | Time 6.207 | ETA in seconds 291.729\n",
      "Epoch 5400 | val loss 1.617 | Time 6.420 | ETA in seconds 295.340\n",
      "Epoch 5500 | val loss 1.618 | Time 6.182 | ETA in seconds 278.212\n",
      "Epoch 5600 | val loss 1.617 | Time 6.209 | ETA in seconds 273.198\n",
      "Epoch 5700 | val loss 1.605 | Time 6.194 | ETA in seconds 266.346\n",
      "Epoch 5800 | val loss 1.606 | Time 6.315 | ETA in seconds 265.213\n",
      "Epoch 5900 | val loss 1.653 | Time 6.229 | ETA in seconds 255.407\n",
      "Epoch 6000 | val loss 1.648 | Time 6.442 | ETA in seconds 257.673\n",
      "Epoch 6100 | val loss 1.612 | Time 6.206 | ETA in seconds 242.026\n",
      "Epoch 6200 | val loss 1.614 | Time 6.306 | ETA in seconds 239.624\n",
      "Epoch 6300 | val loss 1.630 | Time 6.348 | ETA in seconds 234.875\n",
      "Epoch 6400 | val loss 1.581 | Time 6.493 | ETA in seconds 233.751\n",
      "Epoch 6500 | val loss 1.605 | Time 6.502 | ETA in seconds 227.572\n",
      "Epoch 6600 | val loss 1.620 | Time 6.543 | ETA in seconds 222.470\n",
      "Epoch 6700 | val loss 1.647 | Time 6.517 | ETA in seconds 215.058\n",
      "Epoch 6800 | val loss 1.697 | Time 6.407 | ETA in seconds 205.009\n",
      "Epoch 6900 | val loss 1.634 | Time 6.506 | ETA in seconds 201.673\n",
      "Epoch 7000 | val loss 1.606 | Time 6.480 | ETA in seconds 194.389\n",
      "Epoch 7100 | val loss 1.630 | Time 6.502 | ETA in seconds 188.562\n",
      "Epoch 7200 | val loss 1.606 | Time 6.475 | ETA in seconds 181.292\n",
      "Epoch 7300 | val loss 1.607 | Time 6.342 | ETA in seconds 171.229\n",
      "Epoch 7400 | val loss 1.599 | Time 6.352 | ETA in seconds 165.140\n",
      "Epoch 7500 | val loss 1.610 | Time 6.280 | ETA in seconds 157.006\n",
      "Epoch 7600 | val loss 1.607 | Time 6.226 | ETA in seconds 149.417\n",
      "Epoch 7700 | val loss 1.599 | Time 6.432 | ETA in seconds 147.940\n",
      "Epoch 7800 | val loss 1.626 | Time 6.211 | ETA in seconds 136.653\n",
      "Epoch 7900 | val loss 1.613 | Time 6.243 | ETA in seconds 131.106\n",
      "Epoch 8000 | val loss 1.606 | Time 6.295 | ETA in seconds 125.891\n",
      "Epoch 8100 | val loss 1.595 | Time 6.289 | ETA in seconds 119.494\n",
      "Epoch 8200 | val loss 1.587 | Time 6.273 | ETA in seconds 112.912\n",
      "Epoch 8300 | val loss 1.602 | Time 6.356 | ETA in seconds 108.058\n",
      "Epoch 8400 | val loss 1.616 | Time 6.107 | ETA in seconds 97.716\n",
      "Epoch 8500 | val loss 1.591 | Time 6.170 | ETA in seconds 92.549\n",
      "Epoch 8600 | val loss 1.586 | Time 6.265 | ETA in seconds 87.707\n",
      "Epoch 8700 | val loss 1.607 | Time 6.246 | ETA in seconds 81.197\n",
      "Epoch 8800 | val loss 1.610 | Time 6.292 | ETA in seconds 75.509\n",
      "Epoch 8900 | val loss 1.594 | Time 6.209 | ETA in seconds 68.304\n",
      "Epoch 9000 | val loss 1.586 | Time 6.191 | ETA in seconds 61.915\n",
      "Epoch 9100 | val loss 1.639 | Time 6.432 | ETA in seconds 57.886\n",
      "Epoch 9200 | val loss 1.641 | Time 6.436 | ETA in seconds 51.490\n",
      "Epoch 9300 | val loss 1.601 | Time 6.439 | ETA in seconds 45.070\n",
      "Epoch 9400 | val loss 1.582 | Time 6.423 | ETA in seconds 38.539\n",
      "Epoch 9500 | val loss 1.636 | Time 6.400 | ETA in seconds 32.001\n",
      "Epoch 9600 | val loss 1.598 | Time 6.310 | ETA in seconds 25.241\n",
      "Epoch 9700 | val loss 1.601 | Time 6.399 | ETA in seconds 19.196\n",
      "Epoch 9800 | val loss 1.616 | Time 6.308 | ETA in seconds 12.615\n",
      "Epoch 9900 | val loss 1.628 | Time 6.425 | ETA in seconds 6.425\n",
      "validation loss:  1.6278865575790404\n",
      "CPU times: user 1h 51min 38s, sys: 5.25 s, total: 1h 51min 44s\n",
      "Wall time: 11min 10s\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "llama = Llama(model_config)\n",
    "optimizer = torch.optim.Adam(llama.parameters())\n",
    "train(llama, optimizer, print_logs=True)\n",
    "\n",
    "# Save\n",
    "now = datetime.now()\n",
    "model_name = f'./checkpoint/llama_{now.year}_{now.month}_{now.day}_{now.hour}_{now.minute}.pth'\n",
    "torch.save({'model_state_dict': llama.state_dict()}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(model : Llama, max_new_tokens = 10):\n",
    "    model.eval()\n",
    "    config = model.config\n",
    "    max_new_tokens = model.config.max_seq_len if max_new_tokens > model.config.max_seq_len else max_new_tokens\n",
    "    idx = torch.zeros(config.max_batch_size, 1).long()\n",
    "\n",
    "    start_pos = 0\n",
    "    for i in range(max_new_tokens):\n",
    "        # print(i)\n",
    "        if i == 0:\n",
    "            logits = model(idx)\n",
    "        else:\n",
    "            # logits = model(idx[:, -1].unsqueeze(-1), start_pos)\n",
    "            logits = model(idx[:, -config.max_seq_len:], 0)\n",
    "        \n",
    "        last_time_step_logits = logits[:, -1, :]            # all the batches (1), last time step, all the logits\n",
    "        p = F.softmax(last_time_step_logits, dim=-1)        # softmax to get probabilities\n",
    "        idx_next = torch.multinomial(\n",
    "            p, num_samples=1\n",
    "        )                                                   # sample from the distribution to get the next token\n",
    "\n",
    "        start_pos = idx.shape[-1]\n",
    "        idx = torch.cat([idx, idx_next], dim=-1)            # append to the sequence\n",
    "                    \n",
    "    return [decode(x) for x in idx.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rope Initialized! dim:128 max_seq_len:64\n",
      "model params: 690113\n",
      "\n",
      "The manabled approud cheque done? for in then breathest?\n",
      "Ah, fro\n",
      "CPU times: user 1.3 s, sys: 10 ms, total: 1.31 s\n",
      "Wall time: 143 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "## Get lastest checkpoint\n",
    "files = glob.glob('./checkpoint/*.pth')\n",
    "files.sort(key=os.path.getmtime)\n",
    "model_name = files[-1]\n",
    "\n",
    "model_config.max_batch_size = 1\n",
    "Attention.shared_rope = None\n",
    "\n",
    "llama_infer = Llama(model_config)\n",
    "llama_infer.load_state_dict(torch.load(model_name)['model_state_dict'])\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "print(generate(llama_infer, 160)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Attention.shared_rope = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-fundamentals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
